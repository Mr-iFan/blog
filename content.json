{"meta":{"title":"iFan","subtitle":"","description":"","author":"iFan","url":"https://blog.ifan.host","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2024-01-10T06:55:44.549Z","updated":"2023-12-25T05:56:09.149Z","comments":false,"path":"/404.html","permalink":"https://blog.ifan.host/404.html","excerpt":"","text":""},{"title":"关于","date":"2024-01-10T06:55:44.555Z","updated":"2023-12-25T05:56:09.189Z","comments":false,"path":"about/index.html","permalink":"https://blog.ifan.host/about/index.html","excerpt":"","text":"正在路上"},{"title":"书单","date":"2024-01-10T06:55:44.555Z","updated":"2023-12-25T05:56:09.189Z","comments":false,"path":"books/index.html","permalink":"https://blog.ifan.host/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2024-01-10T06:55:44.548Z","updated":"2023-12-25T05:56:09.189Z","comments":false,"path":"categories/index.html","permalink":"https://blog.ifan.host/categories/index.html","excerpt":"","text":""},{"title":"个人网站导航","date":"2024-01-10T06:55:44.549Z","updated":"2023-12-25T05:56:09.261Z","comments":true,"path":"links/index.html","permalink":"https://blog.ifan.host/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2024-01-10T06:55:44.614Z","updated":"2023-12-25T05:56:09.262Z","comments":false,"path":"repository/index.html","permalink":"https://blog.ifan.host/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2024-01-10T06:55:44.549Z","updated":"2023-12-25T05:56:09.262Z","comments":false,"path":"tags/index.html","permalink":"https://blog.ifan.host/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Sqlalchemy Upsert","slug":"language/python/libs/sqlalchemy/sqlalchemy-upsery","date":"2024-10-01T01:00:00.000Z","updated":"2024-10-10T06:12:41.745Z","comments":true,"path":"2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-upsery/","link":"","permalink":"https://blog.ifan.host/2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-upsery/","excerpt":"","text":"实现不存在则插入12345678def upsert(table, insert_data: dict, update_data: dict): &quot;&quot;&quot; 插入或者更新 &quot;&quot;&quot; insert_stmt = insert(table).values(**insert_data) on_duplicate_key_stmt = insert_stmt.on_duplicate_key_update(**update_data) with db.engine.connect() as connect: connect.execute(on_duplicate_key_stmt) 12345678def ignore_insert(table, data: dict): &quot;&quot;&quot; 忽略重复插入 &quot;&quot;&quot; insert_command = table.__table__.insert().prefix_with(&#x27; IGNORE&#x27;).values(data) with db.engine.connect() as connect: connect.execute(insert_command) connect.commit()","categories":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/categories/Sqlalchemy/"},{"name":"ORM","slug":"Sqlalchemy/ORM","permalink":"https://blog.ifan.host/categories/Sqlalchemy/ORM/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/tags/Sqlalchemy/"},{"name":"ORM","slug":"ORM","permalink":"https://blog.ifan.host/tags/ORM/"}]},{"title":"Sqlalchemy 绑定多个数据库","slug":"language/python/libs/sqlalchemy/sqlalchemy-bind-many-db","date":"2024-10-01T01:00:00.000Z","updated":"2024-10-10T06:13:20.518Z","comments":true,"path":"2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-bind-many-db/","link":"","permalink":"https://blog.ifan.host/2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-bind-many-db/","excerpt":"","text":"绑定多个数据库12345SQLALCHEMY_DATABASE_URI = &#x27;postgres://localhost/main&#x27;SQLALCHEMY_BINDS = &#123; &#x27;users&#x27;: &#x27;mysqldb://localhost/users&#x27;, &#x27;appmeta&#x27;: &#x27;sqlite:////path/to/appmeta.db&#x27;&#125; 创建和删除表 1234db.create_all()db.create_all(bind=[&#x27;users&#x27;])db.create_all(bind=&#x27;appmeta&#x27;)db.drop_all(bind=None)","categories":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/categories/Sqlalchemy/"},{"name":"ORM","slug":"Sqlalchemy/ORM","permalink":"https://blog.ifan.host/categories/Sqlalchemy/ORM/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/tags/Sqlalchemy/"},{"name":"ORM","slug":"ORM","permalink":"https://blog.ifan.host/tags/ORM/"}]},{"title":"Sqlalchemy 批量插入数据","slug":"language/python/libs/sqlalchemy/sqlalchemy-batch-data","date":"2024-10-01T01:00:00.000Z","updated":"2024-10-10T06:14:23.708Z","comments":true,"path":"2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-batch-data/","link":"","permalink":"https://blog.ifan.host/2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-batch-data/","excerpt":"","text":"批量插入数据测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121import timeimport sqlite3from sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Column, Integer, String, create_enginefrom sqlalchemy.orm import scoped_session, sessionmakerBase = declarative_base()DBSession = scoped_session(sessionmaker())engine = Noneclass Customer(Base): __tablename__ = &quot;customer&quot; id = Column(Integer, primary_key=True) name = Column(String(255))def init_sqlalchemy(dbname=&#x27;sqlite:///sqlalchemy.db&#x27;): global engine engine = create_engine(dbname, echo=False) DBSession.remove() DBSession.configure(bind=engine, autoflush=False, expire_on_commit=False) Base.metadata.drop_all(engine) Base.metadata.create_all(engine)def test_sqlalchemy_orm(n=100000): init_sqlalchemy() t0 = time.time() for i in range(n): customer = Customer() customer.name = &#x27;NAME &#x27; + str(i) DBSession.add(customer) if i % 1000 == 0: DBSession.flush() DBSession.commit() print( &quot;SQLAlchemy ORM: Total time for &quot; + str(n) + &quot; records &quot; + str(time.time() - t0) + &quot; secs&quot;)def test_sqlalchemy_orm_pk_given(n=100000): init_sqlalchemy() t0 = time.time() for i in range(n): customer = Customer(id=i+1, name=&quot;NAME &quot; + str(i)) DBSession.add(customer) if i % 1000 == 0: DBSession.flush() DBSession.commit() print( &quot;SQLAlchemy ORM pk given: Total time for &quot; + str(n) + &quot; records &quot; + str(time.time() - t0) + &quot; secs&quot;)def test_sqlalchemy_orm_bulk_insert(n=100000): init_sqlalchemy() t0 = time.time() n1 = n while n1 &gt; 0: n1 = n1 - 10000 DBSession.bulk_insert_mappings( Customer, [ dict(name=&quot;NAME &quot; + str(i)) for i in range(min(10000, n1)) ] ) DBSession.commit() print( &quot;SQLAlchemy ORM bulk_save_objects(): Total time for &quot; + str(n) + &quot; records &quot; + str(time.time() - t0) + &quot; secs&quot;)def test_sqlalchemy_core(n=100000): init_sqlalchemy() t0 = time.time() engine.execute( Customer.__table__.insert(), [&#123;&quot;name&quot;: &#x27;NAME &#x27; + str(i)&#125; for i in range(n)] ) print( &quot;SQLAlchemy Core: Total time for &quot; + str(n) + &quot; records &quot; + str(time.time() - t0) + &quot; secs&quot;)def test_sqlalchemy_add_all(n=100000): init_sqlalchemy() t0 = time.time() objs = [] for i in range(n): objs.append(Customer(id = i, name = &quot;name&quot; + str(i))) DBSession.add_all(objs) DBSession.commit() print( &quot;SQLAlchemy all_all: Total time for &quot; + str(n) + &quot; records &quot; + str(time.time() - t0) + &quot; secs&quot;)def init_sqlite3(dbname): conn = sqlite3.connect(dbname) c = conn.cursor() c.execute(&quot;DROP TABLE IF EXISTS customer&quot;) c.execute( &quot;CREATE TABLE customer (id INTEGER NOT NULL, &quot; &quot;name VARCHAR(255), PRIMARY KEY(id))&quot;) conn.commit() return conndef test_sqlite3(n=100000, dbname=&#x27;sqlite3.db&#x27;): conn = init_sqlite3(dbname) c = conn.cursor() t0 = time.time() for i in range(n): row = (&#x27;NAME &#x27; + str(i),) c.execute(&quot;INSERT INTO customer (name) VALUES (?)&quot;, row) conn.commit() print( &quot;sqlite3: Total time for &quot; + str(n) + &quot; records &quot; + str(time.time() - t0) + &quot; sec&quot;)if __name__ == &#x27;__main__&#x27;: test_sqlalchemy_orm(100000) test_sqlalchemy_orm_pk_given(100000) test_sqlalchemy_orm_bulk_insert(100000) test_sqlalchemy_add_all(100000) test_sqlalchemy_core(100000) test_sqlite3(100000)","categories":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/categories/Sqlalchemy/"},{"name":"ORM","slug":"Sqlalchemy/ORM","permalink":"https://blog.ifan.host/categories/Sqlalchemy/ORM/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/tags/Sqlalchemy/"},{"name":"ORM","slug":"ORM","permalink":"https://blog.ifan.host/tags/ORM/"}]},{"title":"Sqlalchemy 打印建表语句","slug":"language/python/libs/sqlalchemy/sqlalchemy-echo-create-table-sql","date":"2024-10-01T01:00:00.000Z","updated":"2024-10-10T06:15:09.222Z","comments":true,"path":"2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-echo-create-table-sql/","link":"","permalink":"https://blog.ifan.host/2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-echo-create-table-sql/","excerpt":"","text":"12from sqlalchemy.schema import CreateTableprint( CreateTable(Model.__table__ ) )","categories":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/categories/Sqlalchemy/"},{"name":"ORM","slug":"Sqlalchemy/ORM","permalink":"https://blog.ifan.host/categories/Sqlalchemy/ORM/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/tags/Sqlalchemy/"},{"name":"ORM","slug":"ORM","permalink":"https://blog.ifan.host/tags/ORM/"}]},{"title":"Sqlalchemy 关联查询","slug":"language/python/libs/sqlalchemy/sqlalchemy-join","date":"2024-10-01T01:00:00.000Z","updated":"2024-10-10T07:16:37.026Z","comments":true,"path":"2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-join/","link":"","permalink":"https://blog.ifan.host/2024/10/01/language/python/libs/sqlalchemy/sqlalchemy-join/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182from sqlalchemy import create_enginefrom sqlalchemy.orm import sessionmakerfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Column, Integer, Stringfrom sqlalchemy.orm import relationship# Create the database engine and sessionengine = create_engine(&#x27;sqlite:///join_demo.db&#x27;)Session = sessionmaker(bind=engine)session = Session()# Create the base modelBase = declarative_base()# Define the table modelsclass Table1(Base): __tablename__ = &#x27;table1&#x27; id = Column(Integer, primary_key=True) name = Column(String) table2_rows = relationship(&quot;Table2&quot;)class Table2(Base): __tablename__ = &#x27;table2&#x27; id = Column(Integer, primary_key=True) table1_id = Column(Integer, ForeignKey(&#x27;table1.id&#x27;)) name = Column(String)# Create the tablesBase.metadata.create_all(engine)# Insert sample datatable1_data = [ Table1(id=1, name=&#x27;Table1 Row 1&#x27;), Table1(id=2, name=&#x27;Table1 Row 2&#x27;), Table1(id=3, name=&#x27;Table1 Row 3&#x27;)]session.bulk_save_objects(table1_data)table2_data = [ Table2(id=1, table1_id=1, name=&#x27;Table2 Row 1&#x27;), Table2(id=2, table1_id=3, name=&#x27;Table2 Row 2&#x27;)]session.bulk_save_objects(table2_data)# 1. Inner Joininner_join_query = session.query(Table1, Table2).join(Table2, Table1.id == Table2.table1_id)inner_join_results = inner_join_query.all()print(&quot;Inner Join Results:&quot;)for result in inner_join_results: print(result[0].name, result[1].name)print()# 2. Left Joinleft_join_query = session.query(Table1, Table2).join(Table2, Table1.id == Table2.table1_id, isouter=True)left_join_results = left_join_query.all()print(&quot;Left Join Results:&quot;)for result in left_join_results: print(result[0].name, result[1].name if result[1] else &quot;NULL&quot;)print()# 3. Right Joinright_join_query = session.query(Table1, Table2).join(Table2, Table1.id == Table2.table1_id, isouter=True).from_statement(&quot;RIGHT JOIN table1 ON table1.id = table2.table1_id&quot;)right_join_results = right_join_query.all()print(&quot;Right Join Results:&quot;)for result in right_join_results: print(result[0].name if result[0] else &quot;NULL&quot;, result[1].name)print()# 4. Full Joinfull_join_query = session.query(Table1, Table2).join(Table2, Table1.id == Table2.table1_id, full=True)full_join_results = full_join_query.all()print(&quot;Full Join Results:&quot;)for result in full_join_results: print(result[0].name if result[0] else &quot;NULL&quot;, result[1].name if result[1] else &quot;NULL&quot;)print()# 5. Cross Joincross_join_query = session.query(Table1, Table2).cross_join(Table2)cross_join_results = cross_join_query.all()print(&quot;Cross Join Results:&quot;)for result in cross_join_results: print(result[0].name, result[1].name)","categories":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/categories/Sqlalchemy/"},{"name":"ORM","slug":"Sqlalchemy/ORM","permalink":"https://blog.ifan.host/categories/Sqlalchemy/ORM/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/tags/Sqlalchemy/"},{"name":"ORM","slug":"ORM","permalink":"https://blog.ifan.host/tags/ORM/"}]},{"title":"Docker Minio 实现文件永久链接","slug":"docker/docker-minio","date":"2022-12-10T01:00:00.000Z","updated":"2023-12-25T05:56:09.169Z","comments":true,"path":"2022/12/10/docker/docker-minio/","link":"","permalink":"https://blog.ifan.host/2022/12/10/docker/docker-minio/","excerpt":"","text":"启动Minio 1234567891011121314151617#!/bin/bashBASEPATH=`pwd`NAME=miniodocker rm -f $NAMEdocker run -d --name $NAME \\ --restart=always \\ -v $BASEPATH/data:/data \\ -v $BASEPATH/conf:/root/.minio \\ -e &quot;MINIO_SERVER_URL=https://APIHost地址&quot; \\ # 文件分享地址 -e &quot;MINIO_BROWSER_REDIRECT_URL=https://minio地址&quot; \\ # 页面查看地址 -e &quot;MINIO_ROOT_USER=xxxx&quot; \\ -e &quot;MINIO_ROOT_PASSWORD=xxxx&quot; \\ minio/minio \\ server ./data --console-address &quot;:9001&quot; --address &quot;:9000&quot;docker logs -f $NAME Nginx 配置 12345678910111213141516171819202122232425server &#123; listen 80; listen [::]:80; server_name xxxx; return 301 https://$server_name$request_uri;&#125;server &#123; listen 443 ssl http2; server_name xxxx; location / &#123; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://minio:9001/; # API自行配置9000 &#125; ssl_certificate /usr/local/openresty/nginx/cert/fullchain.cer; ssl_certificate_key /usr/local/openresty/nginx/cert/ifan.host.key; ssl_protocols TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; access_log /usr/local/openresty/nginx/logs/minio.access.log; error_log /usr/local/openresty/nginx/logs/minio.error.log;&#125; 配置本地mc 123456789101112&#123; &quot;version&quot;: &quot;10&quot;, &quot;aliases&quot;: &#123; &quot;minio&quot;: &#123; &quot;url&quot;: &quot;https://API地址&quot;, &quot;accessKey&quot;: &quot;xxxx&quot;, &quot;secretKey&quot;: &quot;xxx&quot;, &quot;api&quot;: &quot;s3v4&quot;, &quot;path&quot;: &quot;auto&quot; &#125; &#125;&#125; 设置永久链接 12# 设置桶为匿名可读mc anonymous set download minio/桶名称","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Docker","slug":"工具/Docker","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Docker/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Minio","slug":"Minio","permalink":"https://blog.ifan.host/tags/Minio/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.ifan.host/tags/Docker/"}]},{"title":"Minio 配置 nginx 代理","slug":"database/minio/minio-nginx-proxy","date":"2022-12-10T01:00:00.000Z","updated":"2023-12-25T08:26:29.115Z","comments":true,"path":"2022/12/10/database/minio/minio-nginx-proxy/","link":"","permalink":"https://blog.ifan.host/2022/12/10/database/minio/minio-nginx-proxy/","excerpt":"","text":"启动Minio1234567891011121314151617#!/bin/bashBASEPATH=`pwd`NAME=miniodocker rm -f $NAMEdocker run -d --name $NAME \\ --restart=always \\ -v $BASEPATH/data:/data \\ -v $BASEPATH/conf:/root/.minio \\ -e &quot;MINIO_SERVER_URL=https://APIHost地址&quot; \\ # 文件分享地址 -e &quot;MINIO_BROWSER_REDIRECT_URL=https://minio地址&quot; \\ # 页面查看地址 -e &quot;MINIO_ROOT_USER=xxxx&quot; \\ -e &quot;MINIO_ROOT_PASSWORD=xxxx&quot; \\ minio/minio \\ server ./data --console-address &quot;:9001&quot; --address &quot;:9000&quot;docker logs -f $NAME nginx配置12345678910111213141516171819202122232425server &#123; listen 80; listen [::]:80; server_name xxxx; return 301 https://$server_name$request_uri;&#125;server &#123; listen 443 ssl http2; server_name xxxx; location / &#123; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://minio:9001/; # API自行配置9000 &#125; ssl_certificate /usr/local/openresty/nginx/cert/fullchain.cer; ssl_certificate_key /usr/local/openresty/nginx/cert/ifan.host.key; ssl_protocols TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; access_log /usr/local/openresty/nginx/logs/minio.access.log; error_log /usr/local/openresty/nginx/logs/minio.error.log;&#125; 配置本地mc123456789101112&#123; &quot;version&quot;: &quot;10&quot;, &quot;aliases&quot;: &#123; &quot;minio&quot;: &#123; &quot;url&quot;: &quot;https://API地址&quot;, &quot;accessKey&quot;: &quot;xxxx&quot;, &quot;secretKey&quot;: &quot;xxx&quot;, &quot;api&quot;: &quot;s3v4&quot;, &quot;path&quot;: &quot;auto&quot; &#125; &#125;&#125; 设置永久链接12# 设置桶为匿名可读mc anonymous set download minio/桶名称","categories":[{"name":"Minio","slug":"Minio","permalink":"https://blog.ifan.host/categories/Minio/"}],"tags":[{"name":"Minio","slug":"Minio","permalink":"https://blog.ifan.host/tags/Minio/"}]},{"title":"Sqlalchemy 基础学习","slug":"language/python/libs/sqlalchemy/sqlalchemy-base","date":"2022-12-01T01:00:00.000Z","updated":"2024-10-10T06:09:37.342Z","comments":true,"path":"2022/12/01/language/python/libs/sqlalchemy/sqlalchemy-base/","link":"","permalink":"https://blog.ifan.host/2022/12/01/language/python/libs/sqlalchemy/sqlalchemy-base/","excerpt":"","text":"增删改查添加12345678910111213141516171819202122# 添加单个对象user = User(name=&#x27;ed&#x27;, fullname=&#x27;Ed Jones&#x27;, nickname=&#x27;edsnickname&#x27;)session.add(user)# 添加对象忽略重复错误insert_command = User.__table__.insert().prefix_with(&#x27; IGNORE&#x27;).values(data)with db.engine.connect() as connect: connect.execute(insert_command) connect.commit()# 批量添加users = [ User(name=&#x27;wendy&#x27;, fullname=&#x27;Wendy Williams&#x27;, nickname=&#x27;windy&#x27;), User(name=&#x27;mary&#x27;, fullname=&#x27;Mary Contrary&#x27;, nickname=&#x27;mary&#x27;), User(name=&#x27;fred&#x27;, fullname=&#x27;Fred Flintstone&#x27;, nickname=&#x27;freddy&#x27;)]session.add_all(users)session.commit() # 提交session.rollback() # 回滚# Upsertinsert_stmt = insert(User).values(**insert_data)on_duplicate_key_stmt = insert_stmt.on_duplicate_key_update(**update_data)with db.engine.connect() as connect: connect.execute(on_duplicate_key_stmt) 删除12session.delete(jack)session.query(User).filter_by(name=&#x27;jack&#x27;).count() 修改12345678910111213from sqlalchemy import update, bindparamstmt = update(User).where(User.c.name == &quot;iFan&quot;).values(fullname=&quot;iFanlw&quot;)stmt = update(user_table).where(user_table.c.name == bindparam(&quot;oldname&quot;)).values(name=bindparam(&quot;newname&quot;))with engine.begin() as conn: conn.execute( stmt, [ &#123;&quot;oldname&quot;: &quot;jack&quot;, &quot;newname&quot;: &quot;ed&quot;&#125;, &#123;&quot;oldname&quot;: &quot;wendy&quot;, &quot;newname&quot;: &quot;mary&quot;&#125;, &#123;&quot;oldname&quot;: &quot;jim&quot;, &quot;newname&quot;: &quot;jake&quot;&#125;, ], ) 查询1234567891011121314151617181920212223242526272829303132333435363738# 查询全部session.query(User).all()# 根据条件查询session.query(User).filter(User.name == &quot;iFan&quot;).all()session.query(User).filter(User.name != &quot;iFan&quot;).all()session.query(User).filter(User.name.like(&quot;%ifan%&quot;)).all() # 区分大小写session.query(User).filter(User.name.ilike(&quot;%ifan%&quot;)).all() # 不区分大小写session.query(User).filter(User.name.in_([&quot;ifan&quot;, &quot;ifan1&quot;])).all()session.query(User).filter(User.name.in_(session.query(User.name).filter(User.name.like(&quot;%ifan%&quot;)))).all()# 多个In条件组合from sqlalchemy import tuple_query.filter(tuple_(User.name, User.nickname).in_([(&#x27;ed&#x27;, &#x27;edsnickname&#x27;), (&#x27;wendy&#x27;, &#x27;windy&#x27;)]))# not in session.query.filter(~User.name.in_([&#x27;ed&#x27;, &#x27;wendy&#x27;, &#x27;jack&#x27;]))# is Nonesession.query(User).filter(User.name.is_not(None)).all()# andfrom sqlalchemy import and_session.query.filter(and_(User.name == &#x27;ed&#x27;, User.fullname == &#x27;Ed Jones&#x27;))# orfrom sqlalchemy import or_session.query.filter(or_(User.name == &#x27;ed&#x27;, User.name == &#x27;wendy&#x27;))# 存在则返回，否则返回Nonesession.query.filter(User.id == 1).one_or_none()# 使用文本的查询from sqlalchemy import textsession.query.filter(text(&quot;id = 1&quot;)).order_by(text(&quot;id&quot;).all()session.query(User).filter(text(&quot;id&lt;:value and name=:name&quot;)).params(value=224, name=&#x27;fred&#x27;).order_by(User.id).one()session.query(User).from_statement(text(&quot;SELECT * FROM users where name=:name&quot;)).params(name=&#x27;ed&#x27;).all()stmt = text(&quot;SELECT name, id, fullname, nickname FROM users where name=:name&quot;)stmt = stmt.columns(User.name, User.id, User.fullname, User.nickname)session.query(User).from_statement(stmt).params(name=&#x27;ed&#x27;).all()# 限制返回for user in session.query(User).filter(User.age == 18)[:18]: print(user.username, user.age) 分组查询123456# 分组查询from sqlalchemy import funcsession.query(func.count(User.name), User.name).group_by(User.name).all()# countsession.query(func.count(&#x27;*&#x27;)).select_from(User).scalar()session.query(func.count(User.id)).scalar()","categories":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/categories/Sqlalchemy/"},{"name":"ORM","slug":"Sqlalchemy/ORM","permalink":"https://blog.ifan.host/categories/Sqlalchemy/ORM/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/tags/Sqlalchemy/"},{"name":"ORM","slug":"ORM","permalink":"https://blog.ifan.host/tags/ORM/"}]},{"title":"Mac clashx 自动切换代理","slug":"tools/clashx自动切换代理","date":"2022-11-27T14:00:00.000Z","updated":"2023-12-25T05:56:09.182Z","comments":true,"path":"2022/11/27/tools/clashx自动切换代理/","link":"","permalink":"https://blog.ifan.host/2022/11/27/tools/clashx%E8%87%AA%E5%8A%A8%E5%88%87%E6%8D%A2%E4%BB%A3%E7%90%86/","excerpt":"","text":"设置自动切换代理 123456789101112proxy-groups:- name: &quot;auto&quot; type: url-test proxies: - ss1 - ss2 - vmess1 url: &#x27;http://www.gstatic.com/generate_204&#x27; interval: 300 #tolerance: 50 #lazy: true #disable-udp: true","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Mac","slug":"工具/Mac","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Mac/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Mac","slug":"Mac","permalink":"https://blog.ifan.host/tags/Mac/"}]},{"title":"Mac parallels 初始化网络失败","slug":"tools/mac/mac-parallels初始化网络失败","date":"2022-11-27T14:00:00.000Z","updated":"2023-12-25T05:56:09.185Z","comments":true,"path":"2022/11/27/tools/mac/mac-parallels初始化网络失败/","link":"","permalink":"https://blog.ifan.host/2022/11/27/tools/mac/mac-parallels%E5%88%9D%E5%A7%8B%E5%8C%96%E7%BD%91%E7%BB%9C%E5%A4%B1%E8%B4%A5/","excerpt":"","text":"1. parallels 初始化网络失败1sudo vim /Library/Preferences/Parallels/network.desktop.xml 将 &lt;UseKextless&gt;-1&lt;/UseKextless&gt; 改为 &lt;UseKextless&gt;0&lt;/UseKextless&gt; 2. 操作失败 在创建前选择【安装前设置】 选择【硬件】-&gt;【CPU和内存】-&gt;【高级】 选择【虚拟机监控程序】-&gt; 【Parallels】","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Mac","slug":"工具/Mac","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Mac/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Mac","slug":"Mac","permalink":"https://blog.ifan.host/tags/Mac/"}]},{"title":"Mac 关闭特定的软件随系统的颜色变化","slug":"tools/mac/mac-关闭特定软件的随系统颜色的变化","date":"2022-11-27T14:00:00.000Z","updated":"2023-12-25T05:56:09.185Z","comments":true,"path":"2022/11/27/tools/mac/mac-关闭特定软件的随系统颜色的变化/","link":"","permalink":"https://blog.ifan.host/2022/11/27/tools/mac/mac-%E5%85%B3%E9%97%AD%E7%89%B9%E5%AE%9A%E8%BD%AF%E4%BB%B6%E7%9A%84%E9%9A%8F%E7%B3%BB%E7%BB%9F%E9%A2%9C%E8%89%B2%E7%9A%84%E5%8F%98%E5%8C%96/","excerpt":"","text":"123osascript -e &#x27;id of app &quot;Mybase&quot;&#x27;defaults write com.wjjsoft.mybase8 NSRequiresAquaSystemAppearance -bool Yes","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Mac","slug":"工具/Mac","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Mac/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Mac","slug":"Mac","permalink":"https://blog.ifan.host/tags/Mac/"}]},{"title":"Mysql 统计数据库存储使用大小","slug":"database/mysql/mysql-统计存储空间","date":"2022-11-27T13:00:00.000Z","updated":"2023-12-25T05:56:09.166Z","comments":true,"path":"2022/11/27/database/mysql/mysql-统计存储空间/","link":"","permalink":"https://blog.ifan.host/2022/11/27/database/mysql/mysql-%E7%BB%9F%E8%AE%A1%E5%AD%98%E5%82%A8%E7%A9%BA%E9%97%B4/","excerpt":"","text":"前言在 mysql 中有一个默认的数据表 information_schema，information_schema 这张数据表保存了 MySQL 服务器所有数据库的信息。如数据库名，数据库的表，表栏的数据类型与访问权限等。 再简单点，这台 MySQL 服务器上，到底有哪些数据库、各个数据库有哪些表，每张表的字段类型是什么，各个数据库要什么权限才能访问，等等信息都保存在 information_schema 表里面，所以请勿删改此表。 代码 1，切换数据库 1use information_schema; 2，查看数据库使用大小 1select concat(round(sum(data_length/1024/1024),2),’MB’) as data from tables where table_schema=’DB_Name’ ; 3，查看表使用大小 1select concat(round(sum(data_length/1024/1024),2),’MB’) as data from tables where table_schema=’DB_Name’ and table_name=’Table_Name’; 网上找的一个，亲测可用: 先进去 MySQL 自带管理库：information_schema 然后查询 data_length,index_length 你自己的数据库名：dbname 你自己的表名：tablename 1234567891011121314151617181920212223mysql&gt; use information_schema; Database changed mysql&gt; select data_length,index_length -&gt; from tables where -&gt; table_schema=&#x27;dbname&#x27; -&gt; and table_name = &#x27;tablename&#x27;; +-------------+--------------+ | data_length | index_length | +-------------+--------------+ | 166379520 | 235782144 | +-------------+--------------+ row in set (0.02 sec) mysql&gt; select concat(round(sum(data_length/1024/1024),2),&#x27;MB&#x27;) as data_length_MB, -&gt; concat(round(sum(index_length/1024/1024),2),&#x27;MB&#x27;) as index_length_MB -&gt; from tables where -&gt; table_schema=&#x27;dbname&#x27; -&gt; and table_name = &#x27;tablename&#x27;; +----------------+-----------------+ | data_length_MB | index_length_MB | +----------------+-----------------+ | 158.67MB | 224.86MB | +----------------+-----------------+ row in set (0.03 sec) 1. 查看所有数据库容量大小12345678select table_schema as &#x27;数据库&#x27;, sum(table_rows) as &#x27;记录数&#x27;, sum(truncate(data_length/1024/1024, 2)) as &#x27;数据容量(MB)&#x27;, sum(truncate(index_length/1024/1024, 2)) as &#x27;索引容量(MB)&#x27; from information_schema.tables group by table_schema order by sum(data_length) desc, sum(index_length) desc; 2.查看所有数据库各表容量大小12345678select table_schema as &#x27;数据库&#x27;, table_name as &#x27;表名&#x27;, table_rows as &#x27;记录数&#x27;, truncate(data_length/1024/1024, 2) as &#x27;数据容量(MB)&#x27;, truncate(index_length/1024/1024, 2) as &#x27;索引容量(MB)&#x27; from information_schema.tables order by data_length desc, index_length desc; 3. 查看指定数据库容量大小例：查看 mysql 库容量大小 1234567select table_schema as &#x27;数据库&#x27;, sum(table_rows) as &#x27;记录数&#x27;, sum(truncate(data_length/1024/1024, 2)) as &#x27;数据容量(MB)&#x27;, sum(truncate(index_length/1024/1024, 2)) as &#x27;索引容量(MB)&#x27; from information_schema.tables where table_schema=&#x27;mysql&#x27;; 4. 查看指定数据库各表容量大小例：查看 mysql 库各表容量大小 123456789101112select table_schema as &#x27;数据库&#x27;, table_name as &#x27;表名&#x27;, table_rows as &#x27;记录数&#x27;, truncate(data_length/1024/1024, 2) as &#x27;数据容量(MB)&#x27;, truncate(index_length/1024/1024, 2) as &#x27;索引容量(MB)&#x27; from information_schema.tables where table_schema=&#x27;mysql&#x27; order by data_length desc, index_length desc; select concat(round(sum(data_length/1024/1024),2),&#x27;MB&#x27;) as data_length_MB, concat(round(sum(index_length/1024/1024),2),&#x27;MB&#x27;) as index_length_MB from tables where table_schema=&#x27;passport&#x27; and table_name=&#x27;tb_user_info&#x27;; – 569.98MB 141.98MB 123select concat(round(sum(data_length/1024/1024),2),&#x27;MB&#x27;) as data_length_MB, concat(round(sum(index_length/1024/1024),2),&#x27;MB&#x27;) as index_length_MB from tables where table_schema=&#x27;passport_v2&#x27; and table_name=&#x27;tb_user_info&#x27;;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"NodeJS http-proxy-middleware","slug":"language/javascript/libs/http-proxy-middleware","date":"2022-11-27T12:00:00.000Z","updated":"2023-12-25T05:56:09.175Z","comments":true,"path":"2022/11/27/language/javascript/libs/http-proxy-middleware/","link":"","permalink":"https://blog.ifan.host/2022/11/27/language/javascript/libs/http-proxy-middleware/","excerpt":"","text":"http-proxy-middleware用于把请求代理转发到其他服务器的中间件 1npm install --save-dev http-proxy-middleware 123456789101112131415161718192021222324252627// 引用依赖var express = require(&#x27;express&#x27;);var proxy = require(&#x27;http-proxy-middleware&#x27;);// proxy 中间件的选择项var options = &#123; target: &#x27;http://www.example.org&#x27;, // 目标服务器 host changeOrigin: true, // 默认false，是否需要改变原始主机头为目标URL ws: true, // 是否代理websockets pathRewrite: &#123; &#x27;^/api/old-path&#x27; : &#x27;/api/new-path&#x27;, // 重写请求，比如我们源访问的是api/old-path，那么请求会被解析为/api/new-path &#x27;^/api/remove/path&#x27; : &#x27;/path&#x27; // 同上 &#125;, router: &#123; // 如果请求主机 == &#x27;dev.localhost:3000&#x27;, // 重写目标服务器 &#x27;http://www.example.org&#x27; 为 &#x27;http://localhost:8000&#x27; &#x27;dev.localhost:3000&#x27; : &#x27;http://localhost:8000&#x27; &#125; &#125;;// 创建代理var exampleProxy = proxy(options);// 使用代理var app = express(); app.use(&#x27;/api&#x27;, exampleProxy); app.listen(3000); 路径匹配 proxy(&#123;...&#125;): 匹配任何路径，将所有请求转发 proxy(&#123;&#39;/&#39;, &#123;...&#125;&#125;): 匹配任何路径，将所有请求转发 proxy(&#123;&#39;/api&#39;, &#123;...&#125;&#125;): 匹配 /api 开头的请求 proxy([&#39;/api&#39;, &#39;/ajax&#39;], &#123;....&#125;): 匹配多个路径转发 proxy(&#39;**&#39;, &#123;...&#125;): 匹配任何路径，将所有请求转发 proxy(&#39;**/*.html&#39;, &#123;...&#125;): 匹配任何以html结尾的 proxy(&#39;/api/**/*.html&#39;, &#123;...&#125;): 匹配 /api 下以 html 结尾的请求 proxy([&#39;/api/**&#39;, &#39;/ajax/**&#39;], &#123;...&#125;): 组合 proxy([&#39;/api/**&#39;, &#39;!**/bad.json&#39;], &#123;...&#125;): 不包括 **/bad.json 1234var filter = function (pathname, req) &#123; return (pathname.match(&#x27;^/api&#x27;) &amp;&amp; req.method === &#x27;GET&#x27;);&#125;;var apiProxy = proxy(filter, &#123;target: &#x27;http://www.example.org&#x27;&#125;) 重写请求pathRewrite 重写目标url请求1234567891011// 重写pathRewrite: &#123;&#x27;^/old/api&#x27; : &#x27;/new/api&#x27;&#125;// 移除pathRewrite: &#123;&#x27;^/remove/api&#x27; : &#x27;&#x27;&#125;// 添加pathRewrite: &#123;&#x27;^/&#x27; : &#x27;/basepath/&#x27;&#125;// 自定义pathRewrite: function (path, req) &#123; return path.replace(&#x27;/api&#x27;, &#x27;/base/api&#x27;) &#125; router 重新指定请求转发目标12345678910111213// 使用主机或者路径进行匹配，返回最先匹配到结果// 所以配置的顺序很重要router: &#123; &#x27;integration.localhost:3000&#x27; : &#x27;http://localhost:8001&#x27;, // host only &#x27;staging.localhost:3000&#x27; : &#x27;http://localhost:8002&#x27;, // host only &#x27;localhost:3000/api&#x27; : &#x27;http://localhost:8003&#x27;, // host + path &#x27;/rest&#x27; : &#x27;http://localhost:8004&#x27; // path only&#125;// 自定义router: function(req) &#123; return &#x27;http://localhost:8004&#x27;;&#125; http proxyonError12345678// 监听proxy的onerr事件proxy.on(&#x27;error&#x27;, function (err, req, res) &#123; res.writeHead(500, &#123; &#x27;Content-Type&#x27;: &#x27;text/plain&#x27; &#125;); res.end(&#x27;Something went wrong. And we are reporting a custom error message.&#x27;);&#125;); onProxyRes 监听proxy的回应事件123proxy.on(&#x27;proxyRes&#x27;, function (proxyRes, req, res) &#123; console.log(&#x27;RAW Response from the target&#x27;, JSON.stringify(proxyRes.headers, true, 2));&#125;); onProxyReq: 监听proxy的请求事件123proxy.on(&#x27;proxyReq&#x27;, function onProxyReq(proxyReq, req, res) &#123; proxyReq.setHeader(&#x27;x-added&#x27;, &#x27;foobar&#x27;);&#125;); onProxyReqWs123function onProxyReqWs(proxyReq, req, socket, options, head) &#123; proxyReq.setHeader(&#x27;X-Special-Proxy-Header&#x27;, &#x27;foobar&#x27;);&#125; onOpen: 监听来自目标服务器的信息123proxy.on(&#x27;open&#x27;, function (proxySocket) &#123; proxySocket.on(&#x27;data&#x27;, hybiParseAndLogMessage);&#125;); onClose: 链接断开123proxy.on(&#x27;close&#x27;, function (res, socket, head) &#123; console.log(&#x27;Client disconnected&#x27;);&#125;);","categories":[{"name":"NodeJS","slug":"NodeJS","permalink":"https://blog.ifan.host/categories/NodeJS/"}],"tags":[{"name":"NodeJS","slug":"NodeJS","permalink":"https://blog.ifan.host/tags/NodeJS/"}]},{"title":"Frida 学习","slug":"spider/frida","date":"2022-08-11T06:10:00.000Z","updated":"2023-12-25T05:56:09.178Z","comments":true,"path":"2022/08/11/spider/frida/","link":"","permalink":"https://blog.ifan.host/2022/08/11/spider/frida/","excerpt":"","text":"Frida 学习笔记环境开发查看安卓手机的版本 1adb shell getprop ro.product.cpu.abi 安装Python环境和frida-server 12345678910pip install frida==15.0.11pip install frida-tools==10.2.1pip install objectionwget https://github.com/frida/frida/releases?page=7#:~:text=frida%2Dserver%2D15.0.11%2Dandroid%2Darm64.xzadb push frida-server-15.0.11 /data/local/tmp/adb shell su root cd /data/local/tmpchmod +x frida-server-15.0.11./frida-server-15.0.11 -l 0.0.0.0:12345 frida 基础一些函数签名 Field Descriptor Java Language Type Z boolean B byte C char S short I int J long F float D double [ array 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114// 修改静态参数和非静态参数// 如果类中 成员函数 和 成员变量名称相同，则成员变量前添加 _Java.perform(function () &#123; console.log(&quot;start&quot;) var FridaActivity2 = Java.use(&quot;com.example.androiddemo.Activity.FridaActivity2&quot;) //hook静态函数直接调用 FridaActivity2.setStatic_bool_var() //hook动态函数，找到instance实例，从实例调用函数方法 Java.choose(&quot;com.example.androiddemo.Activity.FridaActivity2&quot;, &#123; onMatch: function (instance) &#123; instance.setBool_var() &#125;, onComplete: function () &#123; console.log(&quot;end&quot;) &#125; &#125;)&#125;)// Hook 内部类var InnerClasses = Java.use(&quot;com.example.androiddemo.Activity.FridaActivity4$InnerClasses&quot;)// 修改默认的Java LoaderJava.enumerateClassLoaders(&#123; onMatch: function (loader) &#123; try &#123; // 查看当前Loader中是否含有该类 if(loader.findClass(&quot;com.example.androiddemo.Dynamic.DynamicCheck&quot;))&#123; console.log(&quot;Successfully found loader&quot;) console.log(loader); // 如果存在则设置为默认的Loader Java.classFactory.loader = loader ; &#125; &#125; catch(error)&#123; console.log(&quot;find error:&quot; + error) &#125; &#125;, onComplete: function () &#123; console.log(&quot;end1&quot;) &#125;&#125;)Java.use(&quot;com.example.androiddemo.Dynamic.DynamicCheck&quot;).check.implementation = function () &#123; return true&#125;// 遍历所有静态类Java.enumerateLoadedClasses(&#123; onMatch: function (name, handle)&#123; if (name.indexOf(&quot;com.example.androiddemo.Activity.Frida6&quot;) != -1) &#123; console.log(&quot;name:&quot; + name + &quot; handle:&quot; + handle) Java.use(name).check.implementation = function () &#123; return true &#125; &#125; &#125;, onComplete: function () &#123; console.log(&quot;end&quot;) &#125;&#125;)// 搜索interface的具体实现类 利用反射得到类里面实现的interface数组，并打印出来。Java.enumerateLoadedClasses(&#123; onMatch: function (class_name)&#123; if (class_name.indexOf(&quot;com.example.androiddemo&quot;) &lt; 0) &#123; return &#125; else &#123; var hook_cls = Java.use(class_name) var interfaces = hook_cls.class.getInterfaces() if (interfaces.length &gt; 0) &#123; console.log(class_name + &quot;: &quot;) for (var i in interfaces) &#123; console.log(&quot;\\t&quot;, interfaces[i].toString()) &#125; &#125; &#125; &#125;, onComplete: function () &#123; console.log(&quot;end&quot;) &#125;&#125;)// Hook 构造函数Java.use(&quot;com.tlamb96.kgbmessenger.b.a&quot;).$init.implementation = function (i, str1, str2, z) &#123; this.$init(i, str1, str2, z) console.log(i, str1, str2, z) printStack(&quot;com.tlamb96.kgbmessenger.b.a&quot;)&#125;// 强制转换Java.cast(Childer, Java.use(&quot;com.androidapp.Father&quot;))// 新建Classvar interfaceClass = Java.use(&quot;com.androidapp.InterfaceClazz&quot;);var newClass = Java.registerClass(&#123; name: &#x27;com.androidapp.newClass&#x27;, implements: [interfaceClass], methods: &#123; flow: function () &#123; return &quot;OK&quot;; &#125; &#125;&#125;);console.log(newClass.$new().flow()) // 枚举类Java.choose(&quot;com.exampleapp.Signal&quot;,&#123; onMatch:function(instance)&#123; console.log(&quot;instance.name:&quot;,instance.name()); console.log(&quot;instance.getDeclaringClass:&quot;,instance.getDeclaringClass()); &#125;,onComplete:function()&#123; console.log(&quot;search completed!&quot;) &#125;&#125;) 外部调用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import fridaimport sysimport osclass HookManager: def __init__(self, arg, Flag): self.arg = arg self.js_code = &#x27;&#x27; self.JS_FILE = &#x27;jscode/&#x27; self.load_js() if flag: self.log = open(&#x27;log/&#x27; + arg + &#x27;.txt&#x27;, &#x27;w+&#x27;, encoding=&#x27;utf-8&#x27;) def load_js(self): file_names = os.listdir(self.JS_FILE) for filename in file_names: print(&#x27;载入：&#x27; + filename) with open(self.JS_FILE + filename, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f: self.js_code += f.read() print(&#x27;\\n&#x27;) def message(self, msg, data): if msg[&quot;type&quot;] == &#x27;send&#x27;: print(u&quot;[*] &#123;0&#125;&quot;.format(msg[&#x27;payload&#x27;])) if not self.log: return self.log.write(u&quot;[*] &#123;0&#125;\\n&quot;.format(msg[&#x27;payload&#x27;])) self.log.flush() else: print(msg) def hooking(self, host): try: process = frida.get_device_manager().add_remote_device(host).attach(self.arg) # one devices use # process = frida.get_remote_device(host).attach(self.arg) script = process.create_script(self.js_code) script.on(&quot;message&quot;, self.message) script.load() sys.stdin.read() except frida.ServerNotRunningError: print(&#x27;没有开启对应app， 或没有开启映射端口&#x27;) def close(self): self.log.close()if __name__ == &#x27;__main__&#x27;: bak = &quot;com.example.test&quot; host_list = [ &quot;192.168.191.2:9898&quot;, ] flag = True if len(host_list) ==1 else flag == False # 只有一台机子的时候默认是调试模式， 多台就是控制模式 hk = HookManager(bak, flag) for host in host_list: hk.hooking(host) hk.close() frida-tools 方法1frida-ps -U # 查看当前存在的进程 12345678910111213141. 脱离PC使用Frida的几种方式 a) 在手机上使用Termux终端 b) frida-inject c) frida-gadget.so 优点：可以免root使用frida、frida-gadget比较稳定 缺点：需要重打包app，局限性较大。但是我们可以通过魔改系统，让系统帮我们注入so，免去重打包的繁琐2. frida-inject的配置、选项和使用 -f、-p、-n、-s、-e3. 脱离pc后，如何判断hook是否生效 a) 写文件，然后查看文件内容 b) 主动调用Log的方法输出信息，然后使用logcat查看 c) 修改某些方法逻辑 objection123456789101112# 启动并注入内存objection -g xxx.xxx.xxx explore -P /Users/username/.objection/plugins启动前就hookobjection -N -h &lt;ip&gt; -p &lt;port&gt; -g &lt;进程名&gt; explore --startup-command &quot;android hooking watch class &lt;路径.类名&gt;&quot;启动前就hook打印参数、返回值、函数调用栈objection -N -h &lt;ip&gt; -p &lt;port&gt; -g &lt;进程名&gt; explore -s &quot;android hooking watch class_method &lt;路径.类名.方法名&gt; --dump-args --dump-return --dump-backtrace&quot;如果启动前需要运行多条命令，可以写到一个文件中，使用-c选项objection -g &lt;进程名&gt; explore -c &quot;路径&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 查看当前加载的modulememory list modules# 查看库的导出函数memory list exports libssl.so# 内存搜索 memory search # 列出内存中所有的类 android hooking list classes# 在内存中所有已加载的类的方法中搜索包含特定关键词的方法 android hooking search methods display# 内存中搜索指定类的所有方法 android hooking list class_methods 类名# 在内存中所有已加载的类中搜索包含特定关键词的类。 android hooking search classes display# 搜索堆中的实例android heap search instances &lt;类名&gt;# Hook该类的全部方法 打印入参 调用栈 返回android hooking watch class 类名 --dump-args --dump-backtrace --dump-return# Hook具体的方法的所有重载方法android hooking watch class_method 方法名# 调用实例的方法 android heap execute 实例ID 实例方法# 直接启动activity或者服务 android intent launch_activity/launch_service activity/服务# 查看当前可用的activity或者service android hooking list activities/services# 内存堆搜索实例 android heap search instances 类名# 通过实例调用静态和实例方法android heap execute &lt;handle&gt; &lt;方法名&gt;# 打印所有 activities （页面）android hooking list activities 查看与取消hookjobs listjobs kill &lt;jobId&gt;# 关闭ssl校验 android sslpinning disable# 关闭root检测android root disable# 直接生成该类的Hook方法android hooking generate simple com.android.settings.DisplaySettings 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107！ # 执行操作系统的命令(注意:不是在所连接device上执行命令)android # 执行指定的 Android 命令 clipboard monitor deoptimize # Force the VM to execute everything in the interpreter heap evaluate # 在 Java 类中执行 JavaScript 脚本。 execute # 在 Java 类中执行 方法。android heap execute 实例ID 实例方法 print search instances # 在当前Android heap上搜索类的实例。android heap search instances 类名 hooking generate class # A generic hook manager for Classes simple # Simple hooks for each Class method get current_activity # 获取当前 前景(foregrounded) activity list activities # 列出已经登记的 Activities class_loaders # 列出已经登记的 class loaders class_methods # 列出一个类上的可用的方法 classes # 列出当前载入的所有类 receivers # 列出已经登记的 BroadcastReceivers services # 列出已经登记的 Services search classes 关键字 # 搜索与名称匹配的Java类 methods 关键字 # 搜索与名称匹配的Java方法 set return_value # 设置一个方法的返回值。只支持布尔返回 watch class # Watches for invocations of all methods in a class class_method # Watches for invocations of a specific class method intent launch_activity # 使用Intent启动Activity类 launch_service # Launch a Service class using an Intent keystore clear # 清除 Android KeyStore list # 列出 Android KeyStore 中的条目 watch # 监视 Android KeyStore 的使用 proxy set # 为应用程序设置代理 root disable # 试图禁用 root 检测 simulate # 试图模拟已经 root 的环境 shell_exec # 执行shell命令 sslpinning disable # 尝试禁用 SSL pinning 在各种 Java libraries/classes ui FLAG_SECURE # Control FLAG_SECURE of the current Activity screenshot # 在当前 Activity 进行截图cd # 改变当前工作目录commands clear # 清除当前会话命令的历史记录 history # 列出当前会话命令历史记录 save # 将在此会话中运行的所有惟一命令保存到一个文件中env # 打印环境信息evaluate # 执行 JavaScript。( Evaluate JavaScript within the agent )exit # 退出file cat # 打印文件内容 download # 下载一个文件 http start # Start&#x27;s an HTTP server in the current working directory status # Get the status of the HTTP server stop # Stop&#x27;s a running HTTP server upload # 上传一个文件frida # 获取关于 frida 环境的信息import # 从完整路径导入 frida 脚本并运行ios 执行指定的 ios 命令 bundles cookies heap hooking info jailbreak keychain monitor nsurlcredentialstorage nsuserdefaults pasteboard plist sslpinning uijobs kill # 结束一个任务。这个操作不会写在卸载或者退出当前脚本 list # 列出当前所有的任务ls # 列出当前工作目录下的文件memory dump all 文件名 # Dump 当前进程的整个内存 from_base 起始地址 字节数 文件 # 将(x)个字节的内存从基址转储到文件 list exports # List the exports of a module. (列出模块的导出) modules # List loaded modules in the current process. (列出当前进程中已加载的模块) search # 搜索模块。用法：memory search &quot;&lt;pattern eg: 41 41 41 ?? 41&gt;&quot; (--string) (--offsets-only) write # 将原始字节写入内存地址。小心使用!ping # ping agentplugin load # 载入插接pwd # 打印当前工作目录reconnect # 重新连接 devicerm # 从 device 上删除文件 sqlite # sqlite 数据库命令 connect # 连接到SQLite数据库文件ui alert # 显示警报消息，可选地指定要显示的消息。(目前iOS崩溃) wallbreaker12cd ~/.objection/pluginsgit clone https://github.com/hluwa/Wallbreaker.git 123456789# 搜索类plugin wallbreaker classsearch &lt;pattern&gt;# 搜索对象plugin wallbreaker objectsearch &lt;classname&gt;# 根据类名搜索内存中已经被创建的实例，列出 handle 和 toString() 的结果。plugin wallbreaker classdump &lt;classname&gt; [--fullname]plugin wallbreaker objectdump &lt;handle&gt; [--fullname] frida-dexdump1pip3 install frida-dexdump 123456frida-dexdump -FUfrida-dexdump -U -f com.app.pkgname -o OUTPUT, --output OUTPUT Output folder path, default is &#x27;./&lt;appname&gt;/&#x27;. -d, --deep-search Enable deep search mode. --sleep SLEEP Waiting times for start, spawn mode default is 5s. 工具方法打印调用栈1234567891011121314151617181920212223242526272829303132333435363738394041function printStack1(name) &#123; Java.perform(function () &#123; var Exception = Java.use(&quot;java.lang.Exception&quot;); var ins = Exception.$new(&quot;Exception&quot;); var straces = ins.getStackTrace(); if (straces != undefined &amp;&amp; straces != null) &#123; var strace = straces.toString(); var replaceStr = strace.replace(/,/g, &quot;\\\\n&quot;); console.log(&quot;=============================&quot; + name + &quot; Stack strat=======================&quot;); console.log(replaceStr); console.log(&quot;=============================&quot; + name + &quot; Stack end=======================\\r\\n&quot;); Exception.$dispose(); &#125; &#125;);&#125;function printStack2(str_tag) &#123; var Exception= Java.use(&quot;java.lang.Exception&quot;); var ins = Exception.$new(&quot;Exception&quot;); var straces = ins.getStackTrace(); if (undefined == straces || null == straces) &#123; return; &#125; console.log(&quot;=============================&quot; + str_tag + &quot; Stack strat=======================&quot;); console.log(&quot;&quot;); for (var i = 0; i &lt; straces.length; i++) &#123; var str = &quot; &quot; + straces[i].toString(); console.log(str); &#125; console.log(&quot;&quot;); console.log(&quot;=============================&quot; + str_tag + &quot; Stack end=======================\\r\\n&quot;); Exception.$dispose();&#125; byte2String1234function byteToString(bytes)&#123; const JString = Java.use(&#x27;java.lang.String&#x27;); return JString.$new(bytes);&#125; Hook Md5 aes base64 等123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210//frida hook aes function hook_cipher()&#123; var cipher = Java.use(&quot;javax.crypto.Cipher&quot;); cipher.doFinal.implementation = function(data, offset, length)&#123; var ret = this.doFinal(data, offset, length); console.log(&quot;doFinal: &quot; + ret); return ret; &#125;&#125;function hook_aes()&#123; var aes = Java.use(&quot;javax.crypto.spec.SecretKeySpec&quot;); aes.getEncoded.implementation = function()&#123; var ret = this.getEncoded(); console.log(&quot;getEncoded: &quot; + ret); return ret; &#125;&#125;function hook_base64()&#123; var base64 = Java.use(&quot;android.util.Base64&quot;); base64.encodeToString.implementation = function(data, flags)&#123; var ret = this.encodeToString(data, flags); console.log(&quot;encodeToString: &quot; + ret); return ret; &#125;&#125;function hook_md5()&#123; var md5 = Java.use(&quot;java.security.MessageDigest&quot;); md5.digest.implementation = function(data)&#123; var ret = this.digest(data); console.log(&quot;digest: &quot; + ret); return ret; &#125;&#125;function hook_sha1()&#123; var sha1 = Java.use(&quot;java.security.MessageDigest&quot;); sha1.digest.implementation = function(data)&#123; var ret = this.digest(data); console.log(&quot;digest: &quot; + ret); return ret; &#125;&#125;function hook_sha256()&#123; var sha256 = Java.use(&quot;java.security.MessageDigest&quot;); sha256.digest.implementation = function(data)&#123; var ret = this.digest(data); console.log(&quot;digest: &quot; + ret); return ret; &#125;&#125;function hook_sha512()&#123; var sha512 = Java.use(&quot;java.security.MessageDigest&quot;); sha512.digest.implementation = function(data)&#123; var ret = this.digest(data); console.log(&quot;digest: &quot; + ret); return ret; &#125;&#125;function hook_hmac()&#123; var hmac = Java.use(&quot;javax.crypto.Mac&quot;); hmac.doFinal.implementation = function(data)&#123; var ret = this.doFinal(data); console.log(&quot;doFinal: &quot; + ret); return ret; &#125;&#125;function hook_rsa()&#123; var rsa = Java.use(&quot;java.security.interfaces.RSAPublicKey&quot;); rsa.getEncoded.implementation = function()&#123; var ret = this.getEncoded(); console.log(&quot;getEncoded: &quot; + ret); return ret; &#125;&#125;function hook_map()&#123; var map = Java.use(&quot;java.util.HashMap&quot;); map.put.implementation = function(key, value)&#123; var ret = this.put(key, value); console.log(&quot;put: &quot; + ret); return ret; &#125; &#125;function hook_list()&#123; var list = Java.use(&quot;java.util.ArrayList&quot;); list.add.implementation = function(value)&#123; var ret = this.add(value); console.log(&quot;add: &quot; + ret); return ret; &#125;&#125;function hook_perform_click()&#123; var click = Java.use(&quot;android.view.View&quot;); click.performClick.implementation = function()&#123; var ret = this.performClick(); console.log(&quot;performClick: &quot; + ret); return ret; &#125;&#125;function hook_set_text()&#123; var text = Java.use(&quot;android.widget.TextView&quot;); text.setText.implementation = function(text)&#123; var ret = this.setText(text); console.log(&quot;setText: &quot; + ret); return ret; &#125;&#125;function hook_get_text()&#123; var text = Java.use(&quot;android.widget.TextView&quot;); text.getText.implementation = function()&#123; var ret = this.getText(); console.log(&quot;getText: &quot; + ret); return ret; &#125; &#125;function hook_get_package_name()&#123; var package_name = Java.use(&quot;android.content.Context&quot;); package_name.getPackageName.implementation = function()&#123; var ret = this.getPackageName(); console.log(&quot;getPackageName: &quot; + ret); return ret; &#125;&#125;//frida byte to string function hook_byte_to_string()&#123; var byte = Java.use(&quot;[B&quot;); byte.toString.implementation = function()&#123; var ret = this.toString(); console.log(&quot;toString: &quot; + ret); return ret; &#125;&#125;//frida string to bytefunction hook_string_to_byte()&#123; var string = Java.use(&quot;java.lang.String&quot;); string.getBytes.implementation = function()&#123; var ret = this.getBytes(); console.log(&quot;getBytes: &quot; + ret); return ret; &#125;&#125;function hook_response()&#123; var response = Java.use(&quot;okhttp3.Response&quot;); response.body.implementation = function()&#123; var ret = this.body(); console.log(&quot;body: &quot; + ret); return ret; &#125;&#125;function hook_request_url()&#123; var request = Java.use(&quot;okhttp3.Request&quot;); request.url.implementation = function()&#123; var ret = this.url(); console.log(&quot;url: &quot; + ret); return ret; &#125;&#125;function hook_url()&#123; var url = Java.use(&quot;java.net.URL&quot;); url.toString.implementation = function()&#123; var ret = this.toString(); console.log(&quot;toString: &quot; + ret); return ret; &#125;&#125;function hook_url_connection()&#123; var url_connection = Java.use(&quot;java.net.URLConnection&quot;); url_connection.getInputStream.implementation = function()&#123; var ret = this.getInputStream(); console.log(&quot;getInputStream: &quot; + ret); return ret; &#125;&#125;function hook_output_stream()&#123; var output_stream = Java.use(&quot;java.io.OutputStream&quot;); output_stream.write.implementation = function(data)&#123; var ret = this.write(data); console.log(&quot;write: &quot; + ret); return ret; &#125;&#125;;function hook_loadLibrary()&#123; var lib = Java.use(&quot;java.lang.System&quot;); lib.loadLibrary.implementation = function(name)&#123; var ret = this.loadLibrary(name); console.log(&quot;loadLibrary: &quot; + ret); return ret; &#125; var linker = Java.use(&quot;dalvik.system.DexClassLoader&quot;); linker.loadClass.implementation = function(name)&#123; var ret = this.loadClass(name); console.log(&quot;loadClass: &quot; + ret); return ret; &#125;&#125;function libart_hook()&#123; var libart = Java.use(&quot;libart.DexFile&quot;); libart.openDexFile.implementation = function(path,name,flags)&#123; var ret = this.openDexFile(path,name,flags); console.log(&quot;openDexFile: &quot; + ret); return ret; &#125; var libart_native = Java.use(&quot;libart.DexFile.Native&quot;); libart_native.dexFileOpen.implementation = function(path,name,flags)&#123; var ret = this.dexFileOpen(path,name,flags); console.log(&quot;dexFileOpen: &quot; + ret); return ret; &#125;&#125; 自吐算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381# -*- coding: UTF-8 -*-import frida, sys jsCode = &quot;&quot;&quot; function showStacks() &#123; Java.perform(function () &#123; send(Java.use(&quot;android.util.Log&quot;).getStackTraceString(Java.use(&quot;java.lang.Exception&quot;).$new())); &#125;); &#125;(function () &#123; var base64EncodeChars = &#x27;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/&#x27;, base64DecodeChars = new Array((-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), (-1), 62, (-1), (-1), (-1), 63, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, (-1), (-1), (-1), (-1), (-1), (-1), (-1), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, (-1), (-1), (-1), (-1), (-1), (-1), 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, (-1), (-1), (-1), (-1), (-1)); this.stringToBase64 = function (e) &#123; var r,a,c,h,o,t; for (c = e.length, a = 0, r = &#x27;&#x27;; a &lt; c; ) &#123; if (h = 255 &amp; e.charCodeAt(a++), a == c) &#123; r += base64EncodeChars.charAt(h &gt;&gt; 2), r += base64EncodeChars.charAt((3 &amp; h) &lt;&lt; 4), r += &#x27;==&#x27;; break &#125; if (o = e.charCodeAt(a++), a == c) &#123; r += base64EncodeChars.charAt(h &gt;&gt; 2), r += base64EncodeChars.charAt((3 &amp; h) &lt;&lt; 4 | (240 &amp; o) &gt;&gt; 4), r += base64EncodeChars.charAt((15 &amp; o) &lt;&lt; 2), r += &#x27;=&#x27;; break &#125; t = e.charCodeAt(a++), r += base64EncodeChars.charAt(h &gt;&gt; 2), r += base64EncodeChars.charAt((3 &amp; h) &lt;&lt; 4 | (240 &amp; o) &gt;&gt; 4), r += base64EncodeChars.charAt((15 &amp; o) &lt;&lt; 2 | (192 &amp; t) &gt;&gt; 6), r += base64EncodeChars.charAt(63 &amp; t) &#125; return r &#125; this.base64ToString = function (e) &#123; var r,a,c,h,o,t,d; for (t = e.length, o = 0, d = &#x27;&#x27;; o &lt; t; ) &#123; do r = base64DecodeChars[255 &amp; e.charCodeAt(o++)]; while (o &lt; t &amp;&amp; r == -1); if (r == -1) break; do a = base64DecodeChars[255 &amp; e.charCodeAt(o++)]; while (o &lt; t &amp;&amp; a == -1); if (a == -1) break; d += String.fromCharCode(r &lt;&lt; 2 | (48 &amp; a) &gt;&gt; 4); do &#123; if (c = 255 &amp; e.charCodeAt(o++), 61 == c) return d; c = base64DecodeChars[c] &#125; while (o &lt; t &amp;&amp; c == -1); if (c == -1) break; d += String.fromCharCode((15 &amp; a) &lt;&lt; 4 | (60 &amp; c) &gt;&gt; 2); do &#123; if (h = 255 &amp; e.charCodeAt(o++), 61 == h) return d; h = base64DecodeChars[h] &#125; while (o &lt; t &amp;&amp; h == -1); if (h == -1) break; d += String.fromCharCode((3 &amp; c) &lt;&lt; 6 | h) &#125; return d &#125; this.hexToBase64 = function (str) &#123; return base64Encode(String.fromCharCode.apply(null, str.replace(/\\r|\\n/g, &quot;&quot;).replace(/([\\da-fA-F]&#123;2&#125;) ?/g, &quot;0x$1 &quot;).replace(/ +$/, &quot;&quot;).split(&quot; &quot;))); &#125; this.base64ToHex = function (str) &#123; for (var i = 0, bin = base64Decode(str.replace(/[ \\r\\n]+$/, &quot;&quot;)), hex = []; i &lt; bin.length; ++i) &#123; var tmp = bin.charCodeAt(i).toString(16); if (tmp.length === 1) tmp = &quot;0&quot; + tmp; hex[hex.length] = tmp; &#125; return hex.join(&quot;&quot;); &#125; this.hexToBytes = function (str) &#123; var pos = 0; var len = str.length; if (len % 2 != 0) &#123; return null; &#125; len /= 2; var hexA = new Array(); for (var i = 0; i &lt; len; i++) &#123; var s = str.substr(pos, 2); var v = parseInt(s, 16); hexA.push(v); pos += 2; &#125; return hexA; &#125; this.bytesToHex = function (arr) &#123; var str = &#x27;&#x27;; var k,j; for(var i = 0; i&lt;arr.length; i++) &#123; k = arr[i]; j = k; if (k &lt; 0) &#123; j = k + 256; &#125; if (j &lt; 16) &#123; str += &quot;0&quot;; &#125; str += j.toString(16); &#125; return str; &#125; this.stringToHex = function (str) &#123; var val = &quot;&quot;; for (var i = 0; i &lt; str.length; i++) &#123; if (val == &quot;&quot;) val = str.charCodeAt(i).toString(16); else val += str.charCodeAt(i).toString(16); &#125; return val &#125; this.stringToBytes = function (str) &#123; var ch, st, re = []; for (var i = 0; i &lt; str.length; i++ ) &#123; ch = str.charCodeAt(i); st = []; do &#123; st.push( ch &amp; 0xFF ); ch = ch &gt;&gt; 8; &#125; while ( ch ); re = re.concat( st.reverse() ); &#125; return re; &#125; //将byte[]转成String的方法 this.bytesToString = function (arr) &#123; var str = &#x27;&#x27;; arr = new Uint8Array(arr); for(i in arr)&#123; str += String.fromCharCode(arr[i]); &#125; return str; &#125; this.bytesToBase64=function(e)&#123; var r,a,c,h,o,t; for (c = e.length, a = 0, r = &#x27;&#x27;; a &lt; c; ) &#123; if (h = 255 &amp; e[a++], a == c) &#123; r += base64EncodeChars.charAt(h &gt;&gt; 2), r += base64EncodeChars.charAt((3 &amp; h) &lt;&lt; 4), r += &#x27;==&#x27;; break &#125; if (o = e[a++], a == c) &#123; r += base64EncodeChars.charAt(h &gt;&gt; 2), r += base64EncodeChars.charAt((3 &amp; h) &lt;&lt; 4 | (240 &amp; o) &gt;&gt; 4), r += base64EncodeChars.charAt((15 &amp; o) &lt;&lt; 2), r += &#x27;=&#x27;; break &#125; t = e[a++], r += base64EncodeChars.charAt(h &gt;&gt; 2), r += base64EncodeChars.charAt((3 &amp; h) &lt;&lt; 4 | (240 &amp; o) &gt;&gt; 4), r += base64EncodeChars.charAt((15 &amp; o) &lt;&lt; 2 | (192 &amp; t) &gt;&gt; 6), r += base64EncodeChars.charAt(63 &amp; t) &#125; return r &#125; this.base64ToBytes=function(e)&#123; var r,a,c,h,o,t,d; for (t = e.length, o = 0, d = []; o &lt; t; ) &#123; do r = base64DecodeChars[255 &amp; e.charCodeAt(o++)]; while (o &lt; t &amp;&amp; r == -1); if (r == -1) break; do a = base64DecodeChars[255 &amp; e.charCodeAt(o++)]; while (o &lt; t &amp;&amp; a == -1); if (a == -1) break; d.push(r &lt;&lt; 2 | (48 &amp; a) &gt;&gt; 4); do &#123; if (c = 255 &amp; e.charCodeAt(o++), 61 == c) return d; c = base64DecodeChars[c] &#125; while (o &lt; t &amp;&amp; c == -1); if (c == -1) break; d.push((15 &amp; a) &lt;&lt; 4 | (60 &amp; c) &gt;&gt; 2); do &#123; if (h = 255 &amp; e.charCodeAt(o++), 61 == h) return d; h = base64DecodeChars[h] &#125; while (o &lt; t &amp;&amp; h == -1); if (h == -1) break; d.push((3 &amp; c) &lt;&lt; 6 | h) &#125; return d &#125;&#125;)();//stringToBase64 stringToHex stringToBytes//base64ToString base64ToHex base64ToBytes// hexToBase64 hexToBytes // bytesToBase64 bytesToHex bytesToStringJava.perform(function () &#123; var secretKeySpec = Java.use(&#x27;javax.crypto.spec.SecretKeySpec&#x27;); secretKeySpec.$init.overload(&#x27;[B&#x27;,&#x27;java.lang.String&#x27;).implementation = function (a,b) &#123; showStacks(); var result = this.$init(a, b); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + b + &quot;|Dec密钥:&quot; + bytesToString(a)); send(&quot;算法名：&quot; + b + &quot;|Hex密钥:&quot; + bytesToHex(a)); return result; &#125; var mac = Java.use(&#x27;javax.crypto.Mac&#x27;); mac.getInstance.overload(&#x27;java.lang.String&#x27;).implementation = function (a) &#123; showStacks(); var result = this.getInstance(a); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + a); return result; &#125; mac.update.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); this.update(a); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a)) &#125; mac.update.overload(&#x27;[B&#x27;,&#x27;int&#x27;,&#x27;int&#x27;).implementation = function (a,b,c) &#123; showStacks(); this.update(a,b,c) send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a) + &quot;|&quot; + b + &quot;|&quot; + c); &#125; mac.doFinal.overload().implementation = function () &#123; showStacks(); var result = this.doFinal(); send(&quot;======================================&quot;); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; mac.doFinal.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.doFinal(a); send(&quot;======================================&quot;); send(&quot;doFinal参数:&quot; + bytesToString(a)); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; var md = Java.use(&#x27;java.security.MessageDigest&#x27;); md.getInstance.overload(&#x27;java.lang.String&#x27;,&#x27;java.lang.String&#x27;).implementation = function (a,b) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + a); return this.getInstance(a, b); &#125; md.getInstance.overload(&#x27;java.lang.String&#x27;).implementation = function (a) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + a); return this.getInstance(a); &#125; md.update.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a)) return this.update(a); &#125; md.update.overload(&#x27;[B&#x27;,&#x27;int&#x27;,&#x27;int&#x27;).implementation = function (a,b,c) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a) + &quot;|&quot; + b + &quot;|&quot; + c); return this.update(a,b,c); &#125; md.digest.overload().implementation = function () &#123; showStacks(); send(&quot;======================================&quot;); var result = this.digest(); send(&quot;digest结果:&quot; + bytesToHex(result)); send(&quot;digest结果:&quot; + bytesToBase64(result)); return result; &#125; md.digest.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;digest参数:&quot; + bytesToString(a)); var result = this.digest(a); send(&quot;digest结果:&quot; + bytesToHex(result)); send(&quot;digest结果:&quot; + bytesToBase64(result)); return result; &#125; var ivParameterSpec = Java.use(&#x27;javax.crypto.spec.IvParameterSpec&#x27;); ivParameterSpec.$init.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.$init(a); send(&quot;======================================&quot;); send(&quot;iv向量:&quot; + bytesToString(a)); send(&quot;iv向量:&quot; + bytesToHex(a)); return result; &#125; var cipher = Java.use(&#x27;javax.crypto.Cipher&#x27;); cipher.getInstance.overload(&#x27;java.lang.String&#x27;).implementation = function (a) &#123; showStacks(); var result = this.getInstance(a); send(&quot;======================================&quot;); send(&quot;模式填充:&quot; + a); return result; &#125; cipher.update.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.update(a); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a)); return result; &#125; cipher.update.overload(&#x27;[B&#x27;,&#x27;int&#x27;,&#x27;int&#x27;).implementation = function (a,b,c) &#123; showStacks(); var result = this.update(a,b,c); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a) + &quot;|&quot; + b + &quot;|&quot; + c); return result; &#125; cipher.doFinal.overload().implementation = function () &#123; showStacks(); var result = this.doFinal(); send(&quot;======================================&quot;); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; cipher.doFinal.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.doFinal(a); send(&quot;======================================&quot;); send(&quot;doFinal参数:&quot; + bytesToString(a)); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; var x509EncodedKeySpec = Java.use(&#x27;java.security.spec.X509EncodedKeySpec&#x27;); x509EncodedKeySpec.$init.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.$init(a); send(&quot;======================================&quot;); send(&quot;RSA密钥:&quot; + bytesToBase64(a)); return result; &#125; var rSAPublicKeySpec = Java.use(&#x27;java.security.spec.RSAPublicKeySpec&#x27;); rSAPublicKeySpec.$init.overload(&#x27;java.math.BigInteger&#x27;,&#x27;java.math.BigInteger&#x27;).implementation = function (a,b) &#123; showStacks(); var result = this.$init(a,b); send(&quot;======================================&quot;); //send(&quot;RSA密钥:&quot; + bytesToBase64(a)); send(&quot;RSA密钥N:&quot; + a.toString(16)); send(&quot;RSA密钥E:&quot; + b.toString(16)); return result; &#125;&#125;);&quot;&quot;&quot;; fw = open(sys.argv[1],&#x27;w+&#x27;,encoding=&#x27;utf-8&#x27;) def message(message, data): if message[&quot;type&quot;] == &#x27;send&#x27;: print(u&quot;[*] &#123;0&#125;&quot;.format(message[&#x27;payload&#x27;])) fw.write(u&quot;[*] &#123;0&#125;\\n&quot;.format(message[&#x27;payload&#x27;])) fw.flush() else: print(message) process = frida.get_remote_device().attach(sys.argv[1])script= process.create_script(jsCode)script.on(&quot;message&quot;, message)script.load()sys.stdin.read() 常见算法hook123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157Java.perform(function () &#123; var secretKeySpec = Java.use(&#x27;javax.crypto.spec.SecretKeySpec&#x27;); secretKeySpec.$init.overload(&#x27;[B&#x27;, &#x27;java.lang.String&#x27;).implementation = function (a, b) &#123; showStacks(); var result = this.$init(a, b); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + b + &quot;|Dec密钥:&quot; + bytesToString(a)); send(&quot;算法名：&quot; + b + &quot;|Hex密钥:&quot; + bytesToHex(a)); return result; &#125; var mac = Java.use(&#x27;javax.crypto.Mac&#x27;); mac.getInstance.overload(&#x27;java.lang.String&#x27;).implementation = function (a) &#123; showStacks(); var result = this.getInstance(a); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + a); return result; &#125; mac.update.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); this.update(a); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a)) &#125; mac.update.overload(&#x27;[B&#x27;, &#x27;int&#x27;, &#x27;int&#x27;).implementation = function (a, b, c) &#123; showStacks(); this.update(a, b, c) send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a) + &quot;|&quot; + b + &quot;|&quot; + c); &#125; mac.doFinal.overload().implementation = function () &#123; showStacks(); var result = this.doFinal(); send(&quot;======================================&quot;); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; mac.doFinal.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.doFinal(a); send(&quot;======================================&quot;); send(&quot;doFinal参数:&quot; + bytesToString(a)); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; var md = Java.use(&#x27;java.security.MessageDigest&#x27;); md.getInstance.overload(&#x27;java.lang.String&#x27;, &#x27;java.lang.String&#x27;).implementation = function (a, b) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + a); return this.getInstance(a, b); &#125; md.getInstance.overload(&#x27;java.lang.String&#x27;).implementation = function (a) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;算法名：&quot; + a); return this.getInstance(a); &#125; md.update.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a)) return this.update(a); &#125; md.update.overload(&#x27;[B&#x27;, &#x27;int&#x27;, &#x27;int&#x27;).implementation = function (a, b, c) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a) + &quot;|&quot; + b + &quot;|&quot; + c); return this.update(a, b, c); &#125; md.digest.overload().implementation = function () &#123; showStacks(); send(&quot;======================================&quot;); var result = this.digest(); send(&quot;digest结果:&quot; + bytesToHex(result)); send(&quot;digest结果:&quot; + bytesToBase64(result)); return result; &#125; md.digest.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); send(&quot;======================================&quot;); send(&quot;digest参数:&quot; + bytesToString(a)); var result = this.digest(a); send(&quot;digest结果:&quot; + bytesToHex(result)); send(&quot;digest结果:&quot; + bytesToBase64(result)); return result; &#125; var ivParameterSpec = Java.use(&#x27;javax.crypto.spec.IvParameterSpec&#x27;); ivParameterSpec.$init.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.$init(a); send(&quot;======================================&quot;); send(&quot;iv向量:&quot; + bytesToString(a)); send(&quot;iv向量:&quot; + bytesToHex(a)); return result; &#125; var cipher = Java.use(&#x27;javax.crypto.Cipher&#x27;); cipher.getInstance.overload(&#x27;java.lang.String&#x27;).implementation = function (a) &#123; showStacks(); var result = this.getInstance(a); send(&quot;======================================&quot;); send(&quot;模式填充:&quot; + a); return result; &#125; cipher.update.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.update(a); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a)); return result; &#125; cipher.update.overload(&#x27;[B&#x27;, &#x27;int&#x27;, &#x27;int&#x27;).implementation = function (a, b, c) &#123; showStacks(); var result = this.update(a, b, c); send(&quot;======================================&quot;); send(&quot;update:&quot; + bytesToString(a) + &quot;|&quot; + b + &quot;|&quot; + c); return result; &#125; cipher.doFinal.overload().implementation = function () &#123; showStacks(); var result = this.doFinal(); send(&quot;======================================&quot;); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; cipher.doFinal.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.doFinal(a); send(&quot;======================================&quot;); send(&quot;doFinal参数:&quot; + bytesToString(a)); send(&quot;doFinal结果:&quot; + bytesToHex(result)); send(&quot;doFinal结果:&quot; + bytesToBase64(result)); return result; &#125; var x509EncodedKeySpec = Java.use(&#x27;java.security.spec.X509EncodedKeySpec&#x27;); x509EncodedKeySpec.$init.overload(&#x27;[B&#x27;).implementation = function (a) &#123; showStacks(); var result = this.$init(a); send(&quot;======================================&quot;); send(&quot;RSA密钥:&quot; + bytesToBase64(a)); return result; &#125; var rSAPublicKeySpec = Java.use(&#x27;java.security.spec.RSAPublicKeySpec&#x27;); rSAPublicKeySpec.$init.overload(&#x27;java.math.BigInteger&#x27;, &#x27;java.math.BigInteger&#x27;).implementation = function (a, b) &#123; showStacks(); var result = this.$init(a, b); send(&quot;======================================&quot;); //send(&quot;RSA密钥:&quot; + bytesToBase64(a)); send(&quot;RSA密钥N:&quot; + a.toString(16)); send(&quot;RSA密钥E:&quot; + b.toString(16)); return result; &#125;&#125;);","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/"},{"name":"frida","slug":"爬虫/frida","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/frida/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"frida","slug":"frida","permalink":"https://blog.ifan.host/tags/frida/"}]},{"title":"VS Code 开发配置","slug":"tools/development-tool/vscode-config","date":"2022-08-10T01:00:00.000Z","updated":"2024-10-29T07:20:39.682Z","comments":true,"path":"2022/08/10/tools/development-tool/vscode-config/","link":"","permalink":"https://blog.ifan.host/2022/08/10/tools/development-tool/vscode-config/","excerpt":"","text":"tasks.json123456789101112131415161718192021222324252627282930313233&#123; // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ &#123; &quot;label&quot;: &quot;任务名称&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;$&#123;config:python.pythonPath&#125;&quot;, &quot;args&quot;: [ &quot;$&#123;workspaceFolder&#125;/manager.py&quot;, &quot;-web&quot; ], &quot;presentation&quot;: &#123; &quot;echo&quot;: true, &quot;reveal&quot;: &quot;always&quot;, &quot;focus&quot;: false, &quot;panel&quot;: &quot;shared&quot;, &quot;showReuseMessage&quot;: true, &quot;clear&quot;: true &#125;, &quot;problemMatcher&quot;: [] &#125;, &#123; &quot;name&quot;: &quot;Python Debugger: bx.py with Arguments&quot;, &quot;type&quot;: &quot;debugpy&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;/manager.py&quot;, &quot;console&quot;: &quot;integratedTerminal&quot;, &quot;args&quot;: &quot;$&#123;command:pickArgs&#125;&quot; &#125; ]&#125; sftp.json12345678910111213141516171819202122[ &#123; &quot;name&quot;: &quot;sftp name&quot;, &quot;host&quot;: &quot;172.x.x.x&quot;, &quot;protocol&quot;: &quot;sftp&quot;, &quot;port&quot;: 22, &quot;username&quot;: &quot;username&quot;, &quot;password&quot;: &quot;password&quot;, &quot;remotePath&quot;: &quot;远程地址&quot;, &quot;uploadOnSave&quot;: true, &quot;useTempFile&quot;: false, &quot;openSsh&quot;: false, &quot;ignore&quot;: [ &quot;.idea&quot;, &quot;.vscode&quot;, &quot;.git&quot;, &quot;.DS_Store&quot;, &quot;venv&quot;, &quot;__pycache__&quot; ] &#125;] settings.json123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&#123; &quot;editor.fontSize&quot;: 18, //编辑器字体大小 &quot;workbench.colorTheme&quot;: &quot;Winter is Coming (Dark Blue)&quot;, &quot;workbench.iconTheme&quot;: &quot;vscode-icons&quot;, //vscode文件图标主题 &quot;go.formatTool&quot;: &quot;goimports&quot;, //golang格式化工具 &quot;editor.defaultFormatter&quot;: &quot;esbenp.prettier-vscode&quot;, //编辑器格式化工具 &quot;[javascript]&quot;: &#123; &quot;editor.defaultFormatter&quot;: &quot;rvest.vs-code-prettier-eslint&quot; &#125;, //javascript格式化工具 &quot;[vue]&quot;: &#123; &quot;editor.defaultFormatter&quot;: &quot;octref.vetur&quot; &#125;, //vue格式化工具 &quot;[go]&quot;: &#123; &quot;editor.insertSpaces&quot;: false, &quot;editor.formatOnSave&quot;: true, &quot;editor.codeActionsOnSave&quot;: &#123; &quot;source.organizeImports&quot;: &quot;explicit&quot; &#125;, &quot;editor.defaultFormatter&quot;: &quot;golang.go&quot;, //格式化器 &quot;editor.suggest.snippetsPreventQuickSuggestions&quot;: false &#125;, &quot;[python]&quot;: &#123; // Python 格式化工具 &quot;editor.defaultFormatter&quot;: &quot;ms-python.black-formatter&quot;, &quot;editor.formatOnSave&quot;: true &#125;, // &quot;black-formatter.args&quot;: [ // black 格式化参数 // &quot;--line-length&quot;, // &quot;200&quot; // ], &quot;editor.insertSpaces&quot;: false, &quot;workbench.editor.enablePreview&quot;: false, //打开文件不覆盖 &quot;search.followSymlinks&quot;: false, //关闭rg.exe进程 &quot;editor.minimap.enabled&quot;: false, //关闭快速预览 &quot;files.autoSave&quot;: &quot;afterDelay&quot;, //编辑自动保存 &quot;editor.lineNumbers&quot;: &quot;on&quot;, //开启行数提示 &quot;editor.quickSuggestions&quot;: &#123; //开启自动显示建议 &quot;other&quot;: true, &quot;comments&quot;: true, &quot;strings&quot;: true &#125;, &quot;editor.tabSize&quot;: 4, //制表符符号eslint &quot;editor.formatOnSave&quot;: true, //每次保存自动格式化 &quot;prettier.eslintIntegration&quot;: true, //让prettier使用eslint的代码格式进行校验 &quot;prettier.semi&quot;: true, //去掉代码结尾的分号 &quot;prettier.singleQuote&quot;: false, //使用单引号替代双引号 &quot;javascript.format.insertSpaceBeforeFunctionParenthesis&quot;: true, //让函数(名)和后面的括号之间加个空格 &quot;vetur.format.defaultFormatter.html&quot;: &quot;js-beautify-html&quot;, //格式化.vue中html &quot;vetur.format.defaultFormatter.js&quot;: &quot;vscode-typescript&quot;, //让vue中的js按编辑器自带的ts格式进行格式化 &quot;vetur.format.defaultFormatterOptions&quot;: &#123; &quot;js-beautify-html&quot;: &#123; &quot;wrap_attributes&quot;: &quot;force-aligned&quot; //属性强制折行对齐 &#125;, &quot;prettier&quot;: &#123; &quot;semi&quot;: false, &quot;singleQuote&quot;: true &#125;, &quot;vscode-typescript&quot;: &#123; &quot;semi&quot;: false, &quot;singleQuote&quot;: true &#125; &#125;, &quot;eslint.validate&quot;: [ &quot;vue&quot;, // &quot;javascript&quot;, &quot;typescript&quot;, &quot;typescriptreact&quot;, &quot;html&quot; ], &quot;editor.codeActionsOnSave&quot;: &#123; &quot;source.fixAll.eslint&quot;: &quot;explicit&quot; &#125;, &quot;editor.suggestSelection&quot;: &quot;first&quot;, &quot;vsintellicode.modify.editor.suggestSelection&quot;: &quot;automaticallyOverrodeDefaultValue&quot;, &quot;editor.fontLigatures&quot;: true, &quot;terminal.integrated.tabs.enabled&quot;: true, &quot;terminal.integrated.env.osx&quot;: &#123; // 命令行环境变量 &quot;PYTHONPATH&quot;: &quot;$PYTHONPATH:$&#123;workspaceRoot&#125;&quot; &#125;, &quot;code-runner.runInTerminal&quot;: true, // 在终端运行方法 &quot;code-runner.cwd&quot;: &quot;$&#123;workspaceRoot&#125;&quot;, // 运行时的父地址 &quot;rest-client.environmentVariables&quot;: &#123; // rest-client 使用的公共参数 &quot;$shared&quot;: &#123;&#125; &#125;&#125;","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"工具/编程工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"编程工具","permalink":"https://blog.ifan.host/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"地铁线路图","slug":"tools/Metro","date":"2022-08-09T01:00:00.000Z","updated":"2023-12-25T05:56:09.182Z","comments":true,"path":"2022/08/09/tools/Metro/","link":"","permalink":"https://blog.ifan.host/2022/08/09/tools/Metro/","excerpt":"","text":"","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"生活","slug":"工具/生活","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"生活","slug":"生活","permalink":"https://blog.ifan.host/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"京东京造键位图","slug":"tools/keyboard","date":"2022-08-09T00:56:00.000Z","updated":"2023-12-25T05:56:09.184Z","comments":true,"path":"2022/08/09/tools/keyboard/","link":"","permalink":"https://blog.ifan.host/2022/08/09/tools/keyboard/","excerpt":"","text":"","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"生活","slug":"工具/生活","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%94%9F%E6%B4%BB/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"生活","slug":"生活","permalink":"https://blog.ifan.host/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"Typora 修改源码模式和预览模式的宽度","slug":"tools/typora","date":"2022-08-03T00:56:00.000Z","updated":"2023-12-25T05:56:09.185Z","comments":true,"path":"2022/08/03/tools/typora/","link":"","permalink":"https://blog.ifan.host/2022/08/03/tools/typora/","excerpt":"","text":"修改源码模式和预览模式的宽度源码模式1/Applications/Typora.app/Contents/Resources/TypeMark/style/base-control.css 找到.CodeMirror-lines下的max-width将其修改为2048 预览模式找到【外观】、【打开主题文件夹】、【找到自己使用的主题】 搜索 #write ，修改其属性 max-width 的值改为你想要设置的宽度 都需要关闭之后重新打开才能生效","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"大数据 Hbase 列族的配置","slug":"bigdata/hbase/rank-family","date":"2022-07-10T06:25:00.000Z","updated":"2023-12-25T05:56:09.157Z","comments":true,"path":"2022/07/10/bigdata/hbase/rank-family/","link":"","permalink":"https://blog.ifan.host/2022/07/10/bigdata/hbase/rank-family/","excerpt":"","text":"列族的配置一、创建表及属性查看创建一个测试表 test，列族为 cf： 1create &#x27;test&#x27;, &#123;NAME =&gt; &#x27;cf&#x27;&#125; 默认属性如下： 12345678910111213141516hbase &gt; describe &#x27;test&#x27;&#123;NAME =&gt; &#x27;cf&#x27;, BLOOMFILTER =&gt; &#x27;ROW&#x27;, VERSIONS =&gt; &#x27;1&#x27;, IN_MEMORY =&gt; &#x27;false&#x27;, KEEP_DELETED_CELLS =&gt; &#x27;FALSE&#x27;, DATA_BLOCK_ENCODING =&gt; &#x27;NONE&#x27;, TTL =&gt; &#x27;FOREVER&#x27;, COMPRESSION =&gt; &#x27;GZ&#x27;, MIN_VERSIONS =&gt; &#x27;0&#x27;, BLOCKCACHE =&gt; &#x27;true&#x27;,BLOCKSIZE =&gt; &#x27;65536&#x27;, REPLICATION_SCOPE =&gt; &#x27;0&#x27;&#125; 二、列族属性配置2.1、版本数量（VERSIONS）HBase 一切操作均为更新，Hbase Put 操作不会去覆盖一个值，只会在后面追加写，用时间戳（版本号）来区分，HBase 版本维度按递减顺序存储，以便在从存储文件读取时，首先找到最近的值；Hbase Delete 操作也不是真正删除了记录，而是放置了一个墓碑标记，过早的版本会在执行 Major Compaction 时真正删除。 0.96版本默认是3个， 0.98版本之后是1， 要根据业务来划分，版本是历史记录，版本增多意味空间消耗。 插入数据的时候，版本默认是当前时间；查询的时候可以指定要获取的版本个数 get &#39;test&#39;, &#123; COLUMN =&gt; &#39;cf&#39;, VERSIONS =&gt; 2&#125;； 获取多个版本的时候，多个数据是按照时间戳倒序排序，也可以通过这个特性，来保存类似于事件发生的数据，查询时间历史的时候，拿出来的数据是按照时间排好序，如果要拿最新的事件，不指定版本即可。 版本的时间戳，也可以自定义，不使用默认生成的时间戳，可以自己指定业务相关的ID。 使用方法: 1create &#x27;test1&#x27;, &#123;NAME =&gt; &#x27;cf&#x27;, VERSIONS =&gt; 3&#125; 更改版本号： 1alter &#x27;test1&#x27;, NAME =&gt; &#x27;cf&#x27;, VERSIONS =&gt; 5 2.2、存活时间（TTL）TTL 全称是 Time To Live，ColumnFamilies 可以设置 TTL(单位是s)，HBase 会自动检查 TTL 值是否达到上限，如果 TTL 达到上限后自动删除行。当然真正删除是在Major Compaction过程中执行的。 试验一下： 12345alter &#x27;test1&#x27;, NAME =&gt; &#x27;cf&#x27;, VERSIONS =&gt; 5, TTL =&gt; 10put &#x27;test1&#x27;, &#x27;0001&#x27;, &#x27;cf:name&#x27;, &#x27;james&#x27;get &#x27;test1&#x27;, &#x27;0001&#x27; 发现过 10s 后数据已经删除。 2.3、最小版本数（MIN_VERSIONS ）如果 HBase 中的表设置了 TTL 的时候，MIN_VERSIONS 才会起作用。 每个列族可以设置最小版本数，最小版本数缺省值是0，表示禁用该特性。最小版本数参数和存活时间是一起使用的，允许配置“如保存最后T秒有价值的数据，最多N个版本，但最少M个版本”（M是最小版本，M&lt;N）。 MIN_VERSION &gt; 0时：Cell 至少有 MIN_VERSION 个最新版本会保留下来； MIN_VERSION &#x3D; 0时：Cell 中的数据超过TTL时间时，全部清空，不保留最低版本。 2.4、保留删除的单元格（KEEP_DELETED_CELLS）HBase 的delete 命令，并不是真的删除数据，而是设置一个标记（delete marker）。用户在检索数据的时候，会过滤掉这些标示的数据。该属性可以设置为 FALSE（默认）、TRUE、TTL。 可以从 java API 源码中看到： 123456789101112131415161718192021public enum KeepDeletedCells &#123; /** Deleted Cells are not retained. */ FALSE, /** * Deleted Cells are retained until they are removed by other means * such TTL or VERSIONS. * If no TTL is specified or no new versions of delete cells are * written, they are retained forever. */ TRUE, /** * Deleted Cells are retained until the delete marker expires due to TTL. * This is useful when TTL is combined with MIN_VERSIONS and one * wants to keep a minimum number of versions around but at the same * time remove deleted cells after the TTL. */ TTL; public static KeepDeletedCells getValue(String val) &#123; return valueOf(val.toUpperCase()); &#125;&#125; FALSE：不保留删除的单元格。 TRUE：删除的单元格会保留，超期（TTL）或者数据版本数超过 VERSIONS 设置的值才会被删除；如果没有指定 TTL 或没有超出VERSIONS 值，则会永久保留它们。 TTL：超期（TTL）才会删除，当 TTL 与 MIN_VERSIONS 结合使用时，会删除过期后的数据，但是同时会保留最少数量的版本。 2.5、数据块大小（BLOCKSIZE ）HBase 默认的块大小是 64kb，不同于 HDFS 默认 64MB 的块大小。 数据块索引存储每个 HFile 数据块的起始键，数据块大小配置会影响数据块索引的大小。数据块越小，数据块索引越大，因而占用的内存空间越大。 同时，加载进内存的数据块更小，随机查找性能更好，因为一旦找到了行键所在的块，接下来就会定位对应的单元格，使用更小的数据块效率更优。 但是如果需要更好的顺序扫描性能，那么一次能够加载更多 HFile 数据进入内存则更为合理，这意味数据块大小应该设置更大的值。相应索引将变小，将在随机读性能上付出代价。 对于不同的业务数据，块大小的合理设置对读写性能有很大的影响。如果业务请求以 Get 请求为主，可以考虑将块大小设置较小；如果以 Scan 请求为主，可以将块大小调大；默认的 64K 块大小是在 Scan 和 Get 之间取得的一个平衡。 默认块大小适用于多种数据使用模式，调整块大小是比较高级的操作。配置错误将对性能产生负面影响。因此建议在调整之后进行测试，根据测试结果决定是否可以线上使用。 2.6、块缓存（BLOCKCACHE）默认是 true。缓存是内存存储，HBase 使用块缓存将最近使用的块加载到内存中。块缓存会根据最近最久未使用（LRU）的规则删除数据块。 如果使用场景是经常顺序访问 Scan 或者很少被访问，可以关闭列族的缓存。列族缓存默认是打开的。 2.7、激进缓存（IN_MEMORY）HBase 可以选择一个列族赋予更高的优先级缓存，激进缓存（表示优先级更高），IN_MEMORY 默认是false。 如果设置为true，HBase 会尝试将整个列族保存在内存中，只有在需要保存是才会持久化写入磁盘。但是在运行时 HBase 会尝试将整张表加载到内存里。 这个参数通常适合较小的列族。 2.8、压缩（COMPRESSION）数据压缩是 HBase 提供的一个特性，HBase 在写入数据块到 HDFS 之前会首先对数据块进行压缩，再落盘，从而可以减少磁盘空间使用量。 而在读数据的时候首先从 HDFS 中加载出 block 块之后进行解压缩，然后再缓存到BlockCache，最后返回给用户。 写路径和读路径分别如下： 写路径： Finish DataBlock –&gt; Encoding KVs –&gt; Compress DataBlock –&gt; Flush 读路径： Read Block From Disk –&gt; DeCompress DataBlock –&gt; Cache DataBlock –&gt; Decoding Scan KVs 压缩可以节省空间，但读写数据会增加CPU负载，默认为 NONE，不使用用压缩，HBase 目前提供了三种常用的压缩方式： GZip, LZO, Snappy： GZIP 的压缩率最高，但是 CPU 密集型的，对 CPU 的消耗比其他算法要多，压缩和解压速度也慢； LZO 的压缩率居中，比 GZIP 要低一些，但是压缩和解压速度明显要比GZIP 快很多，其中解压速度快的更多； Snappy 的压缩率最低，而压缩和解压速度要稍微比 LZO 要快一些。 综合来看，Snappy 的压缩率最低，但是编解码速率最高，对 CPU 的消耗也最小，目前一般建议使用 Snappy。 2.9、布隆过滤器（BLOOMFILTER）布隆过滤器用自己的算法，实现了快速的检索一个元素是否在一个较大的元素列表之中。 它的基本思想是：当一个元素被加入集合时，通过 K 个散列函数将这个元素映射成一个位数组中的 K 个点，把它们置为1；检索时，只要看看这些点是不是都是1 就（大约）知道集合中有没有它了——如果这些点有任何一个0，则被检元素一定不在，如果都是1，则被检元素很可能在。 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难使用了Hash算法，必然会存在极限巧合下的 hash 碰撞，会将不存在的数据认为是存在的。但是存在的数据一定是可以正确判断的。 HBase 中的 BloomFilter 主要用来过滤不存在待检索 RowKey 或者 Row-Col 的 HFile 文件，避免无用的 IO 操作。它可以判断 HFile 文件中是否可能存在待检索的KV，如果不存在，就可以不用消耗 IO 打开文件进行 seek。通过设置 BloomFilter 可以提升随机读写的性能。 BloomFilter 是一个列族级别的配置属性，如果在表中设置了BloomFilter，那么HBase 会在生成 StoreFile 时包含一份 BloomFilter 结构的数据，称其为MetaBlock ，和 DataBlock (真实KeyValue数据)一起由 LRUBlockCache 维护。所以开启 BloomFilter 会有一定的存储即内存 Cache 的开销。 BloomFilter 取值有两个，row 和rowcol，需要根据业务来确定具体使用哪种。 如果业务大多数随机查询时仅仅使用 row 作为查询条件，BloomFilter 设置为row； 如果大多数随机查询使用 row+cf 作为查询条件，BloomFilter 需要设置为rowcol； 如果不确定查询类型，建议设置为 row。 2.10、数据块编码（DATA_BLOCK_ENCODING）除了数据压缩之外，HBase 还提供了数据编码功能。 和压缩一样，数据在落盘之前首先会对 KV 数据进行编码；但又和压缩不同，数据块在缓存前并没有执行解码。因此即使后续命中缓存的查询是编码的数据块，需要解码后才能获取到具体的 KV 数据。 和不编码情况相比，编码后相同数据 block 块占用内存更少，即内存利用率更高，但是读取的时候需要解码，又不利于读性能，在内存不足的情况下，可以压榨 CPU 字段，换区更多的缓存数据。 HBase目前提供了四种常用的编码方式： Prefix_Tree、 Diff 、 Fast_Diff 、Prefix。 写路径和读路径分别如下： 写路径： Finish DataBlock –&gt; Encoding KVs –&gt; Compress DataBlock –&gt; Flush 读路径： Read Block From Disk –&gt; DeCompress DataBlock –&gt; Cache DataBlock –&gt; Decoding Scan KVs 2.11、复制范围（REPLICATION_SCOPE ）HBase 提供了跨级群同步的功能，本地集群的数据更新可以及时同步到其他集群。复制范围（replication scope）的参数默认为0，表示复制功能处于关闭状态。 三、列族设置3.1、列族数量不要在一张表中定义太多的列族。 目前 HBase 并不能很好的处理 2~3 以上的列族，flush和compaction 操作是针对一个 Region 的。 当一个列族操作大量数据的时候会引发一个 flush，它邻近的列族也会因关联效应被触发 flush，尽管它没有操作多少数据。compaction 操作是根据一个列族下的全部文件的数量触发的，而不是根据文件大小触发的。 当很多的列族在 flush 和 compaction 时，会造成很多没用的 IO 负载。 尽量在模式中只针对一个列族进行操作。将使用率相近的列归为一个列族，这样每次访问就只用访问一个列族，既能提升查询效率，也能保持尽可能少的访问不同的磁盘文件。 3.2、列族的基数如果一个表存在多个列族，要注意列族之间基数（如行数）相差不要太大。例如列族 A 有100 万行，列族 B 有 10 亿行，按照 RowKey 切分后，列族A可能被分散到很多很多 Region（及RegionServer），这导致扫描列族A十分低效。 3.3、列族名、列名长度列族名和列名越短越好，冗长的名字虽然可读性好，但是更短的名字在 HBase 中更好。 一个具体的值由存储该值的行键、对应的列（列族:列）以及该值的时间戳决定。HBase 中索引是为了加速随机访问的速度，索引的创建是基于“行键+列族:列+时间戳+值”的，如果行键和列族的大小过大，甚至超过值本身的大小，那么将会增加索引的大小。并且在HBase中数据记录往往非常之多，重复的行键、列将不但使索引的大小过大，也将加重系统的负担。 四、总结根据 HBase 列族属性配置，结合使用场景，HBase 列族可以进行如下优化： 列族不宜过多，将相关性很强的 key-value 都放在同一个列族下； 尽量最小化行键和列族的大小； 提前预估数据量，再根据 RowKey 规则，提前规划好 Region 分区，在创建表的时候进行预分区； 在业务上没有特别要求的情况下，只使用一个版本，即最大版本和最小版本一样，均为1； 根据业务需求合理设置好失效时间（存储的时间越短越好）； 根据查询条件，设置合理的BloomFilter配置。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"大数据","slug":"数据库/大数据","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"HBase","slug":"数据库/大数据/HBase","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.ifan.host/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"HBase","slug":"HBase","permalink":"https://blog.ifan.host/tags/HBase/"}]},{"title":"Nginx 使用 goaccess 分析日志","slug":"web/nginx/goaccess-nginx-log","date":"2022-07-01T01:01:00.000Z","updated":"2023-12-25T05:56:09.187Z","comments":true,"path":"2022/07/01/web/nginx/goaccess-nginx-log/","link":"","permalink":"https://blog.ifan.host/2022/07/01/web/nginx/goaccess-nginx-log/","excerpt":"","text":"安装 Goaccess1sudo yum install access 创建分析脚本12345678910111213141516171819202122232425#!/bin/bash# 日志路径LOG_PATH=/tmp/logs# 需要分析的APP# PS 博客的日志为 blog.access.log 和 blog.error.log# 则 APPS 中应该填入 blogAPPS=(blog gogs)# 创建 静态文件生成的路径STATIC_HMTL=/tmp/goaccessmkdir -p $STATIC_HMTLfor(( i=0;i&lt;$&#123;#APPS[@]&#125;;i++))do APP=$&#123;APPS[i]&#125;; cat $LOG_PATH/$APP.access.log $LOG_PATH/$APP.error.log &gt; $APP.log goaccess -f $APP.log -o $STATIC_HMTL/$APP.html rm $APP.logdone# 合并所有日志for(( i=0;i&lt;$&#123;#APPS[@]&#125;;i++))do APP=$&#123;APPS[i]&#125;; cat $LOG_PATH/$APP.access.log $LOG_PATH/$APP.error.log &gt;&gt; all.logdonegoaccess -f all.log -o $STATIC_HMTL/all.htmlrm all.log 创建定时任务12# 每10分钟分析一次*/10 * * * * &quot;bash /tmp/start-goaccess.sh&quot; &gt; /dev/null","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"Nginx","slug":"WEB/Nginx","permalink":"https://blog.ifan.host/categories/WEB/Nginx/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.ifan.host/tags/Nginx/"}]},{"title":"Java Spring Boot 在初始化时添加业务逻辑","slug":"language/java/framework/springboot/springboot-start-with","date":"2022-06-23T01:00:00.000Z","updated":"2023-12-25T05:56:09.174Z","comments":true,"path":"2022/06/23/language/java/framework/springboot/springboot-start-with/","link":"","permalink":"https://blog.ifan.host/2022/06/23/language/java/framework/springboot/springboot-start-with/","excerpt":"","text":"1. ApplicationListener&lt;ContextRefreshedEvent&gt;Spring 内置的事件 ContextRefreshedEvent：ApplicationContext 被初始化或刷新时，该事件被发布。这也可以在 ConfigurableApplicationContext 接口中使用 refresh() 方法来发生。此处的初始化是指：所有的Bean被成功装载，后处理Bean被检测并激活，所有 Singleton Bean 被预实例化，&#96;&#96;ApplicationContext容器已就绪可用。 ContextStartedEvent：当使用 ConfigurableApplicationContext （ApplicationContext子接口）接口中的 start() 方法启动 ApplicationContext 时，该事件被发布。你可以调查你的数据库，或者你可以在接受到这个事件后重启任何停止的应用程序。 ContextStoppedEvent：当使用 ConfigurableApplicationContext 接口中的 stop() 停止 ApplicationContext 时，发布这个事件。你可以在接受到这个事件后做必要的清理的工作。 ContextClosedEvent：当使用 ConfigurableApplicationContext 接口中的 close() 方法关闭 ApplicationContext 时，该事件被发布。一个已关闭的上下文到达生命周期末端；它不能被刷新或重启。 RequestHandledEvent：这是一个 web-specific 事件，告诉所有 bean HTTP 请求已经被服务。只能应用于使用 DispatcherServlet 的Web应用。在使用Spring作为前端的MVC控制器时，当Spring处理用户请求结束后，系统会自动触发该事件。 代码实例： 12345678@Componentpublic class TestApplicationListener implements ApplicationListener&lt;ContextRefreshedEvent&gt;&#123; @Override public void onApplicationEvent(ContextRefreshedEvent contextRefreshedEvent) &#123; System.out.println(contextRefreshedEvent); System.out.println(&quot;TestApplicationListener............................&quot;); &#125;&#125; 可以自定义事件完成一些特定请求 自定义 EmailEvent 12345678910111213public class EmailEvent extends ApplicationEvent&#123; private String address; private String text; public EmailEvent(Object source, String address, String text)&#123; super(source); this.address = address; this.text = text; &#125; public EmailEvent(Object source) &#123; super(source); &#125; //......address和text的setter、getter&#125; 自定义监听器 1234567891011public class EmailNotifier implements ApplicationListener&#123; public void onApplicationEvent(ApplicationEvent event) &#123; if (event instanceof EmailEvent) &#123; EmailEvent emailEvent = (EmailEvent)event; System.out.println(&quot;邮件地址：&quot; + emailEvent.getAddress()); System.our.println(&quot;邮件内容：&quot; + emailEvent.getText()); &#125; else &#123; System.our.println(&quot;容器本身事件：&quot; + event); &#125; &#125;&#125; 发送邮件后，触发事件 123456789public class SpringTest &#123; public static void main(String args[])&#123; ApplicationContext context = new ClassPathXmlApplicationContext(&quot;bean.xml&quot;); //创建一个ApplicationEvent对象 EmailEvent event = new EmailEvent(&quot;hello&quot;,&quot;abc@163.com&quot;,&quot;This is a test&quot;); //主动触发该事件 context.publishEvent(event); &#125;&#125; 2. Spring Boot 的 CommandLineRunner 接口123456789101112@Component@Slf4jpublic class CustomCommandLineRunner implements CommandLineRunner &#123; /** * @param args 接收控制台传入的参数 */ @Override public void run(String... args) throws Exception &#123; log.debug(&quot;从控制台接收参数&gt;&gt;&gt;&gt;&quot;+ Arrays.asList(args)); &#125;&#125; 3. Spring Boot 的 ApplicationRunner 接口ApplicationRunner和CommandLineRunner都是Spring Boot 提供的，相对于CommandLineRunner来说对于控制台传入的参数封装更好一些，可以通过键值对来获取指定的参数，比如 --version=1 4. @PostConstruct针对 Bean 初始化完成后做一些事情 123456789@Component@Slf4jpublic class SimpleExampleBean &#123; @PostConstruct public void init()&#123; log.debug(&quot;Bean初始化完成，调用...........&quot;); &#125;&#125; 5. @Bean注解中指定初始化方法123456789101112@Slf4jpublic class SimpleExampleBean &#123; public void init()&#123; log.debug(&quot;Bean初始化完成，调用...........&quot;); &#125;&#125;@Bean(initMethod = &quot;init&quot;)public SimpleExampleBean simpleExampleBean()&#123; return new SimpleExampleBean();&#125; 6. InitializingBean 接口InitializingBean的用法基本上与@PostConstruct一致，只不过相应的Bean需要实现afterPropertiesSet方法 123456789@Slf4j@Componentpublic class SimpleExampleBean implements InitializingBean &#123; @Override public void afterPropertiesSet() &#123; log.debug(&quot;Bean初始化完成，调用...........&quot;); &#125;&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/categories/Java/"},{"name":"Spring Boot","slug":"Java/Spring-Boot","permalink":"https://blog.ifan.host/categories/Java/Spring-Boot/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.ifan.host/tags/Spring-Boot/"}]},{"title":"大数据 Hbase 一些命令","slug":"bigdata/hbase/cmd","date":"2022-06-10T06:25:00.000Z","updated":"2023-12-25T05:56:09.157Z","comments":true,"path":"2022/06/10/bigdata/hbase/cmd/","link":"","permalink":"https://blog.ifan.host/2022/06/10/bigdata/hbase/cmd/","excerpt":"","text":"增删改查添加1put &#x27;test&#x27;,&#x27;row1&#x27;,&#x27;cf:name&#x27;,&#x27;iFan&#x27; 查询123456789get &#x27;test&#x27;,&#x27;row1&#x27;,&#123;COLUMN=&gt;&#x27;cf:name&#x27;, VERSIONS=&gt;3&#125;scan &#x27;test&#x27;,&#123;STARTROW=&gt;&#x27;row3&#x27;, ENDROW=&gt;&#x27;row4&#x27;&#125;// 查询表未经过过滤的原始记录scan &#x27;test&#x27;,&#123;RAW=&gt;true, VERSION&gt;5&#125; scan &#x27;表名&#x27;, &#123;LIMIT=&gt;行数量&#125;# 分页查询scan &#x27;myns:user_info&#x27;, &#123;COLUMNS =&gt; [&#x27;base_info:username&#x27;], LIMIT =&gt; 10, STARTROW =&gt; &#x27;001&#x27;&#125;# 根据时间查询scan &#x27;myns:user_info&#x27;,&#x27;001&#x27;, &#123;&#x27;TIMERANGE&#x27; =&gt; [1571650017702, 1571650614606]&#125; 删除HBase并不是即时删除数据，而是设置了一个墓碑标记，会定期清除过期数据 12345# 删除这个版本之前的所有数据delete &#x27;test&#x27;,&#x27;row4&#x27;,&#x27;cf:name&#x27; ts # 删除整列的数据deleteall &#x27;test&#x27;,&#x27;row5&#x27; ts 表操作创建表12# 创建表create &#x27;表名&#x27;, &#123;NAME=&gt;&#x27;列族1&#x27;, 属性名=&gt;值&#125;, &#123;NAME=&gt;&#x27;列族2&#x27;&#125; 添加列族1alter &#x27;test&#x27;, &#x27;cf2&#x27; 修改列族属性12345alter &#x27;test&#x27;,&#123;NAME=&gt;&#x27;cf&#x27;, VERSIONS=&gt;5&#125;alter &#x27;表名&#x27;, 属性名=&gt;值, 属性名=&gt;值alter &#x27;表名&#x27;, NAME=&gt;&#x27;列族&#x27;, 属性名=&gt;值, 属性名=&gt;值alter &#x27;表名&#x27;, &#123;NAME=&gt;&#x27;列族&#x27;, 属性名=&gt;值, 属性名=&gt;值&#125;,&#123;NAME=&gt;&#x27;列族&#x27;, 属性名=&gt;值, 属性名=&gt;值&#125;alter &#x27;表名&#x27;, &#x27;delete&#x27; =&gt; &#x27;列族&#x27; 查看符合条件的表1list [&#x27;通配符&#x27;] 查看表属性123describe &#x27;表名&#x27;# 查看表是否存在exit &#x27;table_name&#x27; 禁用表12345# 禁用表disable &#x27;table_name&#x27;# 禁用符合条件的表disable_all &#x27;tab.*&#x27; 启用表1enable &#x27;表名&#x27; 删除表12345# 删除表drop &#x27;table_name&#x27;# 删除符合条件的表drop_all &#x27;tab.*&#x27; 数据导入导出12345# 数据导出hbase org.apache.hadoop.hbase.mapreduce.Export 表名 数据文件位置 版本 开始时间 结束时间# 数据导入hbase org.apache.hadoop.hbase.mapreduce.Import 表名称 hdfs上的目录 REST APIAPI 查询单个column cf:q： 1curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*?columns=cf:q 查询多个column cf1:q1和cf2:q2： 1curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*?columns=cf1:q1,cf2:q2 从key1开始查询两条记录： 1curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*?startrow=key1&amp;limit=2 查询1389900769772和1389900800000之间的版本： 1curl -H &quot;Content-Type: text/xml&quot; https://localhost:8080/table/*？starttime=1389900769772&amp;endtime=1389900800000","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"大数据","slug":"数据库/大数据","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"HBase","slug":"数据库/大数据/HBase","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.ifan.host/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"HBase","slug":"HBase","permalink":"https://blog.ifan.host/tags/HBase/"}]},{"title":"大数据 Hbase","slug":"bigdata/hbase/base","date":"2022-06-10T06:20:00.000Z","updated":"2023-12-25T05:56:09.157Z","comments":true,"path":"2022/06/10/bigdata/hbase/base/","link":"","permalink":"https://blog.ifan.host/2022/06/10/bigdata/hbase/base/","excerpt":"","text":"HBase 架构 和 HDFS、YARN 一样，HBase 也采用 master &#x2F; slave 架构： HBase 有一个 master 节点。master 节点负责将区域（region）分配给 region 节点；恢复 region 节点的故障。 HBase 有多个 region 节点。region 节点负责零个或多个区域（region）的管理并相应客户端的读写请求。region 节点还负责区域的划分并通知 master 节点有了新的子区域。 客户端获取数据是直连 RegionServer 的，所以当 Master 挂掉之后，还是可以获取数据，但是不能创建表了。 RegionServer 的数据直接保存在 HDFS 上 Zookeeper 管理所有 RegionServer 的信息，包括具体的数据段存放的 RegionServer. HBase 依赖 ZooKeeper 来实现故障恢复。 RegionHBase 表按行键范围水平自动划分为区域（region）。每个区域由表中行的子集构成。每个区域由它所属的表、它所含的第一行及最后一行来表示。 Region只不过是表被拆分，并分布在区域服务器。 region 不能跨服务器，当数据量很小时，一个 Region 会存储所有数据；当数据量很大，HBase 则会拆分 Region HBase 在负载均衡时，会将一个 RegionServer 上的 Region 移动到另一台 RegionServer Master 服务器区域分配、DDL(create、delete)操作由 HBase master 服务器处理。 master 服务器负责协调 region 服务器 协助区域启动，出现故障恢复或负载均衡情况时，重新分配 region 服务器 监控集群中的所有 region 服务器 支持 DDL 接口（创建、删除、更新表） Regin Server区域服务器运行在 HDFS 数据节点上，具有以下组件 WAL - Write Ahead Log 是 HDFS 上的文件。WAL 存储尚未持久存储到永久存储的新数据，它用于在发生故障时进行恢复。 BlockCache - 是读缓存。它将频繁读取的数据存储在内存中。至少最近使用的数据在完整时被逐出。 MemStore - 是写缓存。它存储尚未写入磁盘的新数据。在写入磁盘之前对其进行排序。每个区域每个列族有一个 MemStore。 Hfiles - 将行存储为磁盘上的排序键值对。 ZooKeeperHBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。Zookeeper 维护哪些服务器是活动的和可用的，并提供服务器故障通知。集群至少应该有 3 个节点。 数据模型HBase 是一个面向列的数据库，在表中它由行排序。 HBase 表模型结构为： 表（table）是行的集合。 行（row）是列族的集合。 列族（column family）是列的集合。 列（row）是键值对的集合。 HBase 表的单元格（cell）由行和列的坐标交叉决定，是有版本的。默认情况下，版本号是自动分配的，为 HBase 插入单元格时的时间戳。 每个行（row）都拥有唯一的行键（row key）来标定这个行的唯一性。每个列都有多个版本，每个版本的值存储在单元格（cell）中。 HBase 是根据 rowkey 来排序的，所以在定一个rowkey的时候需要尽量将其打散，一个常用的方法就是将原选的ID倒叙。 列族：HBase 会讲相同列族的列尽量放在同一台机器上。 维护工具管理均衡器移动 Region 到不同的 RegionServer 上以平摊压力 StochasticLoadBalancer 考虑的因素： Region Load: Region 的负载 Table Load: 表的负载 Data Locality: 数据本地化 Memstore Szie: 存储在内存中的大小 Storefile Size: 存储在磁盘上的大小 与均衡器相关的参数 hbase.balancer.period: 执行周期，默认5分钟。 hbase.regions.slop: 均衡容忍值，默认为 0.001。 hbase.master.loadbalancer.class: 均衡器实现类 规整器通过合并或者拆分的手段，将 Region 的大小控制在一个相对稳定的范围内。 具体的实现步骤： 获取该表所有的 Region 计算该表 Region 平均大小 如果某个 Region 大于平均大小的两倍，则需要拆分 不管合并最小的两个 Region，只要最小的两个 Region 大小之和小于 Region 平均大小，则就会合并 空 Region（小于1MB）不参与合并 拆分&#x2F;合并风暴：在某种情况下，拆分了几个 Region 后，系统达到了某个阈值，该阈值会触发 Region 的合并，于是开始合并，合并后又触发了另一个阈值，导致 Region开始拆分。 可能影响的因素： 均衡器定义的 hbase.regions.slop 偏移量 拆分 Region 的策略定义 hbase.regionserver.region.split.policy 单个 Region 下最大文件大小 hbase.hregion.max.filesize 拆分策略 ConstantSizeRegionSplitPolicy：单个 Region 大小超过 10G ，则拆分，目前不用了 IncreasingToUpperBoundRegionSplitPolicy：默认，限制不断增长的文件尺寸的策略。计算公式为 min(tableRegionCount ^ 3 * intialSize, defaultRegionMaxFileSize) tableRegionCount：表在所有 RegionServer 上所拥有的 Region 数量总和 initialSize：默认使用 hbase.increasing.policy.initial.size，否则使用 hbase.hregion.memstore.flush.size * 2 defaultRegionMaxFileSize: hbase.hregion.max.fileeize 即 Region 最大大小。 KeyPrefixRegionSplitPolicy：按照 keyPrefixRegionSplitPolicy.prefix_length 所定义的长度截取 rowkey 作为分组的依据，同一组的数据不会被划分到不同的 Region 上 DelimitedKeyPrefixRegionSplitPolicy：同上，但是是根据分隔符来判断的。 BusyRegionSplitPolicy：根据 rowkey 热点来分割数据 hbase.busy.policy.blickedRequests: 请求阻塞率，默认为0.2，即20%的请求被阻塞。 hbase.busy.policy.minAge：拆分最小年龄，小于该年龄的数据不会被拆分，默认10分钟。 hbase.busy.policy.aggWindow：计算是否繁忙的时间窗口，默认5分钟。 DisabledRegionSplitPolicy：不拆分，用户自己拆分。 预拆分和强拆分1hbase org.apache.hadoop.hbase.util.RegionSplitter tablename HexStringSplit -c 10 -f mycf tablename: 需要拆分的表名 HexStringSplit: 指定拆分算法 -c: 要拆分 Region 数量 -f: 要建立的列族名称 HexStringSplit: 将数据从 00000000 到 FFFFFFFF 之间的数据长度按照n等分之后算出每一段的起始 rowkey 和结束 rowkey，以此作为拆分点。 UniformSplit：起始 rowkey 是 ArrayUtils.EMPTY_BYTE_ARRAY，结束 rowkey 是 new byte[]&#123;xFF, xFF, xFF, xFF, xFF, xFF, xFF, xFF&#125; 合并删除大量的数据之后，每个 Region 都变小了，需要合并。 1habse&gt; merge_region &#x27;rowkey1&#x27;,&#x27;rowkey2&#x27; 目录管理器hbase:meta 表中存储的 Region 信息。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"大数据","slug":"数据库/大数据","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"HBase","slug":"数据库/大数据/HBase","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://blog.ifan.host/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"HBase","slug":"HBase","permalink":"https://blog.ifan.host/tags/HBase/"}]},{"title":"Java MyBatis 返回类型","slug":"language/java/framework/mybatis/mybatis-result-type","date":"2022-06-10T01:00:00.000Z","updated":"2023-12-25T05:56:09.174Z","comments":true,"path":"2022/06/10/language/java/framework/mybatis/mybatis-result-type/","link":"","permalink":"https://blog.ifan.host/2022/06/10/language/java/framework/mybatis/mybatis-result-type/","excerpt":"","text":"返回单个结果Map123&lt;select id=&quot;selectMap&quot; resultType=&quot;java.util.Map&quot;&gt; SELECT id, name, email FROM user LIMIT 1 &lt;/select&gt; 12@MapKey(&quot;id&quot;)Map&lt;Long, Object&gt; selectMap(); 12345@Testpublic void selectMap() &#123; Map&lt;Long, Object&gt; map = mapper.selectMap(); System.out.println(JSON.toJSONString(map));&#125; 返回结果 1&#123;&quot;1&quot;:&#123;&quot;name&quot;:&quot;ifan&quot;,&quot;id&quot;:1,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;&#125; LinkedHashMap123&lt;select id=&quot;selectLinkedHashMap&quot; resultType=&quot;java.util.LinkedHashMap&quot;&gt; SELECT id, name, email FROM user LIMIT 1 &lt;/select&gt; 1LinkedHashMap&lt;String, Object&gt; selectLinkedHashMap(); 1LinkedHashMap&lt;String, Object&gt; linkedHashMap = mapper.selectLinkedHashMap(); 返回结果 1&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125; 返回多个结果List&lt;Map&gt;123&lt;select id=&quot;selectMapList&quot; resultType=&quot;java.util.Map&quot;&gt; SELECT id, name, email FROM user LIMIT 1&lt;/select&gt; 1List&lt;Map&lt;String, Object&gt;&gt; selectMapList(); 1List&lt;Map&lt;String, Object&gt;&gt; maps = mapper.selectMapList(); 返回结果 12345[ &#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125; &#123;&quot;id&quot;:2,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125; &#123;&quot;id&quot;:3,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;] Map&lt;Map&gt;123&lt;select id=&quot;selectMapMap&quot; resultType=&quot;java.util.Map&quot;&gt; SELECT id, name, email FROM user LIMIT 1&lt;/select&gt; 12@MapKey(&quot;id&quot;)Map&lt;String, Map&lt;String, Object&gt;&gt; selectMapMap(); 1Map&lt;String, Map&lt;String, Object&gt;&gt; stringMapMap = mapper.selectMapMap(); 返回结果 12345&#123; &quot;1&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;, &quot;2&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;, &quot;3&quot;:&#123;&quot;id&quot;:3,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;,&#125; List&lt;LinkedHashMap&gt;123&lt;select id=&quot;selectListLinkedHashMap&quot; resultType=&quot;java.util.LinkedHashMap&quot;&gt; SELECT id, name, email FROM user LIMIT 1&lt;/select&gt; 1List&lt;LinkedHashMap&lt;String, Object&gt;&gt; selectListLinkedHashMap(); 1List&lt;LinkedHashMap&lt;String, Object&gt;&gt; linkedHashMap = mapper.selectListLinkedHashMap(); 12345[ &#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;, &#123;&quot;id&quot;:2,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;, &#123;&quot;id&quot;:3,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;,] Map&lt;LinkedHashMap&gt;123&lt;select id=&quot;selectMapLinkedHashMap&quot; resultType=&quot;java.util.LinkedHashMap&quot;&gt; SELECT id, name, email FROM user LIMIT 1&lt;/select&gt; 12@MapKey(&quot;id&quot;)Map&lt;String, LinkedHashMap&lt;String, Object&gt;&gt; selectMapLinkedHashMap(); 1Map&lt;String, LinkedHashMap&lt;String, Object&gt;&gt; mapLinkedHashMaps = mapper.selectMapLinkedHashMap(); 12345&#123; &quot;1&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;, &quot;2&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;, &quot;3&quot;:&#123;&quot;id&quot;:3,&quot;name&quot;:&quot;ifan&quot;,&quot;email&quot;:&quot;ifan@qq.com&quot;&#125;,&#125; List&lt;Pair&lt;Integer, Long&gt;&gt;123&lt;select id=&quot;countByAuthor&quot; resultType=&quot;javafx.util.Pair&quot;&gt; SELECT id, name, email FROM user LIMIT 1&lt;/select&gt; 1List&lt;Pair&lt;Integer, Long&gt;&gt; countByAuthor(); 12List&lt;Pair&lt;Integer, Long&gt;&gt; list = mapper.countByAuthor();Map&lt;Integer, Long&gt; map = list.stream().collect(Collectors.toMap(Pair::getKey, Pair::getValue)); 返回结果 1&#123;&quot;ifan&quot;: 1, &quot;fan&quot;:2&#125;","categories":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/categories/Java/"},{"name":"MyBatis","slug":"Java/MyBatis","permalink":"https://blog.ifan.host/categories/Java/MyBatis/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/tags/Java/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://blog.ifan.host/tags/MyBatis/"}]},{"title":"Mysql 基础知识 常用的SQL","slug":"database/mysql/mysql-sql","date":"2022-06-08T10:30:00.000Z","updated":"2023-12-25T05:56:09.165Z","comments":true,"path":"2022/06/08/database/mysql/mysql-sql/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-sql/","excerpt":"","text":"DQL连接和组合 连接内连接（INNER JOIN）内连接又称等值连接，使用 INNER JOIN 关键字。在没有条件语句的情况下返回笛卡尔积。 123SELECT vend_name, prod_name, prod_priceFROM vendors INNER JOIN productsON vendors.vend_id = products.vend_id; 自连接（=）自连接可以看成内连接的一种，只是连接的表是自身而已。自然连接是把同名列通过 = 连接起来的，同名列可以有多个。 1234SELECT c1.cust_id, c1.cust_name, c1.cust_contactFROM customers c1, customers c2WHERE c1.cust_name = c2.cust_nameAND c2.cust_contact = &#x27;Jim Jones&#x27;; 自然连接（NATURAL JOIN）内连接提供连接的列，而自然连接自动连接所有同名列。自然连接使用 NATURAL JOIN 关键字。 123SELECT *FROM ProductsNATURAL JOIN Customers; 外连接（OUTER JOIN）外连接返回一个表中的所有行，并且仅返回来自此表中满足连接条件的那些行，即两个表中的列是相等的。外连接分为左外连接、右外连接、全外连接（Mysql 不支持）。 左连接（LEFT JOIN）左外连接就是保留左表没有关联的行。 123SELECT customers.cust_id, orders.order_numFROM customers LEFT JOIN ordersON customers.cust_id = orders.cust_id; 右连接（RIGHT JOIN）右外连接就是保留右表没有关联的行。 123SELECT customers.cust_id, orders.order_numFROM customers RIGHT JOIN ordersON customers.cust_id = orders.cust_id; 组合（UNION） UNION 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 UNION 中参与查询的提取行。 UNION 基本规则： 所有查询的列数和列顺序必须相同。 每个查询中涉及表的列的数据类型必须相同或兼容。 通常返回的列名取自第一个查询。 默认会去除相同行，如果需要保留相同行，使用 UNION ALL。 只能包含一个 ORDER BY 子句，并且必须位于语句的最后。 应用场景： 在一个查询中从不同的表返回结构数据。 对一个表执行多个查询，按一个查询返回数据。 组合查询示例： 1234567SELECT cust_name, cust_contact, cust_emailFROM customersWHERE cust_state IN (&#x27;IL&#x27;, &#x27;IN&#x27;, &#x27;MI&#x27;)UNIONSELECT cust_name, cust_contact, cust_emailFROM customersWHERE cust_name = &#x27;Fun4All&#x27;; 五、函数文本处理 函数 说明 LEFT()、RIGHT() 左边或者右边的字符 LOWER()、UPPER() 转换为小写或者大写 LTRIM()、RTIM() 去除左边或者右边的空格 LENGTH() 长度 SUBSTRING_INDEX(str, split, index) 截取 第 index 个 split 之前、之后(index为负数)的所有字符，如果不存在，则返回整个字符串 SUBSTRING(str, index, len) 截取指定长度的字符串 日期和时间处理 日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 函 数 说 明 AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 字符串格式化 格式化时间戳：select from_unixtime(1655012478, &#39;%Y-%m-%d&#39;); 格式化时间格式：select date_format(now(), &#39;%Y-%m-%d&#39;) format 描述 %S,%s 两位数字形式的秒（ 00,01, …, 59） %I,%i 两位数字形式的分（ 00,01, …, 59） %H 两位数字形式的小时，24 小时（00,01, …, 23） %h 两位数字形式的小时，12 小时（01,02, …, 12） %k 数字形式的小时，24 小时（0,1, …, 23） %l 数字形式的小时，12 小时（1, 2, …, 12） %T 24 小时的时间形式（hh:mm:ss） %r 12 小时的时间形式（hh:mm:ss AM 或hh:mm:ss PM） %p AM或PM %W 一周中每一天的名称（Sunday, Monday, …, Saturday） %a 一周中每一天名称的缩写（Sun, Mon, …, Sat） %d 两位数字表示月中的天数（00, 01,…, 31） %e 数字形式表示月中的天数（1, 2， …, 31） %D 英文后缀表示月中的天数（1st, 2nd, 3rd,…） %w 以数字形式表示周中的天数（ 0 &#x3D; Sunday, 1&#x3D;Monday, …, 6&#x3D;Saturday） %j 以三位数字表示年中的天数（ 001, 002, …, 366） %U 周（0, 1, 52），其中Sunday 为周中的第一天 %u 周（0, 1, 52），其中Monday 为周中的第一天 %M 月名（January, February, …, December） %b 缩写的月名（ January, February,…., December） %m 两位数字表示的月份（01, 02, …, 12） %c 数字表示的月份（1, 2, …., 12） %Y 四位数字表示的年份 %y 两位数字表示的年份 %% 直接值% 数值处理 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 汇总 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。 使用 DISTINCT 可以让汇总函数值汇总不同的值。 12SELECT AVG(DISTINCT col1) AS avg_colFROM mytable 约束 SQL 约束用于规定表中的数据规则。 如果存在违反约束的数据行为，行为会被约束终止。 约束可以在创建表时规定（通过 CREATE TABLE 语句），或者在表创建之后规定（通过 ALTER TABLE 语句）。 约束类型 NOT NULL - 指示某列不能存储 NULL 值。 UNIQUE - 保证某列的每行必须有唯一的值。 PRIMARY KEY - NOT NULL 和 UNIQUE 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。 FOREIGN KEY - 保证一个表中的数据匹配另一个表中的值的参照完整性。 CHECK - 保证列中的值符合指定的条件。 DEFAULT - 规定没有给列赋值时的默认值。 创建表时使用约束条件： 12345678CREATE TABLE Users ( Id INT(10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT &#x27;自增Id&#x27;, Username VARCHAR(64) NOT NULL UNIQUE DEFAULT &#x27;default&#x27; COMMENT &#x27;用户名&#x27;, Password VARCHAR(64) NOT NULL DEFAULT &#x27;default&#x27; COMMENT &#x27;密码&#x27;, Email VARCHAR(64) NOT NULL DEFAULT &#x27;default&#x27; COMMENT &#x27;邮箱地址&#x27;, Enabled TINYINT(4) DEFAULT NULL COMMENT &#x27;是否有效&#x27;, PRIMARY KEY (Id)) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT=&#x27;用户表&#x27;; DML索引添加索引1alter table 表名 add index 索引名 (字段名1[，字段名2 …]); 添加唯一限制条件的索引1alter table 表名 add unique 索引名 (字段名); 删除某个索引1alter table 表名 drop index 索引名; 表添加主键1alter table 表名 add primary key (字段名); 添加字段1ALTER TABLE table_name ADD field_name field_type; 修改字段名称和类型1ALTER TABLE table_name CHANGE old_field_name new_field_name field_type; 删除字段1ALTER TABLE table_name DROP field_name; 修改数据库的字符集1ALTER DATABASE db_name DEFAULT CHARACTER SET character_name [COLLATE ...]; 修改表默认字符集和所有字符列12ALTER TABLE tbl_name CONVERT TO CHARACTER SET character_name [COLLATE ...]如：ALTER TABLE logtest CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci; 修改表的默认字符集12ALTER TABLE tbl_name DEFAULT CHARACTER SET character_name [COLLATE...];如：ALTER TABLE logtest DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci; 修改字段的字符集12ALTER TABLE tbl_name CHANGE c_name c_name CHARACTER SET character_name [COLLATE ...];如：ALTER TABLE logtest CHANGE title title VARCHAR(100) CHARACTER SET utf8 COLLATE utf8_general_ci; DCL用户创建用户123CREATE USER &#x27;ifan&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;12345&#x27;;GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, ALTER, REFERENCES, INDEX, DROP ON database.table TO &#x27;ifan&#x27;@&#x27;%&#x27;;FLUSH PRIVILEGES; 修改密码1set password for &#x27;ifan&#x27;@&#x27;%&#x27; = password(&#x27;123456&#x27;); 删除用户1DROP USER &#x27;ifan&#x27;@&#x27;%&#x27;; 删除用户权限12REVOKE DELETE, DROP ON database.table FROM &#x27;ifan&#x27;@&#x27;%&#x27;FLUSH PRIVILEGES;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql 基础知识 Explain","slug":"database/mysql/mysql-optimize-explain","date":"2022-06-08T10:25:00.000Z","updated":"2023-12-25T05:56:09.164Z","comments":true,"path":"2022/06/08/database/mysql/mysql-optimize-explain/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-optimize-explain/","excerpt":"","text":"1EXPLAIN SELECT * from user WHERE id = 1 字段 含义 id SELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_type SELECT 查询的类型. table 查询的是哪个表 partitions 匹配的分区 type join 类型 possible_keys 此次查询中可能选用的索引 key 此次查询中确切使用到的索引. ref 哪个字段或常数与 key 一起被使用 rows 显示此查询一共扫描了多少行. 这个是一个估计值. filtered 表示此查询条件所过滤的数据的百分比 extra 额外的信息 select_type查询的类型 取值 含义 SIMPLE 表示此查询不包含 UNION 查询或子查询 PRIMARY 表示此查询是最外层的查询 UNION 表示此查询是 UNION 的第二或随后的查询 DEPENDENT UNION, UNION 中的第二个或后面的查询语句, 取决于外面的查询 UNION RESULT, UNION 的结果 SUBQUERY 子查询中的第一个 SELECT DEPENDENT SUBQUERY 子查询中的第一个 SELECT, 取决于外面的查询. 即子查询依赖于外层查询的结果. type提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是 全表扫描 还是 索引扫描 等. 取值 含义 system 表中只有一条数据. 这个类型是特殊的 const 类型. const 针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可. eq_ref 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 &#x3D;, 查询效率较高. ref 此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了 最左前缀 规则索引的查询. range 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 &#x3D;, &lt;&gt;, &gt;, &gt;&#x3D;, &lt;, &lt;&#x3D;, IS NULL, &lt;&#x3D;&gt;, BETWEEN, IN() 操作中. 当 type 是 range 时, 那么 EXPLAIN 输出的 ref 字段为 NULL, 并且 key_len 字段是此次查询中使用到的索引的最长的那个. index 表示全索引扫描(full index scan), 和 ALL 类型类似, 只不过 ALL 类型是全表扫描, 而 index 类型则仅仅扫描所有的索引, 而不扫描数据. index 所要查询的数据直接在索引树中就可以获取到, 而不需要扫描数据. 当是这种情况时, Extra 字段 会显示 Using index. ALL 表示全表扫描, 这个类型的查询是性能最差的查询之一. 通常来说, 我们的查询不应该出现 ALL 类型的查询, 因为这样的查询在数据量大的情况下, 对数据库的性能是巨大的灾难. 如一个查询是 ALL 类型查询, 那么一般来说可以对相应的字段添加索引来避免. 性能比较 ALL &lt; index &lt; range ~ index_merge &lt; ref &lt; eq_ref &lt; const &lt; system possible_keys表示 MySQL 在查询时, 能够使用到的索引. 注意, 即使有些索引在 possible_keys 中出现, 但是并不表示此索引会真正地被 MySQL 使用到. MySQL 在查询时具体使用了哪些索引, 由 key 字段决定. 取值 含义 key 此字段是 MySQL 在当前查询时所真正使用到的索引. key_len 表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到. key_len 的计算规则如下: 字段属性 具体类型 字节数 字符串 char(n) n 字节长度 varchar(n) 如果是 utf8 编码, 则是 3 n + 2字节; 如果是 utf8mb4 编码, 则是 4 n + 2 字节. 数值类型 TINYINT 1字节 SMALLINT 2字节 MEDIUMINT 3字节 INT 4字节 BIGINT 8字节 时间类型 DATE 3字节 TIMESTAMP 4字节 DATETIME 8字节 NULL NULL rowsMySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数.这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好. ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: 取值 含义 Using filesort 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大. Using index “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using temporary 查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 查询效率不高, 建议优化.","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql 基础知识 查询优化","slug":"database/mysql/mysql-optimize-select","date":"2022-06-08T10:20:00.000Z","updated":"2023-12-25T05:56:09.164Z","comments":true,"path":"2022/06/08/database/mysql/mysql-optimize-select/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-optimize-select/","excerpt":"","text":"如何查询优化 优化数据访问，不返回不必要的数据，只返回必要的列（SELECT id, name）和必要的行数（LIMIT number） 查询尽量覆盖索引，避免回表的发生 优化数据存储，较大的字段单独存储在一个表中 将复杂的查询拆分成多个简单的查询，因为Mysql的链接时比较轻量级的 最好使用自增ID代替OFFSET逻辑查询 比较复杂的查询语句最好先使用 EXPLAIN 进行分析一遍 如果多个表是完全无关系的话，将记录和列名最少的表，写在最后，然后依次类推。也就是说：选择记录条数最少的表放在最后。 如果多个表是有关系的话，将引用最多的表，放在最后，然后依次类推。也就是说：被其他表所引用的表放在最后。 SQL语句执行流程 建立连接，权限验证 查询缓存，如果命中则直接返回 分析器：对 SQL 语句进行分析 优化器：对 SQL 查询的逻辑进行优化 关联表（join）的顺序可能会变 outer join可能会变成内连接 优化条件表达式， 例如 5&#x3D;5 AND a&gt;5被简化成a&gt;5 优化MAX\\MIN， 如果是MAX(索引），那么直接拿B+树的第一条或者最后一条即可。 当发现某个查询或者表达式的结果是可以提前计算出来的时候，就会优化成常数 索引覆盖，如果只要返回索引列，就不会走到最底层去。 子查询优化 提前终止查询（例如LIMIT） 等值传播： join中可能把左表的where 拿给右表一起用 IN(1,2,3,4,5,6)这个条件， 并不是简单遍历判断， 会先排序，然后用二分去判断是否存在。 执行器：执行查询语句查询结果 范式和反范式范式和反范式各有利弊，需要根据实际情况权衡。 范式化的目标是尽力减少冗余列，节省空间。 范式化的优点是： 减少冗余列，要写的数据就少，写操作的性能提高； 检索列数据时，DISTINCT 或 GROUP BY 操作减少。 范式化的缺点是：增加关联查询。 反范式化的目标是适当增加冗余列，以避免关联查询。 反范式化的缺点是： 冗余列增多，空间变大，写操作性能下降； 检索列数据时，DISTINCT 或 GROUP BY 操作变多；","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql 基础知识 工作流","slug":"database/mysql/mysql-workflow","date":"2022-06-08T10:15:00.000Z","updated":"2023-12-25T05:56:09.165Z","comments":true,"path":"2022/06/08/database/mysql/mysql-workflow/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-workflow/","excerpt":"","text":"1. 基础架构大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 2. 查询过程SQL 语句在 MySQL 中是如何执行的？ MySQL 整个查询执行过程，总的来说分为 6 个步骤： 客户端和 MySQL 服务器建立连接；客户端向 MySQL 服务器发送一条查询请求。 MySQL 服务器首先检查查询缓存，如果命中缓存，则立刻返回结果。否则进入下一阶段。 MySQL 服务器进行 SQL 分析：语法分析、词法分析。 MySQL 服务器用优化器生成对应的执行计划。 MySQL 服务器根据执行计划，调用存储引擎的 API 来执行查询。 MySQL 服务器将结果返回给客户端，同时缓存查询结果。 2.1. （一）连接使用 MySQL 第一步自然是要连接数据库。 MySQL 客户端&#x2F;服务端通信是半双工模式：即任一时刻，要么是服务端向客户端发送数据，要么是客户端向服务器发送数据。客户端用一个单独的数据包将查询请求发送给服务器，所以当查询语句很长的时候，需要设置max_allowed_packet参数。但是需要注意的是，如果查询实在是太大，服务端会拒绝接收更多数据并抛出异常。 MySQL 客户端连接命令：mysql -h&lt;主机&gt; -P&lt;端口&gt; -u&lt;用户名&gt; -p&lt;密码&gt;。如果没有显式指定密码，会要求输入密码才能访问。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。客户端如果太长时间没动静，连接器就会自动将它断开。客户端连接维持时间是由参数 wait_timeout 控制的，默认值是 8 小时。如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 建立连接的过程通常是比较复杂的，建议在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。为了在程序中提高数据库连接的服用了，一般会使用数据库连接池来维护管理。 但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 怎么解决这个问题呢？你可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 2.2. （二）查询缓存 不建议使用数据库缓存，因为往往弊大于利。 解析一个查询语句前，如果查询缓存是打开的，那么 MySQL 会检查这个查询语句是否命中查询缓存中的数据。如果当前查询恰好命中查询缓存，在检查一次用户权限后直接返回缓存中的结果。这种情况下，查询不会被解析，也不会生成执行计划，更不会执行。 MySQL 将缓存存放在一个引用表（不要理解成table，可以认为是类似于HashMap的数据结构），通过一个哈希值索引，这个哈希值通过查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息计算得来。所以两个查询在任何字符上的不同（例如：空格、注释），都会导致缓存不会命中。 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysql 库中的系统表，其查询结果都不会被缓存。比如函数NOW()或者CURRENT_DATE()会因为不同的查询时间，返回不同的查询结果，再比如包含CURRENT_USER或者CONNECION_ID()的查询语句会因为不同的用户而返回不同的结果，将这样的查询结果缓存起来没有任何的意义。 不建议使用数据库缓存，因为往往弊大于利。查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样： 1select SQL_CACHE * from T where ID=10; 注意：MySQL 8.0 版本直接将查询缓存的整块功能删掉了。 2.3. （三）语法分析如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。MySQL 通过关键字对 SQL 语句进行解析，并生成一颗对应的语法解析树。这个过程中，分析器主要通过语法规则来验证和解析。比如 SQL 中是否使用了错误的关键字或者关键字的顺序是否正确等等。预处理则会根据 MySQL 规则进一步检查解析树是否合法。比如检查要查询的数据表和数据列是否存在等等。 分析器先会先做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。 接下来，要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。 2.4. （四）查询优化经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 经过前面的步骤生成的语法树被认为是合法的了，并且由优化器将其转化成执行计划。多数情况下，一条查询可以有很多种执行方式，最后都返回相应的结果。优化器的作用就是找到这其中最好的执行计划。 MySQL 使用基于成本的优化器，它尝试预测一个查询使用某种执行计划时的成本，并选择其中成本最小的一个。在 MySQL 可以通过查询当前会话的 last_query_cost 的值来得到其计算当前查询的成本。 123456789mysql&gt; select * from t_message limit 10;...省略结果集mysql&gt; show status like &#x27;last_query_cost&#x27;;+-----------------+-------------+| Variable_name | Value |+-----------------+-------------+| Last_query_cost | 6391.799000 |+-----------------+-------------+ 示例中的结果表示优化器认为大概需要做 6391 个数据页的随机查找才能完成上面的查询。这个结果是根据一些列的统计信息计算得来的，这些统计信息包括：每张表或者索引的页面个数、索引的基数、索引和数据行的长度、索引的分布情况等等。 有非常多的原因会导致 MySQL 选择错误的执行计划，比如统计信息不准确、不会考虑不受其控制的操作成本（用户自定义函数、存储过程）、MySQL 认为的最优跟我们想的不一样（我们希望执行时间尽可能短，但 MySQL 值选择它认为成本小的，但成本小并不意味着执行时间短）等等。 MySQL 的查询优化器是一个非常复杂的部件，它使用了非常多的优化策略来生成一个最优的执行计划： 重新定义表的关联顺序（多张表关联查询时，并不一定按照 SQL 中指定的顺序进行，但有一些技巧可以指定关联顺序） 优化MIN()和MAX()函数（找某列的最小值，如果该列有索引，只需要查找 B+Tree 索引最左端，反之则可以找到最大值，具体原理见下文） 提前终止查询（比如：使用 Limit 时，查找到满足数量的结果集后会立即终止查询） 优化排序（在老版本 MySQL 会使用两次传输排序，即先读取行指针和需要排序的字段在内存中对其排序，然后再根据排序结果去读取数据行，而新版本采用的是单次传输排序，也就是一次读取所有的数据行，然后根据给定的列排序。对于 I&#x2F;O 密集型应用，效率会高很多） 随着 MySQL 的不断发展，优化器使用的优化策略也在不断的进化，这里仅仅介绍几个非常常用且容易理解的优化策略，其他的优化策略，大家自行查阅吧。 2.5. （五）查询执行引擎在完成解析和优化阶段以后，MySQL 会生成对应的执行计划，查询执行引擎根据执行计划给出的指令逐步执行得出结果。整个执行过程的大部分操作均是通过调用存储引擎实现的接口来完成，这些接口被称为handler API。查询过程中的每一张表由一个handler实例表示。实际上，MySQL 在查询优化阶段就为每一张表创建了一个handler实例，优化器可以根据这些实例的接口来获取表的相关信息，包括表的所有列名、索引统计信息等。存储引擎接口提供了非常丰富的功能，但其底层仅有几十个接口，这些接口像搭积木一样完成了一次查询的大部分操作。 2.6. （六）返回结果查询过程的最后一个阶段就是将结果返回给客户端。即使查询不到数据，MySQL 仍然会返回这个查询的相关信息，比如该查询影响到的行数以及执行时间等等。 如果查询缓存被打开且这个查询可以被缓存，MySQL 也会将结果存放到缓存中。 结果集返回客户端是一个增量且逐步返回的过程。有可能 MySQL 在生成第一条结果时，就开始向客户端逐步返回结果集了。这样服务端就无须存储太多结果而消耗过多内存，也可以让客户端第一时间获得返回结果。需要注意的是，结果集中的每一行都会以一个满足 ① 中所描述的通信协议的数据包发送，再通过 TCP 协议进行传输，在传输过程中，可能对 MySQL 的数据包进行缓存然后批量发送。 3. 更新过程MySQL 更新过程和 MySQL 查询过程类似，也会将流程走一遍。不一样的是：更新流程还涉及两个重要的日志模块，：redo log（重做日志）和 binlog（归档日志）。 3.1. redo logredo log 是 InnoDB 引擎特有的日志。redo log 即重做日志。redo log 是物理日志，记录的是“在某个数据页上做了什么修改”。 redo log 是基于 WAL 技术。WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘。具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log 里，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。 InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 3.2. bin logbin log 即归档日志。binlog 是逻辑日志，记录的是这个语句的原始逻辑。 binlog 是可以追加写入的，即写到一定大小后会切换到下一个，并不会覆盖以前的日志。 binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 3.3. redo log vs. bin log这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID&#x3D;2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 执行器先找引擎取 ID&#x3D;2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID&#x3D;2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 这里我给出这个 update 语句的执行流程图，图中浅色框表示是在 InnoDB 内部执行的，深色框表示是在执行器中执行的。 3.4. 两阶段提交redo log 的写入拆成了两个步骤：prepare 和 commit，这就是”两阶段提交”。为什么日志需要“两阶段提交”。 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql 基础知识 事务","slug":"database/mysql/mysql-transaction","date":"2022-06-08T10:10:00.000Z","updated":"2023-12-25T05:56:09.165Z","comments":true,"path":"2022/06/08/database/mysql/mysql-transaction/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-transaction/","excerpt":"","text":"1. 事务简介 事务简单来说：一个 Session 中所进行所有的操作，要么同时成功，要么同时失败。进一步说，事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 事务就是一组原子性的 SQL 语句。具体来说，事务指的是满足 ACID 特性的一组操作。 事务内的 SQL 语句，要么全执行成功，要么全执行失败。 通过加锁的方式，可以实现不同的事务隔离机制。 想象一下，如果没有事务，在并发环境下，就可能出现丢失修改的问题。 T1 和 T2 两个线程都对一个数据进行修改，T1 先修改，T2 随后修改，T2 的修改覆盖了 T1 的修改。 2. 事务用法2.1. 事务处理指令Mysql 中，使用 START TRANSACTION 语句开始一个事务；使用 COMMIT 语句提交所有的修改；使用 ROLLBACK 语句撤销所有的修改。不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。 START TRANSACTION - 指令用于标记事务的起始点。 SAVEPOINT - 指令用于创建保留点。 ROLLBACK TO - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 START TRANSACTION 语句处。 COMMIT - 提交事务。 事务处理示例： （1）创建一张示例表 12345678910-- 撤销表 userDROP TABLE IF EXISTS user;-- 创建表 userCREATE TABLE user ( id int(10) unsigned NOT NULL COMMENT &#x27;Id&#x27;, username varchar(64) NOT NULL DEFAULT &#x27;default&#x27; COMMENT &#x27;用户名&#x27;, password varchar(64) NOT NULL DEFAULT &#x27;default&#x27; COMMENT &#x27;密码&#x27;, email varchar(64) NOT NULL DEFAULT &#x27;default&#x27; COMMENT &#x27;邮箱&#x27;) COMMENT=&#x27;用户表&#x27;; （2）执行事务操作 12345678910111213141516171819-- 开始事务START TRANSACTION;-- 插入操作 AINSERT INTO `user`VALUES (1, &#x27;root1&#x27;, &#x27;root1&#x27;, &#x27;xxxx@163.com&#x27;);-- 创建保留点 updateASAVEPOINT updateA;-- 插入操作 BINSERT INTO `user`VALUES (2, &#x27;root2&#x27;, &#x27;root2&#x27;, &#x27;xxxx@163.com&#x27;);-- 回滚到保留点 updateAROLLBACK TO updateA;-- 提交事务，只有操作 A 生效COMMIT; （3）执行结果 1SELECT * FROM user; 结果： 11 root1 root1 xxxx@163.com 2.2. AUTOCOMMITMySQL 默认采用隐式提交策略（autocommit）。每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。 通过 set autocommit=0 可以取消自动提交，直到 set autocommit=1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。 12345678-- 查看 AUTOCOMMITSHOW VARIABLES LIKE &#x27;AUTOCOMMIT&#x27;;-- 关闭 AUTOCOMMITSET autocommit = 0;-- 开启 AUTOCOMMITSET autocommit = 1; 3. ACIDACID 是数据库事务正确执行的四个基本要素。 原子性（Atomicity） 事务被视为不可分割的最小单元，事务中的所有操作要么全部提交成功，要么全部失败回滚。 回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 一致性（Consistency） 数据库在事务执行前后都保持一致性状态。 在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性（Isolation） 一个事务所做的修改在最终提交以前，对其它事务是不可见的。 持久性（Durability） 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 一个支持事务（Transaction）中的数据库系统，必需要具有这四种特性，否则在事务过程（Transaction processing）当中无法保证数据的正确性，交易过程极可能达不到交易。 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并行执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对系统崩溃的情况。 MySQL 默认采用自动提交模式（AUTO COMMIT）。也就是说，如果不显式使用 START TRANSACTION 语句来开始一个事务，那么每个查询操作都会被当做一个事务并自动提交。 4. 事务隔离级别4.1. 事务隔离简介在并发环境下，事务的隔离性很难保证，因此会出现很多并发一致性问题： 丢失修改 脏读 不可重复读 幻读 在 SQL 标准中，定义了四种事务隔离级别（级别由低到高）： 未提交读 提交读 可重复读 串行化 Mysql 中查看和设置事务隔离级别： 1234567891011121314-- 查看事务隔离级别SHOW VARIABLES LIKE &#x27;transaction_isolation&#x27;;-- 设置事务隔离级别为 READ UNCOMMITTEDSET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;-- 设置事务隔离级别为 READ COMMITTEDSET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;-- 设置事务隔离级别为 REPEATABLE READSET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;-- 设置事务隔离级别为 SERIALIZABLESET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE; 4.2. 未提交读未提交读（READ UNCOMMITTED） 是指：事务中的修改，即使没有提交，对其它事务也是可见的。 未提交读的问题：事务可以读取未提交的数据，也被称为 脏读（Dirty Read）。 T1 修改一个数据，T2 随后读取这个数据。如果 T1 撤销了这次修改，那么 T2 读取的数据是脏数据。 4.3. 提交读提交读（READ COMMITTED） 是指：事务提交后，其他事务才能看到它的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。提交读解决了脏读的问题。 提交读是大多数数据库的默认事务隔离级别。 提交读有时也叫不可重复读，它的问题是：执行两次相同的查询，得到的结果可能不一致。 T2 读取一个数据，T1 对该数据做了修改。如果 T2 再次读取这个数据，此时读取的结果和第一次读取的结果不同。 4.4. 可重复读可重复读（REPEATABLE READ） 是指：保证在同一个事务中多次读取同样数据的结果是一样的。可重复读解决了不可重复读问题。 可重复读是 Mysql 的默认事务隔离级别。 可重复读的问题：当某个事务读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务又再次读取该范围的记录时，会产生 幻读（Phantom Read）。 T1 读取某个范围的数据，T2 在这个范围内插入新的数据，T1 再次读取这个范围的数据，此时读取的结果和和第一次读取的结果不同。 4.5. 串行化串行化（SERIALIXABLE） 是指：强制事务串行执行。 强制事务串行执行，则避免了所有的并发问题。串行化策略会在读取的每一行数据上都加锁，这可能导致大量的超时和锁竞争。这对于高并发应用基本上是不可接受的，所以一般不会采用这个级别。 4.6. 隔离级别小结 未提交读（READ UNCOMMITTED） - 事务中的修改，即使没有提交，对其它事务也是可见的。 提交读（READ COMMITTED） - 一个事务只能读取已经提交的事务所做的修改。换句话说，一个事务所做的修改在提交之前对其它事务是不可见的。 重复读（REPEATABLE READ） - 保证在同一个事务中多次读取同样数据的结果是一样的。 串行化（SERIALIXABLE） - 强制事务串行执行。 数据库隔离级别解决的问题： 隔离级别 丢失修改 脏读 不可重复读 幻读 未提交读 ✔️ ❌ ❌ ❌ 提交读 ✔️ ✔️ ❌ ❌ 可重复读 ✔️ ✔️ ✔️ ❌ 可串行化 ✔️ ✔️ ✔️ ✔️ 5. 死锁死锁是指两个或多个事务竞争同一资源，并请求锁定对方占用的资源，从而导致恶性循环的现象。 产生死锁的场景： 当多个事务试图以不同的顺序锁定资源时，就可能会产生死锁。 多个事务同时锁定同一个资源时，也会产生死锁。 5.1. 死锁的原因行锁的具体实现算法有三种：record lock、gap lock 以及 next-key lock。record lock 是专门对索引项加锁；gap lock 是对索引项之间的间隙加锁；next-key lock 则是前面两种的组合，对索引项以其之间的间隙加锁。 只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock，在 Select、Update 和 Delete 时，除了基于唯一索引的查询之外，其它索引查询时都会获取 gap lock 或 next-key lock，即锁住其扫描的范围。主键索引也属于唯一索引，所以主键索引是不会使用 gap lock 或 next-key lock。 在 MySQL 中，gap lock 默认是开启的，即 innodb_locks_unsafe_for_binlog 参数值是 disable 的，且 MySQL 中默认的是 RR 事务隔离级别。 当我们执行以下查询 SQL 时，由于 order_no 列为非唯一索引，此时又是 RR 事务隔离级别，所以 SELECT 的加锁类型为 gap lock，这里的 gap 范围是 (4,+∞）。 SELECT id FROM demo.order_record where order_no &#x3D; 4 for update; 执行查询 SQL 语句获取的 gap lock 并不会导致阻塞，而当我们执行以下插入 SQL 时，会在插入间隙上再次获取插入意向锁。插入意向锁其实也是一种 gap 锁，它与 gap lock 是冲突的，所以当其它事务持有该间隙的 gap lock 时，需要等待其它事务释放 gap lock 之后，才能获取到插入意向锁。 以上事务 A 和事务 B 都持有间隙 (4,+∞）的 gap 锁，而接下来的插入操作为了获取到插入意向锁，都在等待对方事务的 gap 锁释放，于是就造成了循环等待，导致死锁。 INSERT INTO demo.order_record(order_no, status, create_date) VALUES (5, 1, ‘2019-07-13 10:57:03’); 另一个死锁场景 InnoDB 存储引擎的主键索引为聚簇索引，其它索引为辅助索引。如果使用辅助索引来更新数据库，就需要使用聚簇索引来更新数据库字段。如果两个更新事务使用了不同的辅助索引，或一个使用了辅助索引，一个使用了聚簇索引，就都有可能导致锁资源的循环等待。由于本身两个事务是互斥，也就构成了以上死锁的四个必要条件了。 出现死锁的步骤： 综上可知，在更新操作时，我们应该尽量使用主键来更新表字段，这样可以有效避免一些不必要的死锁发生。 5.2. 避免死锁预防死锁的注意事项： 在编程中尽量按照固定的顺序来处理数据库记录，假设有两个更新操作，分别更新两条相同的记录，但更新顺序不一样，有可能导致死锁； 在允许幻读和不可重复读的情况下，尽量使用 RC 事务隔离级别，可以避免 gap lock 导致的死锁问题； 更新表时，尽量使用主键更新； 避免长事务，尽量将长事务拆解，可以降低与其它事务发生冲突的概率； 设置合理的锁等待超时参数，我们可以通过 innodb_lock_wait_timeout 设置合理的等待超时阈值，特别是在一些高并发的业务中，我们可以尽量将该值设置得小一些，避免大量事务等待，占用系统资源，造成严重的性能开销。 另外，我们还可以将 order_no 列设置为唯一索引列。虽然不能防止幻读，但我们可以利用它的唯一性来保证订单记录不重复创建，这种方式唯一的缺点就是当遇到重复创建订单时会抛出异常。 我们还可以使用其它的方式来代替数据库实现幂等性校验。例如，使用 Redis 以及 ZooKeeper 来实现，运行效率比数据库更佳。 5.3. 解决死锁当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。 但是，我们又不可能直接把这个时间设置成一个很小的值，比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。 所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。为了解决死锁问题，不同数据库实现了各自的死锁检测和超时机制。InnoDB 的处理策略是：将持有最少行级排它锁的事务进行回滚。 主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。你可以想象一下这个过程：每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。 6. 分布式事务在单一数据节点中，事务仅限于对单一数据库资源的访问控制，称之为 本地事务。几乎所有的成熟的关系型数据库都提供了对本地事务的原生支持。 分布式事务指的是事务操作跨越多个节点，并且要求满足事务的 ACID 特性。 分布式事务的常见方案如下： 两阶段提交（2PC） - 将事务的提交过程分为两个阶段来进行处理：准备阶段和提交阶段。参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。 三阶段提交（3PC） - 与二阶段提交不同的是，引入超时机制。同时在协调者和参与者中都引入超时机制。将二阶段的准备阶段拆分为 2 个阶段，插入了一个 preCommit 阶段，使得原先在二阶段提交中，参与者在准备之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题得以解决。 补偿事务（TCC） Try - 操作作为一阶段，负责资源的检查和预留。 Confirm - 操作作为二阶段提交操作，执行真正的业务。 Cancel - 是预留资源的取消。 本地消息表 - 在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。 MQ 事务 - 基于 MQ 的分布式事务方案其实是对本地消息表的封装。 SAGA - Saga 事务核心思想是将长事务拆分为多个本地短事务，由 Saga 事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。 分布式事务方案分析： 2PC&#x2F;3PC 依赖于数据库，能够很好的提供强一致性和强事务性，但相对来说延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。 TCC 适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。 本地消息表&#x2F;MQ 事务 都适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账&#x2F;校验系统兜底。 Saga 事务 由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。 Saga 相比缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。Saga 事务较适用于补偿动作容易处理的场景。 分布式事务详细说明、分析请参考：分布式事务基本原理 7. 事务最佳实践高并发场景下的事务到底该如何调优？ 7.1. 尽量使用低级别事务隔离结合业务场景，尽量使用低级别事务隔离 7.2. 避免行锁升级表锁在 InnoDB 中，行锁是通过索引实现的，如果不通过索引条件检索数据，行锁将会升级到表锁。我们知道，表锁是会严重影响到整张表的操作性能的，所以应该尽力避免。 7.3. 缩小事务范围有时候，数据库并发访问量太大，会出现以下异常： 1MySQLQueryInterruptedException: Query execution was interrupted 高并发时对一条记录进行更新的情况下，由于更新记录所在的事务还可能存在其他操作，导致一个事务比较长，当有大量请求进入时，就可能导致一些请求同时进入到事务中。 又因为锁的竞争是不公平的，当多个事务同时对一条记录进行更新时，极端情况下，一个更新操作进去排队系统后，可能会一直拿不到锁，最后因超时被系统打断踢出。 如上图中的操作，虽然都是在一个事务中，但锁的申请在不同时间，只有当其他操作都执行完，才会释放所有锁。因为扣除库存是更新操作，属于行锁，这将会影响到其他操作该数据的事务，所以我们应该尽量避免长时间地持有该锁，尽快释放该锁。又因为先新建订单和先扣除库存都不会影响业务，所以我们可以将扣除库存操作放到最后，也就是使用执行顺序 1，以此尽量减小锁的持有时间。 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql 基础知识 锁","slug":"database/mysql/mysql-lock","date":"2022-06-08T10:05:00.000Z","updated":"2023-12-25T05:56:09.164Z","comments":true,"path":"2022/06/08/database/mysql/mysql-lock/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-lock/","excerpt":"","text":"1. 悲观锁和乐观锁确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性，乐观锁和悲观锁是并发控制主要采用的技术手段。 悲观锁 - 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作 在查询完数据的时候就把事务锁起来，直到提交事务（COMMIT） 实现方式：使用数据库中的锁机制。 乐观锁 - 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 在修改数据的时候把事务锁起来，通过 version 的方式来进行锁定 实现方式：使用 version 版本或者时间戳。 【示例】乐观锁示例 商品 goods 表中有一个字段 status，status 为 1 代表商品未被下单，status 为 2 代表商品已经被下单，那么我们对某个商品下单时必须确保该商品 status 为 1。假设商品的 id 为 1。 12345select (status,status,version) from t_goods where id=#&#123;id&#125;update t_goodsset status=2,version=version+1where id=#&#123;id&#125; and version=#&#123;version&#125;; 2. 表级锁和行级锁从数据库的锁粒度来看，MySQL 中提供了两种封锁粒度：行级锁和表级锁。 表级锁（table lock） - 锁定整张表。用户对表进行写操作前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有没有写锁时，其他用户才能获得读锁，读锁之间不会相互阻塞。 行级锁（row lock） - 锁定指定的行记录。这样其它进程还是可以对同一个表中的其它记录进行操作。 应该尽量只锁定需要修改的那部分数据，而不是所有的资源。锁定的数据量越少，锁竞争的发生频率就越小，系统的并发程度就越高。但是加锁需要消耗资源，锁的各种操作（包括获取锁、释放锁、以及检查锁状态）都会增加系统开销。因此锁粒度越小，系统开销就越大。 在选择锁粒度时，需要在锁开销和并发程度之间做一个权衡。 在 InnoDB 中，行锁是通过给索引上的索引项加锁来实现的。如果没有索引，InnoDB 将会通过隐藏的聚簇索引来对记录加锁。 在 InnoDB 中，行锁是在需要的时候加上去的，但不是不需要了就立即释放，而是等到事务结束之后才释放，这就是两阶段锁协议。 3. 读写锁 独享锁（Exclusive），简写为 X 锁，又称写锁。使用方式：SELECT ... FOR UPDATE; 共享锁（Shared），简写为 S 锁，又称读锁。使用方式：SELECT ... LOCK IN SHARE MODE; 写锁和读锁的关系，简言之：独享锁存在，其他事务就不能做任何操作。 InnoDB 下的行锁、间隙锁、next-key 锁统统属于独享锁。 4. 意向锁当存在表级锁和行级锁的情况下，必须先申请意向锁（表级锁，但不是真的加锁），再获取行级锁。使用意向锁（Intention Locks）可以更容易地支持多粒度封锁。 意向锁是 InnoDB 自动加的，不需要用户干预。 在存在行级锁和表级锁的情况下，事务 T 想要对表 A 加 X 锁，就需要先检测是否有其它事务对表 A 或者表 A 中的任意一行加了锁，那么就需要对表 A 的每一行都检测一次，这是非常耗时的。 意向锁规定： IX&#x2F;IS 是表锁； X&#x2F;S 是行锁。 一个事务在获得某个数据行的 S 锁之前，必须先获得表的 IS 锁或者更强的锁； 一个事务在获得某个数据行的 X 锁之前，必须先获得表的 IX 锁。 通过引入意向锁，事务 T 想要对表 A 加 X 锁，只需要先检测是否有其它事务对表 A 加了 X&#x2F;IX&#x2F;S&#x2F;IS 锁，如果加了就表示有其它事务正在使用这个表或者表中某一行的锁，因此事务 T 加 X 锁失败。 各种锁的兼容关系如下： - X IX S IS X ❌ ❌ ❌ ❌ IX ❌ ✔️ ❌ ✔️ S ❌ ❌ ✔️ ✔️ IS ❌ ✔️ ✔️ ✔️ 解释如下： 任意 IS&#x2F;IX 锁之间都是兼容的，因为它们只表示想要对表加锁，而不是真正加锁； 这里兼容关系针对的是表级锁，而表级的 IX 锁和行级的 X 锁兼容，两个事务可以对两个数据行加 X 锁。（事务 T1 想要对数据行 R1 加 X 锁，事务 T2 想要对同一个表的数据行 R2 加 X 锁，两个事务都需要对该表加 IX 锁，但是 IX 锁是兼容的，并且 IX 锁与行级的 X 锁也是兼容的，因此两个事务都能加锁成功，对同一个表中的两个数据行做修改。） 5. MVCC多版本并发控制（Multi-Version Concurrency Control, MVCC）可以视为行级锁的一个变种。它在很多情况下都避免了加锁操作，因此开销更低。不仅是 Mysql，包括 Oracle、PostgreSQL 等其他数据库都实现了各自的 MVCC，实现机制没有统一标准。 MVCC 是 InnoDB 存储引擎实现隔离级别的一种具体方式，用于实现提交读和可重复读这两种隔离级别。而未提交读隔离级别总是读取最新的数据行，要求很低，无需使用 MVCC。可串行化隔离级别需要对所有读取的行都加锁，单纯使用 MVCC 无法实现。 5.1. MVCC 思想加锁能解决多个事务同时执行时出现的并发一致性问题。在实际场景中读操作往往多于写操作，因此又引入了读写锁来避免不必要的加锁操作，例如读和读没有互斥关系。读写锁中读和写操作仍然是互斥的。 MVCC 的思想是： 保存数据在某个时间点的快照，写操作（DELETE、INSERT、UPDATE）更新最新的版本快照；而读操作去读旧版本快照，没有互斥关系。这一点和 CopyOnWrite 类似。 脏读和不可重复读最根本的原因是事务读取到其它事务未提交的修改。在事务进行读取操作时，为了解决脏读和不可重复读问题，MVCC 规定只能读取已经提交的快照。当然一个事务可以读取自身未提交的快照，这不算是脏读。 5.2. 版本号InnoDB 的 MVCC 实现是：在每行记录后面保存两个隐藏列，一个列保存行的创建时间，另一个列保存行的过期时间（这里的时间是指系统版本号）。每开始一个新事务，系统版本号会自动递增，事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。 系统版本号 SYS_ID：是一个递增的数字，每开始一个新的事务，系统版本号就会自动递增。 事务版本号 TRX_ID ：事务开始时的系统版本号。 5.3. Undo 日志MVCC 的多版本指的是多个版本的快照，快照存储在 Undo 日志中，该日志通过回滚指针 ROLL_PTR 把一个数据行的所有快照连接起来。 例如在 MySQL 创建一个表 t，包含主键 id 和一个字段 x。我们先插入一个数据行，然后对该数据行执行两次更新操作。 123INSERT INTO t(id, x) VALUES(1, &quot;a&quot;);UPDATE t SET x=&quot;b&quot; WHERE id=1;UPDATE t SET x=&quot;c&quot; WHERE id=1; 因为没有使用 START TRANSACTION 将上面的操作当成一个事务来执行，根据 MySQL 的 AUTOCOMMIT 机制，每个操作都会被当成一个事务来执行，所以上面的操作总共涉及到三个事务。快照中除了记录事务版本号 TRX_ID 和操作之外，还记录了一个 bit 的 DEL 字段，用于标记是否被删除。 INSERT、UPDATE、DELETE 操作会创建一个日志，并将事务版本号 TRX_ID 写入。DELETE 可以看成是一个特殊的 UPDATE，还会额外将 DEL 字段设置为 1。 5.4. ReadViewMVCC 维护了一个一致性读视图 consistent read view ，主要包含了当前系统未提交的事务列表 TRX_IDs &#123;TRX_ID_1, TRX_ID_2, ...&#125;，还有该列表的最小值 TRX_ID_MIN 和 TRX_ID_MAX。 这样，对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能： 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见；b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 在进行 SELECT 操作时，根据数据行快照的 TRX_ID 与 TRX_ID_MIN 和 TRX_ID_MAX 之间的关系，从而判断数据行快照是否可以使用： TRX_ID &lt; TRX_ID_MIN，表示该数据行快照时在当前所有未提交事务之前进行更改的，因此可以使用。 TRX_ID &gt; TRX_ID_MAX，表示该数据行快照是在事务启动之后被更改的，因此不可使用。 TRX_ID_MIN &lt;&#x3D; TRX_ID &lt;&#x3D; TRX_ID_MAX，需要根据隔离级别再进行判断： 提交读：如果 TRX_ID 在 TRX_IDs 列表中，表示该数据行快照对应的事务还未提交，则该快照不可使用。否则表示已经提交，可以使用。 可重复读：都不可以使用。因为如果可以使用的话，那么其它事务也可以读到这个数据行快照并进行修改，那么当前事务再去读这个数据行得到的值就会发生改变，也就是出现了不可重复读问题。 在数据行快照不可使用的情况下，需要沿着 Undo Log 的回滚指针 ROLL_PTR 找到下一个快照，再进行上面的判断。 5.5. 快照读与当前读快照读 MVCC 的 SELECT 操作是快照中的数据，不需要进行加锁操作。 1SELECT * FROM table ...; 当前读 MVCC 其它会对数据库进行修改的操作（INSERT、UPDATE、DELETE）需要进行加锁操作，从而读取最新的数据。可以看到 MVCC 并不是完全不用加锁，而只是避免了 SELECT 的加锁操作。 123INSERT;UPDATE;DELETE; 在进行 SELECT 操作时，可以强制指定进行加锁操作。以下第一个语句需要加 S 锁，第二个需要加 X 锁。 12SELECT * FROM table WHERE ? lock in share mode;SELECT * FROM table WHERE ? for update; 6. 行锁行锁的具体实现算法有三种：record lock、gap lock 以及 next-key lock。 Record Lock - 行锁对索引项加锁，若没有索引则使用表锁。 Gap Lock - 对索引项之间的间隙加锁。锁定索引之间的间隙，但是不包含索引本身。例如当一个事务执行以下语句，其它事务就不能在 t.c 中插入 15：SELECT c FROM t WHERE c BETWEEN 10 and 20 FOR UPDATE;。在 MySQL 中，gap lock 默认是开启的，即 innodb_locks_unsafe_for_binlog 参数值是 disable 的，且 MySQL 中默认的是 RR 事务隔离级别。 Next-key lock -它是 Record Lock 和 Gap Lock 的结合，不仅锁定一个记录上的索引，也锁定索引之间的间隙。它锁定一个前开后闭区间。 只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock。在 Select、Update 和 Delete 时，除了基于唯一索引的查询之外，其它索引查询时都会获取 gap lock 或 next-key lock，即锁住其扫描的范围。主键索引也属于唯一索引，所以主键索引是不会使用 gap lock 或 next-key lock。 MVCC 不能解决幻读问题，Next-Key 锁就是为了解决幻读问题。在可重复读（REPEATABLE READ）隔离级别下，使用 MVCC + Next-Key 锁 可以解决幻读问题。 索引分为主键索引和非主键索引两种，如果一条 SQL 语句操作了主键索引，MySQL 就会锁定这条主键索引；如果一条语句操作了非主键索引，MySQL 会先锁定该非主键索引，再锁定相关的主键索引。在 UPDATE、DELETE 操作时，MySQL 不仅锁定 WHERE 条件扫描过的所有索引记录，而且会锁定相邻的键值，即所谓的 next-key lock。 当两个事务同时执行，一个锁住了主键索引，在等待其他相关索引。另一个锁定了非主键索引，在等待主键索引。这样就会发生死锁。发生死锁后，InnoDB 一般都可以检测到，并使一个事务释放锁回退，另一个获取锁完成事务。 7. 死锁和死锁检查查看死锁 1show engine innodb status \\G 发生死锁后，InnoDB 一般都能自动检测到，并使一个事务释放锁并回退，另一个事务获得锁，继续完成事务；但在涉及外部锁，或涉及表锁的情况下，InnoDB 并不能完全自动检测到死锁， 这需要通过设置锁等待超时参数 &#96;innodb_lock_wait_timeout&#96;&#96; 来解决 12# 死锁记录只记录最近一个死锁信息，若要将每个死锁信息都保存到错误日志，启用以下参数：show variables like &#x27;innodb_print_all_deadlocks&#x27;; 相关配置 12345[mysqld]log-error =/var/log/mysqld3306.loginnodb_lock_wait_timeout=60 #锁请求超时时间(秒)innodb_rollback_on_timeout = 1 #事务中某个语句锁请求超时将回滚真个事务innodb_print_all_deadlocks = 1 #死锁都保存到错误日志","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql 基础知识 索引","slug":"database/mysql/mysql-Indexes","date":"2022-06-08T10:00:00.000Z","updated":"2023-12-25T05:56:09.163Z","comments":true,"path":"2022/06/08/database/mysql/mysql-Indexes/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-Indexes/","excerpt":"","text":"索引是数据库为了提高查找效率的一种数据结构 1. 索引简介1.1. 索引的优缺点优点 减少扫描的数据量，提高检索速度 将随机IO变为顺序IO InnoDB 会减少锁的竞争，提高并发 唯一索引会保证数据的唯一性，减少业务的复杂性 帮助服务器避免排序和临时表。 缺点 闯将和维护索引会消耗时间 会占用一定的物理空间 INSERT&#x2F;UPDATE&#x2F;DELETE等操作可能会更新索引，导致数据库的写操作性能降低。 2. 索引概念2.1. 聚簇索引和非聚簇索引 聚簇索引：表中该行数据的索引和数据放在一起，一个表只能有一个聚簇索引。 非聚簇索引：表中改行数据的索引和数据分开存储 聚簇索引和非聚簇索引的查询有什么区别 如果语句是 select * from T where ID=1，即聚簇索引查询方式，则只需要搜索 ID 这棵 B+ 树； 如果语句是 select * from T where k=1，即非聚簇索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。 基于非聚簇索引的查询需要多扫描一棵索引树。 显然，主键长度越小，非聚簇索引的叶子节点就越小，非聚簇索引占用的空间也就越小。 2.2. 覆盖索引索引上的信息足够满足查询请求，不需要回表查询数据。 2.3. 前缀索引有时候需要索引很长的字符列，这会让索引变得大且慢。 这时，可以使用前缀索引，即只索引开始的部分字符，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的选择性。对于 BLOB&#x2F;TEXT&#x2F;VARCHAR 这种文本类型的列，必须使用前缀索引，因为数据库往往不允许索引这些列的完整长度。 索引的选择性是指：不重复的索引值和数据表记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。如果存在多条命中前缀索引的情况，就需要依次扫描，直到最终找到正确记录。 使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。 那么，如何确定前缀索引合适的长度呢？ 可以使用下面这个语句，算出这个列上有多少个不同的值： 1select count(distinct email) as L from SUser; 然后，依次选取不同长度的前缀来看这个值，比如我们要看一下 4~7 个字节的前缀索引，可以用这个语句： 123456select count(distinct left(email,4)）as L4, count(distinct left(email,5)）as L5, count(distinct left(email,6)）as L6, count(distinct left(email,7)）as L7,from SUser; 当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的 L4~L7 中，找出不小于 L * 95% 的值，假设这里 L6、L7 都满足，你就可以选择前缀长度为 6。 此外，**order by 无法使用前缀索引，无法把前缀索引用作覆盖索引**。 2.4. 最左前缀匹配原则不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 MySQL 会一直向右匹配直到遇到范围查询 &gt;,&lt;,BETWEEN,LIKE 就停止匹配。 索引可以简单如一个列(a)，也可以复杂如多个列(a, b, c, d)，即联合索引。 如果是联合索引，那么 key 也由多个列组成，同时，索引只能用于查找 key 是否存在（相等），遇到范围查询(&gt;、&lt;、between、like 左匹配)等就不能进一步匹配了，后续退化为线性查找。 因此，列的排列顺序决定了可命中索引的列数。 不要为每个列都创建独立索引。 将选择性高的列或基数大的列优先排在多列索引最前列。但有时，也需要考虑 WHERE 子句中的排序、分组和范围条件等因素，这些因素也会对查询性能造成较大影响。 例如：a = 1 and b = 2 and c &gt; 3 and d = 4，如果建立（a,b,c,d）顺序的索引，d 是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d 的顺序可以任意调整。 让选择性最强的索引列放在前面，索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，查询效率也越高。 例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。 1234SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,COUNT(*)FROM payment; 123 staff_id_selectivity: 0.0001customer_id_selectivity: 0.0373 COUNT(*): 16049 2.5. &#x3D; 和 in 可以乱序不需要考虑 =、IN 等的顺序，Mysql 会自动优化这些条件的顺序，以匹配尽可能多的索引列。 【示例】如有索引 (a, b, c, d)，查询条件 c &gt; 3 and b = 2 and a = 1 and d &lt; 4 与 a = 1 and c &gt; 3 and b = 2 and d &lt; 4 等顺序都是可以的，MySQL 会自动优化为 a &#x3D; 1 and b &#x3D; 2 and c &gt; 3 and d &lt; 4，依次命中 a、b、c、d。 3. 数据结构索引有很多种数据结构：哈希索引、全文索引，R-Tree索引等，这里只记录用的比较多的B+ Tree索引 B+Tree中的 B 是指balance，意为平衡。需要注意的是，B+树索引并不能找到一个给定键值的具体行，它找到的只是被查找数据行所在的页，接着数据库会把页读入到内存，再在内存中进行查找，最后得到要查找的数据。 为什么选择 B+树 二叉树：遇到极端情况，二叉树会退化成链表。 红黑树：当数据量很大时，树的深度也很深，搜索的数据越多，需要进行的磁盘IO就越多。 B树：如果搜索的数据量太大，无法通过一次从磁盘换出很大的数据。 B-树：每个节点中存在了Key和Data，导致每个节点存放的Key的数量很小，到存储数据量很大时，B-Tree的深度会很大，查询时IO次数增加，影响查询效率。 B+树：在叶节点存放数据，非叶节点存放关键字记录的指针，进行索引，保证每次搜索会加载更多的索引，提高搜索效率 2.1 B+ 树B+树的特点： 所有的数据都存储在叶子节点，非叶子节点并不存储真正的数据，所有记录节点都是按键值大小顺序存放在同一层叶子节点上。 所有的叶子节点由指针连接。 MyISAM索引文件和数据文件是分开的，InnoDB中表数据文件本身就是按照B+Tree组织的索引结构。这就造成了 MyISAM 回表表快，InnoDB 比较慢。 B+ 树索引适用于全键值查找、键值范围查找和键前缀查找，其中键前缀查找只适用于最左前缀查找。 InnoDB 要求表必须存在主键，如果不存在，则会自动生成一个隐含字段作为主键，长度为6个字节的长整形。 不建议使用过长的字段作为主键，因为二级索引都是引用主键索引，过长的主键会导致二级索引变的过大。所以用自增主键作为主键是一种比较好的选择 4. 索引的类型主流的关系型数据库一般都支持以下索引类型： 3.1. 主键索引（PRIMARY）主键索引：一种特殊的唯一索引，不允许有空值。一个表只能有一个主键（在 InnoDB 中本质上即聚簇索引），一般是在建表的时候同时创建主键索引。 12345CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT, ... PRIMARY KEY (`id`)) 3.2. 唯一索引（UNIQUE）唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一。 1234CREATE TABLE `table` ( ... UNIQUE indexName (title(length))) 3.3. 普通索引（INDEX）普通索引：最基本的索引，没有任何限制。 1234CREATE TABLE `table` ( ... INDEX index_name (title(length))) 3.4. 全文索引（FULLTEXT）全文索引：主要用来查找文本中的关键字，而不是直接与索引中的值相比较。 全文索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的 WHERE 语句的参数匹配。全文索引配合 match against 操作使用，而不是一般的 WHERE 语句加 LIKE。它可以在 CREATE TABLE，ALTER TABLE ，CREATE INDEX 使用，不过目前只有 char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用 CREATE INDEX 创建全文索引，要比先为一张表建立全文索引然后再将数据写入的速度快很多。 12345CREATE TABLE `table` ( `content` text CHARACTER NULL, ... FULLTEXT (content)) 3.5. 联合索引组合索引：多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合。 1234CREATE TABLE `table` ( ... INDEX index_name (title(length), title(length), ...)) 5. 索引最佳实践 只在高选择性字段添加索引，创建索引的字段尽量有很少的重复。 建立联合索引时，高选择性的字段放在前面。 字符字段必须创建前缀索引。 不要在索引上进行数学运算或函数运算。 自增列或者全局ID做innodb主键。 尽量使用覆盖索引进行查询，避免回表。 对经常更新的表避免创建过多的索引。 可以使用 EXPLAIN 或者 optimizer_trace 对 SQL 语句进行分析。 6. 参考文档 【MySQL】MySQL为什么用B+树做索引而不用B-树或红黑树？ MySQL8索引与调优篇1-索引的数据结构 数据库两大神器【索引和锁】 高性能Mysql","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql dump 备份数据","slug":"database/mysql/mysql-dump","date":"2022-06-08T10:00:00.000Z","updated":"2023-12-25T05:56:09.163Z","comments":true,"path":"2022/06/08/database/mysql/mysql-dump/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/mysql/mysql-dump/","excerpt":"","text":"只涉及到 mysqldump 工具的使用，没有 binlog 日志的恢复 备份1. 导出所有数据1mysqldump -uroot -proot --all-database &gt; all.sql 2. 备份指定数据库123# --add-drop-database 创建数据库之前先删除之前的库mysqldump -uroot -proot db1 --add-drop-database &gt; db1.sqlmysqldump -uroot -proot --databases db1 db2 --add-drop-database &gt; db1_and_db2.sql 3. 备份指定表123# --add-drop-table 创建表之前先删除之前的表# -single-transaction 添加全局锁，如果在导出多个表的时候，不添加全局锁，会导致两个表的数据不一致mysqldump -uroot -proot --databases db1 -single-transaction --tables t1 t2 &gt; t1_t2.sql 4. 根据条件导出1mysqldump -uroot -proot --databases db1 --tables t1 --where=&#x27;id=1&#x27; &gt; t1.sql 5. 只导出表结构1mysqldump -uroot -proot --no-data --databases db1 &gt; db1.sql 6. 只导出表数据1mysqldump -uroot -proot --no-data --databases db1 --tables t1 --no-create-db --no-create-info --skip-add-drop-table &gt; t1.sql 7. 数据不落地导出导入123mysqldump --host=h1 -uroot -proot --databases db1 |mysql --host=h2 -uroot -proot db2# 压缩传输 -Cmysqldump --host=h1 -uroot -proot -C --databases test |mysql --host=h2 -uroot -proot test 8. 导出数据为CSV1mysqldump -uroot -proot --no-data --databases db1 --tables t1 --fields-terminated-by &#x27;,&#x27; &gt; t1.sql 9. 使用sql语句备份指定查询的数据1select * from tablename into outfile &#x27;target_file&#x27; [option]; option 参数 1234567fields terminated by &#x27;string&#x27; // 字段分隔符，默认为制表符&#x27;\\t&#x27;fields [optionally] enclosed by &#x27;char&#x27; // 字段引用符，如果加 optionally 选项则只用在 char、varchar 和 text 等字符型字段上，默认不使用引用符 fields escaped by &#x27;char&#x27; // 转移字符、默认为 &#x27;\\&#x27; lines starting by &#x27;string&#x27; // 每行前都加此字符串，默认&#x27;&#x27; lines terminated by &#x27;string&#x27; // 行结束符，默认为&#x27;\\n&#x27; # char 表示此符号只能是单个字符，string表示可以是字符串。 导出csv文件 1select * from user into outfile &#x27;user.csv&#x27; fields terminated by &quot;,&quot; optionally enclosed by &#x27;&quot;&#x27;; 恢复1. 使用sql语句导入数据12source table.sqlmysqldump --host=h1 -uroot -proot &lt; backup.sql 2. 导入指定格式的文件1load data [local]infile &#x27;filename&#x27; into table tablename [option] option 选项 12345678fields terminated by &#x27;string&#x27; (字段分割符，默认为制表符&#x27;t&#x27;)fields [optionally] enclosed by &#x27;char&#x27; (字段引用符，如果加 optionally 选项则只用在 char varchar text 等字符型字段上。默认不使用引用符)fields escaped by &#x27;char&#x27; (转义字符，默认为&#x27;&#x27;)lines starting by &#x27;string&#x27; (每行前都加此字符串，默认为&#x27;&#x27;)lines terminated by &#x27;string&#x27; (行结束符，默认为&#x27;n&#x27;)ignore number lines (忽略输入文件中的前几行数据)(col_name_or_user_var,...) (按照列出的字段顺序和字段数量加载数据)set col_name = expr,...将列做一定的数值转换后再加载。 导入csv文件 1load data infile &#x27;user.csv&#x27; into table user fields terminated by &#x27;,&#x27; enclosed by &#x27;&quot;&#x27;; mysqldump 参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316--all-databases , -A导出全部数据库。mysqldump -uroot -p --all-databases--all-tablespaces , -Y导出全部表空间。mysqldump -uroot -p --all-databases --all-tablespaces--no-tablespaces , -y不导出任何表空间信息。mysqldump -uroot -p --all-databases --no-tablespaces--add-drop-database每个数据库创建之前添加drop数据库语句。mysqldump -uroot -p --all-databases --add-drop-database--add-drop-table每个数据表创建之前添加drop数据表语句。(默认为打开状态，使用--skip-add-drop-table取消选项)mysqldump -uroot -p --all-databases (默认添加drop语句)mysqldump -uroot -p --all-databases –skip-add-drop-table (取消drop语句)--add-locks在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE。(默认为打开状态，使用--skip-add-locks取消选项)mysqldump -uroot -p --all-databases (默认添加LOCK语句)mysqldump -uroot -p --all-databases –skip-add-locks (取消LOCK语句)--allow-keywords允许创建是关键词的列名字。这由表名前缀于每个列名做到。mysqldump -uroot -p --all-databases --allow-keywords--apply-slave-statements在&#x27;CHANGE MASTER&#x27;前添加&#x27;STOP SLAVE&#x27;，并且在导出的最后添加&#x27;START SLAVE&#x27;。mysqldump -uroot -p --all-databases --apply-slave-statements--character-sets-dir字符集文件的目录mysqldump -uroot -p --all-databases --character-sets-dir=/usr/local/mysql/share/mysql/charsets--comments附加注释信息。默认为打开，可以用--skip-comments取消mysqldump -uroot -p --all-databases (默认记录注释)mysqldump -uroot -p --all-databases --skip-comments (取消注释)--compatible导出的数据将和其它数据库或旧版本的MySQL 相兼容。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options、no_field_options等，要使用几个值，用逗号将它们隔开。它并不保证能完全兼容，而是尽量兼容。mysqldump -uroot -p --all-databases --compatible=ansi--compact导出更少的输出信息(用于调试)。去掉注释和头尾等结构。可以使用选项：--skip-add-drop-table --skip-add-locks --skip-comments --skip-disable-keysmysqldump -uroot -p --all-databases --compact--complete-insert, -c使用完整的insert语句(包含列名称)。这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败。mysqldump -uroot -p --all-databases --complete-insert--compress, -C在客户端和服务器之间启用压缩传递所有信息mysqldump -uroot -p --all-databases --compress--create-options, -a在CREATE TABLE语句中包括所有MySQL特性选项。(默认为打开状态)mysqldump -uroot -p --all-databases--databases, -B导出几个数据库。参数后面所有名字参量都被看作数据库名。mysqldump -uroot -p --databases test mysql--debug输出debug信息，用于调试。默认值为：d:t,/tmp/mysqldump.tracemysqldump -uroot -p --all-databases --debugmysqldump -uroot -p --all-databases --debug=&quot; d:t,/tmp/debug.trace&quot;--debug-check检查内存和打开文件使用说明并退出。mysqldump -uroot -p --all-databases --debug-check--debug-info输出调试信息并退出mysqldump -uroot -p --all-databases --debug-info--default-character-set设置默认字符集，默认值为utf8mysqldump -uroot -p --all-databases --default-character-set=utf8--delayed-insert采用延时插入方式（INSERT DELAYED）导出数据mysqldump -uroot -p --all-databases --delayed-insert--delete-master-logsmaster备份后删除日志. 这个参数将自动激活--master-data。mysqldump -uroot -p --all-databases --delete-master-logs--disable-keys对于每个表，用/*!40000 ALTER TABLE tbl_name DISABLE KEYS */;和/*!40000 ALTER TABLE tbl_name ENABLE KEYS */;语句引用INSERT语句。这样可以更快地导入dump出来的文件，因为它是在插入所有行后创建索引的。该选项只适合MyISAM表，默认为打开状态。mysqldump -uroot -p --all-databases --dump-slave该选项将主的binlog位置和文件名追加到导出数据的文件中(show slave status)。设置为1时，将会以CHANGE MASTER命令输出到数据文件；设置为2时，会在change前加上注释。该选项将会打开--lock-all-tables，除非--single-transaction被指定。该选项会自动关闭--lock-tables选项。默认值为0。mysqldump -uroot -p --all-databases --dump-slave=1mysqldump -uroot -p --all-databases --dump-slave=2 --master-data该选项将当前服务器的binlog的位置和文件名追加到输出文件中(show master status)。如果为1，将会输出CHANGE MASTER 命令；如果为2，输出的CHANGE MASTER命令前添加注释信息。该选项将打开--lock-all-tables 选项，除非--single-transaction也被指定（在这种情况下，全局读锁在开始导出时获得很短的时间；其他内容参考下面的--single-transaction选项）。该选项自动关闭--lock-tables选项。mysqldump -uroot -p --host=localhost --all-databases --master-data=1;mysqldump -uroot -p --host=localhost --all-databases --master-data=2; --events, -E导出事件。mysqldump -uroot -p --all-databases --events--extended-insert, -e使用具有多个VALUES列的INSERT语法。这样使导出文件更小，并加速导入时的速度。默认为打开状态，使用--skip-extended-insert取消选项。mysqldump -uroot -p --all-databasesmysqldump -uroot -p --all-databases--skip-extended-insert (取消选项)--fields-terminated-by导出文件中忽略给定字段。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p test test --tab=&quot;/home/mysql&quot; --fields-terminated-by=&quot;#&quot;--fields-enclosed-by输出文件中的各个字段用给定字符包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p test test --tab=&quot;/home/mysql&quot; --fields-enclosed-by=&quot;#&quot;--fields-optionally-enclosed-by输出文件中的各个字段用给定字符选择性包裹。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p test test --tab=&quot;/home/mysql&quot; --fields-enclosed-by=&quot;#&quot; --fields-optionally-enclosed-by =&quot;#&quot;--fields-escaped-by输出文件中的各个字段忽略给定字符。与--tab选项一起使用，不能用于--databases和--all-databases选项mysqldump -uroot -p mysql user --tab=&quot;/home/mysql&quot; --fields-escaped-by=&quot;#&quot;--flush-logs开始导出之前刷新日志。请注意：假如一次导出多个数据库(使用选项--databases或者--all-databases)，将会逐个数据库刷新日志。除使用--lock-all-tables或者--master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。因此，如果打算同时导出和刷新日志应该使用--lock-all-tables 或者--master-data 和--flush-logs。mysqldump -uroot -p --all-databases --flush-logs--flush-privileges在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句。为了正确恢复，该选项应该用于导出mysql数据库和依赖mysql数据库数据的任何时候。mysqldump -uroot -p --all-databases --flush-privileges--force在导出过程中忽略出现的SQL错误。mysqldump -uroot -p --all-databases --force--help显示帮助信息并退出。mysqldump --help--hex-blob使用十六进制格式导出二进制字符串字段。如果有二进制数据就必须使用该选项。影响到的字段类型有BINARY、VARBINARY、BLOB。mysqldump -uroot -p --all-databases --hex-blob--host, -h需要导出的主机信息mysqldump -uroot -p --host=localhost --all-databases--ignore-table不导出指定表。指定忽略多个表时，需要重复多次，每次一个表。每个表必须同时指定数据库和表名。例如：--ignore-table=database.table1 --ignore-table=database.table2 ……mysqldump -uroot -p --host=localhost --all-databases --ignore-table=mysql.user--include-master-host-port在--dump-slave产生的&#x27;CHANGE MASTER TO..&#x27;语句中增加&#x27;MASTER_HOST=&lt;host&gt;，MASTER_PORT=&lt;port&gt;&#x27; mysqldump -uroot -p --host=localhost --all-databases --include-master-host-port--insert-ignore在插入行时使用INSERT IGNORE语句.mysqldump -uroot -p --host=localhost --all-databases --insert-ignore--lines-terminated-by输出文件的每行用给定字符串划分。与--tab选项一起使用，不能用于--databases和--all-databases选项。mysqldump -uroot -p --host=localhost test test --tab=&quot;/tmp/mysql&quot; --lines-terminated-by=&quot;##&quot;--lock-all-tables, -x提交请求锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局读锁，并且自动关闭--single-transaction 和--lock-tables 选项。mysqldump -uroot -p --host=localhost --all-databases --lock-all-tables--lock-tables, -l开始导出前，锁定所有表。用READ LOCAL锁定表以允许MyISAM表并行插入。对于支持事务的表例如InnoDB和BDB，--single-transaction是一个更好的选择，因为它根本不需要锁定表。请注意当导出多个数据库时，--lock-tables分别为每个数据库锁定表。因此，该选项不能保证导出文件中的表在数据库之间的逻辑一致性。不同数据库表的导出状态可以完全不同。mysqldump -uroot -p --host=localhost --all-databases --lock-tables--log-error附加警告和错误信息到给定文件mysqldump -uroot -p --host=localhost --all-databases --log-error=/tmp/mysqldump_error_log.err--max_allowed_packet服务器发送和接受的最大包长度。mysqldump -uroot -p --host=localhost --all-databases --max_allowed_packet=10240--net_buffer_lengthTCP/IP和socket连接的缓存大小。mysqldump -uroot -p --host=localhost --all-databases --net_buffer_length=1024--no-autocommit使用autocommit/commit 语句包裹表。mysqldump -uroot -p --host=localhost --all-databases --no-autocommit--no-create-db, -n只导出数据，而不添加CREATE DATABASE 语句。mysqldump -uroot -p --host=localhost --all-databases --no-create-db--no-create-info, -t只导出数据，而不添加CREATE TABLE 语句。mysqldump -uroot -p --host=localhost --all-databases --no-create-info--no-data, -d不导出任何数据，只导出数据库表结构。mysqldump -uroot -p --host=localhost --all-databases --no-data--no-set-names, -N等同于--skip-set-charsetmysqldump -uroot -p --host=localhost --all-databases --no-set-names--opt等同于--add-drop-table, --add-locks, --create-options, --quick, --extended-insert, --lock-tables, --set-charset, --disable-keys 该选项默认开启, 可以用--skip-opt禁用.mysqldump -uroot -p --host=localhost --all-databases --opt--order-by-primary如果存在主键，或者第一个唯一键，对每个表的记录进行排序。在导出MyISAM表到InnoDB表时有效，但会使得导出工作花费很长时间。 mysqldump -uroot -p --host=localhost --all-databases --order-by-primary--password, -p连接数据库密码--pipe(windows系统可用)使用命名管道连接mysqlmysqldump -uroot -p --host=localhost --all-databases --pipe--port, -P连接数据库端口号--protocol使用的连接协议，包括：tcp, socket, pipe, memory.mysqldump -uroot -p --host=localhost --all-databases --protocol=tcp--quick, -q不缓冲查询，直接导出到标准输出。默认为打开状态，使用--skip-quick取消该选项。mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-quick--quote-names,-Q使用（`）引起表和列名。默认为打开状态，使用--skip-quote-names取消该选项。mysqldump -uroot -p --host=localhost --all-databasesmysqldump -uroot -p --host=localhost --all-databases --skip-quote-names--replace使用REPLACE INTO 取代INSERT INTO.mysqldump -uroot -p --host=localhost --all-databases --replace--result-file, -r直接输出到指定文件中。该选项应该用在使用回车换行对（\\\\r\\\\n）换行的系统上（例如：DOS，Windows）。该选项确保只有一行被使用。mysqldump -uroot -p --host=localhost --all-databases --result-file=/tmp/mysqldump_result_file.txt--routines, -R导出存储过程以及自定义函数。mysqldump -uroot -p --host=localhost --all-databases --routines--set-charset添加&#x27;SET NAMES default_character_set&#x27;到输出文件。默认为打开状态，使用--skip-set-charset关闭选项。mysqldump -uroot -p --host=localhost --all-databases mysqldump -uroot -p --host=localhost --all-databases --skip-set-charset--single-transaction该选项在导出数据之前提交一个BEGIN SQL语句，BEGIN 不会阻塞任何应用程序且能保证导出时数据库的一致性状态。它只适用于多版本存储引擎，仅InnoDB。本选项和--lock-tables 选项是互斥的，因为LOCK TABLES 会使任何挂起的事务隐含提交。要想导出大表的话，应结合使用--quick 选项。mysqldump -uroot -p --host=localhost --all-databases --single-transaction--dump-date将导出时间添加到输出文件中。默认为打开状态，使用--skip-dump-date关闭选项。mysqldump -uroot -p --host=localhost --all-databasesmysqldump -uroot -p --host=localhost --all-databases --skip-dump-date--skip-opt禁用–opt选项.mysqldump -uroot -p --host=localhost --all-databases --skip-opt--socket,-S指定连接mysql的socket文件位置，默认路径/tmp/mysql.sockmysqldump -uroot -p --host=localhost --all-databases --socket=/tmp/mysqld.sock--tab,-T为每个表在给定路径创建tab分割的文本文件。注意：仅仅用于mysqldump和mysqld服务器运行在相同机器上。mysqldump -uroot -p --host=localhost test test --tab=&quot;/home/mysql&quot;--tables覆盖--databases (-B)参数，指定需要导出的表名。mysqldump -uroot -p --host=localhost --databases test --tables test--triggers导出触发器。该选项默认启用，用--skip-triggers禁用它。mysqldump -uroot -p --host=localhost --all-databases --triggers--tz-utc在导出顶部设置时区TIME_ZONE=&#x27;+00:00&#x27; ，以保证在不同时区导出的TIMESTAMP 数据或者数据被移动其他时区时的正确性。mysqldump -uroot -p --host=localhost --all-databases --tz-utc--user, -u指定连接的用户名。--verbose, --v输出多种平台信息。--version, -V输出mysqldump版本信息并退出--where, -w只转储给定的WHERE条件选择的记录。请注意如果条件包含命令解释符专用空格或字符，一定要将条件引用起来。mysqldump -uroot -p --host=localhost --all-databases --where=&quot; user=&#x27;root&#x27;&quot;--xml, -X导出XML格式.mysqldump -uroot -p --host=localhost --all-databases --xml--plugin_dir客户端插件的目录，用于兼容不同的插件版本。mysqldump -uroot -p --host=localhost --all-databases --plugin_dir=&quot;/usr/local/lib/plugin&quot;--default_auth客户端插件默认使用权限。mysqldump -uroot -p --host=localhost --all-databases --default-auth=&quot;/usr/local/lib/plugin/&lt;PLUGIN&gt;&quot;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"ElasticSearch 安装","slug":"database/elasticsearch/elasticsearch-interview","date":"2022-06-08T01:45:00.000Z","updated":"2023-12-25T05:56:09.160Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-interview/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-interview/","excerpt":"","text":"Elasticsearch 面试总结集群部署ES 部署情况： 5 节点（配置：8 核 64 G 1T），总计 320 G，5 T。 约 10+ 索引，5 分片，每日新增数据量约为 2G，4000w 条。记录保存 30 天。 性能优化filesystem cache你往 es 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去。 es 的搜索引擎严重依赖于底层的 filesystem cache ，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。 性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1 秒、5 秒、10 秒。但如果是走 filesystem cache ，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。 这里有个真实的案例。某个公司 es 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G 。每台机器给 es jvm heap 是 32G ，那么剩下来留给 filesystem cache 的就是每台机器才 32G ，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，es 数据量是 1T ，那么每台机器的数据量是 300G 。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。 归根结底，你要让 es 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。 根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 es 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。 比如说你现在有一行数据。 id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql&#x2F;hbase 里，我们一般是建议用 es + hbase 这么一个架构。 hbase 的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id ，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。 写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。 数据预热假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。 其实可以做数据预热。 举个例子，拿微博来说，你可以把一些大 V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。 或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。 对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。 冷热分离es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。 你看，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。 document 模型设计对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。 document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join&#x2F;nested&#x2F;parent-child 搜索都要尽量避免，性能都很差的。 分页性能优化es 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 shard 上存储的前 1000 条数据都查到一个协调节点上，如果你有个 5 个 shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。 分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 shard，每个 shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？你必须得从每个 shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。你翻页的时候，翻的越深，每个 shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 es 做分页的时候，你会发现越翻到后面，就越是慢。 我们之前也是遇到过这个问题，用 es 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。 有什么解决方案吗？ 不允许深度分页（默认深度分页性能很差）跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。 类似于 app 里的推荐商品不断下拉出来一页一页的类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api ，关于如何使用，自行上网搜索。 scroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。 但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。 初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。 除了用 scroll api ，你也可以用 search_after 来做， search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。 1.1、设计阶段调优 （1）根据业务增量需求，采取基于日期模板创建索引，通过 roll over API 滚动索引； （2）使用别名进行索引管理； （3）每天凌晨定时对索引做 force_merge 操作，以释放空间； （4）采取冷热分离机制，热数据存储到 SSD，提高检索效率；冷数据定期进行 shrink 操作，以缩减存储； （5）采取 curator 进行索引的生命周期管理； （6）仅针对需要分词的字段，合理的设置分词器； （7）Mapping 阶段充分结合各个字段的属性，是否需要检索、是否需要存储等。…….. 1.2、写入调优 （1）写入前副本数设置为 0； （2）写入前关闭 refresh_interval 设置为-1，禁用刷新机制； （3）写入过程中：采取 bulk 批量写入； （4）写入后恢复副本数和刷新间隔； （5）尽量使用自动生成的 id。 1.3、查询调优 （1）禁用 wildcard； （2）禁用批量 terms（成百上千的场景）； （3）充分利用倒排索引机制，能 keyword 类型尽量 keyword； （4）数据量大时候，可以先基于时间敲定索引再检索； （5）设置合理的路由机制。 1.4、其他调优 部署调优，业务调优等。 上面的提及一部分，面试者就基本对你之前的实践或者运维经验有所评估了。 工作原理es 写数据过程 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node （协调节点）。 coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node 。 coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。 es 读数据过程可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。 客户端发送请求到任意一个 node，成为 coordinate node 。 coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。 接收请求的 node 返回 document 给 coordinate node 。 coordinate node 返回 document 给客户端。 es 搜索数据过程es 最强大的是做全文检索，就是比如你有三条数据： 123java真好玩儿啊java好难学啊j2ee特别牛 你根据 java 关键词来搜索，将包含 java 的 document 给搜索出来。es 就会给你返回：java 真好玩儿啊，java 好难学啊。 客户端发送请求到一个 coordinate node 。 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard ，都可以。 query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id ）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。 fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。 写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。 写数据底层原理先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。 如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据 refresh 到一个新的 segment file 中，但是此时数据不是直接进入 segment file 磁盘文件，而是先进入 os cache 。这个过程就是 refresh 。 每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file ，每秒钟会产生一个新的磁盘文件 segment file ，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。 但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。 操作系统里面，磁盘文件其实都有一个东西，叫做 os cache ，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache ，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache 中，这个数据就可以被搜索到了。 为什么叫 es 是准实时的？ NRT ，全称 near real-time 。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api 或者 java api ，手动执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 os cache 中，让数据立马就可以被搜索到。只要数据被输入 os cache 中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。 重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将 buffer 数据写入一个又一个新的 segment file 中去，每次 refresh 完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发 commit 操作。 commit 操作发生第一步，就是将 buffer 中现有数据 refresh 到 os cache 中去，清空 buffer。然后，将一个 commit point 写入磁盘文件，里面标识着这个 commit point 对应的所有 segment file ，同时强行将 os cache 中目前所有的数据都 fsync 到磁盘文件中去。最后清空 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。 这个 commit 操作叫做 flush 。默认 30 分钟自动执行一次 flush ，但如果 translog 过大，也会触发 flush 。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。 translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 translog 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。 translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会丢失 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 fsync 到磁盘，但是性能会差很多。 实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的数据丢失。 总结一下，数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。 数据写入 segment file 之后，同时就建立好了倒排索引。 删除&#x2F;更新数据底层原理如果是删除操作，commit 的时候会生成一个 .del 文件，里面将某个 doc 标识为 deleted 状态，那么搜索的时候根据 .del 文件就知道这个 doc 是否被删除了。 如果是更新操作，就是将原来的 doc 标识为 deleted 状态，然后新写入一条数据。 buffer 每 refresh 一次，就会产生一个 segment file ，所以默认情况下是 1 秒钟一个 segment file ，这样下来 segment file 会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个 segment file 合并成一个，同时这里会将标识为 deleted 的 doc 给物理删除掉，然后将新的 segment file 写入磁盘，这里会写一个 commit point ，标识所有新的 segment file ，然后打开 segment file 供搜索使用，同时删除旧的 segment file 。 底层 lucene简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。 通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。 倒排索引在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。 那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。 举个栗子。 有以下文档： DocId Doc 1 谷歌地图之父跳槽 Facebook 2 谷歌地图之父加盟 Facebook 3 谷歌地图创始人拉斯离开谷歌加盟 Facebook 4 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关 5 谷歌地图之父拉斯加盟社交网站 Facebook 对文档进行分词之后，得到以下倒排索引。 WordId Word DocIds 1 谷歌 1, 2, 3, 4, 5 2 地图 1, 2, 3, 4, 5 3 之父 1, 2, 4, 5 4 跳槽 1, 4 5 Facebook 1, 2, 3, 4, 5 6 加盟 2, 3, 5 7 创始人 3 8 拉斯 3, 5 9 离开 3 10 与 4 .. .. .. 另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。 那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 Facebook ，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。 要注意倒排索引的两个重要细节： 倒排索引中的所有词项对应一个或多个文档； 倒排索引中的词项根据字典顺序升序排列 上面只是一个简单的栗子，并没有严格按照字典顺序升序排列。 elasticsearch 的倒排索引是什么面试官：想了解你对基础概念的认知。 解答：通俗解释一下就可以。 传统的我们的检索是通过文章，逐个遍历找到对应关键词的位置。 而倒排索引，是通过分词策略，形成了词和文章的映射关系表，这种词典+映射表即为倒排索引。有了倒排索引，就能实现 o（1）时间复杂度的效率检索文章了，极大的提高了检索效率。 学术的解答方式： 倒排索引，相反于一篇文章包含了哪些词，它从词出发，记载了这个词在哪些文档中出现过，由两部分组成——词典和倒排表。 加分项：倒排索引的底层实现是基于：FST（Finite State Transducer）数据结构。 lucene 从 4+版本后开始大量使用的数据结构是 FST。FST 有两个优点： （1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间； （2）查询速度快。O(len(str))的查询时间复杂度。 3、elasticsearch 索引数据多了怎么办，如何调优，部署面试官：想了解大数据量的运维能力。 解答：索引数据的规划，应在前期做好规划，正所谓“设计先行，编码在后”，这样才能有效的避免突如其来的数据激增导致集群处理能力不足引发的线上客户检索或者其他业务受到影响。 如何调优，正如问题 1 所说，这里细化一下： 3.1 动态索引层面 基于模板+时间+rollover api 滚动创建索引，举例：设计阶段定义：blog 索引的模板格式为：blogindex时间戳的形式，每天递增数据。这样做的好处：不至于数据量激增导致单个索引数据量非常大，接近于上线 2 的 32 次幂-1，索引存储达到了 TB+甚至更大。 一旦单个索引很大，存储等各种风险也随之而来，所以要提前考虑+及早避免。 3.2 存储层面 冷热数据分离存储，热数据（比如最近 3 天或者一周的数据），其余为冷数据。 对于冷数据不会再写入新数据，可以考虑定期 force_merge 加 shrink 压缩操作，节省存储空间和检索效率。 3.3 部署层面 一旦之前没有规划，这里就属于应急策略。 结合 ES 自身的支持动态扩展的特点，动态新增机器的方式可以缓解集群压力，注意：如果之前主节点等规划合理，不需要重启集群也能完成动态新增的。 4、elasticsearch 是如何实现 master 选举的面试官：想了解 ES 集群的底层原理，不再只关注业务层面了。 解答： 前置前提： （1）只有候选主节点（master：true）的节点才能成为主节点。 （2）最小主节点数（min_master_nodes）的目的是防止脑裂。 核对了一下代码，核心入口为 findMaster，选择主节点成功返回对应 Master，否则返回 null。选举流程大致描述如下： 第一步：确认候选主节点数达标，elasticsearch.yml 设置的值 discovery.zen.minimum_master_nodes； 第二步：比较：先判定是否具备 master 资格，具备候选主节点资格的优先返回； 若两节点都为候选主节点，则 id 小的值会主节点。注意这里的 id 为 string 类型。 题外话：获取节点 id 的方法。 1231GET /_cat/nodes?v&amp;h=ip,port,heapPercent,heapMax,id,name2ip port heapPercent heapMax id name 详细描述一下 Elasticsearch 索引文档的过程面试官：想了解 ES 的底层原理，不再只关注业务层面了。 解答： 这里的索引文档应该理解为文档写入 ES，创建索引的过程。 文档写入包含：单文档写入和批量 bulk 写入，这里只解释一下：单文档写入流程。 记住官方文档中的这个图。 第一步：客户写集群某节点写入数据，发送请求。（如果没有指定路由&#x2F;协调节点，请求的节点扮演路由节点的角色。） 第二步：节点 1 接受到请求后，使用文档_id 来确定文档属于分片 0。请求会被转到另外的节点，假定节点 3。因此分片 0 的主分片分配到节点 3 上。 第三步：节点 3 在主分片上执行写操作，如果成功，则将请求并行转发到节点 1 和节点 2 的副本分片上，等待结果返回。所有的副本分片都报告成功，节点 3 将向协调节点（节点 1）报告成功，节点 1 向请求客户端报告写入成功。 如果面试官再问：第二步中的文档获取分片的过程？ 回答：借助路由算法获取，路由算法就是根据路由和文档 id 计算目标的分片 id 的过程。 11shard = hash(_routing) % (num_of_primary_shards) 详细描述一下 Elasticsearch 搜索的过程？面试官：想了解 ES 搜索的底层原理，不再只关注业务层面了。 解答： 搜索拆解为“query then fetch” 两个阶段。 query 阶段的目的：定位到位置，但不取。 步骤拆解如下： （1）假设一个索引数据有 5 主+1 副本 共 10 分片，一次请求会命中（主或者副本分片中）的一个。 （2）每个分片在本地进行查询，结果返回到本地有序的优先队列中。 （3）第 2）步骤的结果发送到协调节点，协调节点产生一个全局的排序列表。 fetch 阶段的目的：取数据。 路由节点获取所有文档，返回给客户端。 Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法面试官：想了解对 ES 集群的运维能力。 解答： （1）关闭缓存 swap; （2）堆内存设置为：Min（节点内存&#x2F;2, 32GB）; （3）设置最大文件句柄数； （4）线程池+队列大小根据业务需要做调整； （5）磁盘存储 raid 方式——存储有条件使用 RAID10，增加单节点性能以及避免单节点存储故障。 lucence 内部结构是什么？面试官：想了解你的知识面的广度和深度。 解答： Lucene 是有索引和搜索的两个过程，包含索引创建，索引，搜索三个要点。可以基于这个脉络展开一些。 Elasticsearch 是如何实现 Master 选举的？（1）Elasticsearch 的选主是 ZenDiscovery 模块负责的，主要包含 Ping（节点之间通过这个 RPC 来发现彼此）和 Unicast（单播模块包含一个主机列表以控制哪些节点需要 ping 通）这两部分； （2）对所有可以成为 master 的节点（node.master: true）根据 nodeId 字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个（第 0 位）节点，暂且认为它是 master 节点。 （3）如果对某个节点的投票数达到一定的值（可以成为 master 节点数 n&#x2F;2+1）并且该节点自己也选举自己，那这个节点就是 master。否则重新选举一直到满足上述条件。 （4）补充：master 节点的职责主要包括集群、节点和索引的管理，不负责文档级别的管理；data 节点可以关闭 http 功能*。 10、Elasticsearch 中的节点（比如共 20 个），其中的 10 个选了一个 master，另外 10 个选了另一个 master，怎么办？ （1）当集群 master 候选数量不小于 3 个时，可以通过设置最少投票通过数量（discovery.zen.minimum_master_nodes）超过所有候选节点一半以上来解决脑裂问题； （3）当候选数量为两个时，只能修改为唯一的一个 master 候选，其他作为 data 节点，避免脑裂问题。 客户端在和集群连接时，如何选择特定的节点执行请求的？TransportClient 利用 transport 模块远程连接一个 elasticsearch 集群。它并不加入到集群中，只是简单的获得一个或者多个初始化的 transport 地址，并以 轮询 的方式与这些地址进行通信。 详细描述一下 Elasticsearch 索引文档的过程。协调节点默认使用文档 ID 参与计算（也支持通过 routing），以便为路由提供合适的分片。 1shard = hash(document_id) % (num_of_primary_shards) （1）当分片所在的节点接收到来自协调节点的请求后，会将请求写入到 MemoryBuffer，然后定时（默认是每隔 1 秒）写入到 Filesystem Cache，这个从 MomeryBuffer 到 Filesystem Cache 的过程就叫做 refresh； （2）当然在某些情况下，存在 Momery Buffer 和 Filesystem Cache 的数据可能会丢失，ES 是通过 translog 的机制来保证数据的可靠性的。其实现机制是接收到请求后，同时也会写入到 translog 中 ，当 Filesystem cache 中的数据写入到磁盘中时，才会清除掉，这个过程叫做 flush； （3）在 flush 过程中，内存中的缓冲将被清除，内容被写入一个新段，段的 fsync 将创建一个新的提交点，并将内容刷新到磁盘，旧的 translog 将被删除并开始一个新的 translog。 （4）flush 触发的时机是定时触发（默认 30 分钟）或者 translog 变得太大（默认为 512M）时； 补充：关于 Lucene 的 Segement： （1）Lucene 索引是由多个段组成，段本身是一个功能齐全的倒排索引。 （2）段是不可变的，允许 Lucene 将新的文档增量地添加到索引中，而不用从头重建索引。 （3）对于每一个搜索请求而言，索引中的所有段都会被搜索，并且每个段会消耗 CPU 的时钟周、文件句柄和内存。这意味着段的数量越多，搜索性能会越低。 （4）为了解决这个问题，Elasticsearch 会合并小段到一个较大的段，提交新的合并段到磁盘，并删除那些旧的小段。 详细描述一下 Elasticsearch 更新和删除文档的过程。（1）删除和更新也都是写操作，但是 Elasticsearch 中的文档是不可变的，因此不能被删除或者改动以展示其变更； （2）磁盘上的每个段都有一个相应的.del 文件。当删除请求发送后，文档并没有真的被删除，而是在.del 文件中被标记为删除。该文档依然能匹配查询，但是会在结果中被过滤掉。当段合并时，在.del 文件中被标记为删除的文档将不会被写入新段。 （3）在新的文档被创建时，Elasticsearch 会为该文档指定一个版本号，当执行更新时，旧版本的文档在.del 文件中被标记为删除，新版本的文档被索引到一个新段。旧版本的文档依然能匹配查询，但是会在结果中被过滤掉。 详细描述一下 Elasticsearch 搜索的过程。（1）搜索被执行成一个两阶段过程，我们称之为 Query Then Fetch； （2）在初始查询阶段时，查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的大小为 from + size 的优先队列。 PS：在搜索的时候是会查询 Filesystem Cache 的，但是有部分数据还在 MemoryBuffer，所以搜索是近实时的。 （3）每个分片返回各自优先队列中 所有文档的 ID 和排序值 给协调节点，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。 （4）接下来就是 取回阶段，协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。每个分片加载并 丰 富 文档，如果有需要的话，接着返回文档给协调节点。一旦所有的文档都被取回了，协调节点返回结果给客户端。 （5）补充：Query Then Fetch 的搜索类型在文档相关性打分的时候参考的是本分片的数据，这样在文档数量较少的时候可能不够准确，DFS Query Then Fetch 增加了一个预查询的处理，询问 Term 和 Document frequency，这个评分更准确，但是性能会变差。* 在 Elasticsearch 中，是怎么根据一个词找到对应的倒排索引的？（1）Lucene 的索引过程，就是按照全文检索的基本过程，将倒排表写成此文件格式的过程。 （2）Lucene 的搜索过程，就是按照此文件格式将索引进去的信息读出来，然后计算每篇文档打分(score)的过程。 Elasticsearch 在部署时，对 Linux 的设置有哪些优化方法？（1）64 GB 内存的机器是非常理想的， 但是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会适得其反。 （2）如果你要在更快的 CPUs 和更多的核心之间选择，选择更多的核心更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。 （3）如果你负担得起 SSD，它将远远超出任何旋转介质。 基于 SSD 的节点，查询和索引性能都有提升。如果你负担得起，SSD 是一个好的选择。 （4）即使数据中心们近在咫尺，也要避免集群跨越多个数据中心。绝对要避免集群跨越大的地理距离。 （5）请确保运行你应用程序的 JVM 和服务器的 JVM 是完全一样的。 在 Elasticsearch 的几个地方，使用 Java 的本地序列化。 （6）通过设置 gateway.recover_after_nodes、gateway.expected_nodes、gateway.recover_after_time 可以在集群重启的时候避免过多的分片交换，这可能会让数据恢复从数个小时缩短为几秒钟。 （7）Elasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。最好使用单播代替组播。 （8）不要随意修改垃圾回收器（CMS）和各个线程池的大小。 （9）把你的内存的（少于）一半给 Lucene（但不要超过 32 GB！），通过 ES_HEAP_SIZE 环境变量设置。 （10）内存交换到磁盘对服务器性能来说是致命的。如果内存交换到磁盘上，一个 100 微秒的操作可能变成 10 毫秒。 再想想那么多 10 微秒的操作时延累加起来。 不难看出 swapping 对于性能是多么可怕。 （11）Lucene 使用了大 量 的文件。同时，Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字。 所有这一切都需要足够的文件描述符。你应该增加你的文件描述符，设置一个很大的值，如 64,000。 补充：索引阶段性能提升方法 （1）使用批量请求并调整其大小：每次批量数据 5–15 MB 大是个不错的起始点。 （2）存储：使用 SSD （3）段和合并：Elasticsearch 默认值是 20 MB&#x2F;s，对机械磁盘应该是个不错的设置。如果你用的是 SSD，可以考虑提高到 100–200 MB&#x2F;s。如果你在做批量导入，完全不在意搜索，你可以彻底关掉合并限流。另外还可以增加 index.translog.flush_threshold_size 设置，从默认的 512 MB 到更大一些的值，比如 1 GB，这可以在一次清空触发的时候在事务日志里积累出更大的段。 （4）如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s。 （5）如果你在做大批量导入，考虑通过设置 index.number_of_replicas: 0 关闭副本。 对于 GC 方面，在使用 Elasticsearch 时要注意什么？（1）倒排词典的索引需要常驻内存，无法 GC，需要监控 data node 上 segmentmemory 增长趋势。 （2）各类缓存，field cache, filter cache, indexing cache, bulk queue 等等，要设置合理的大小，并且要应该根据最坏的情况来看 heap 是否够用，也就是各类缓存全部占满的时候，还有 heap 空间可以分配给其他任务吗？避免采用 clear cache 等“自欺欺人”的方式来释放内存。 （3）避免返回大量结果集的搜索与聚合。确实需要大量拉取数据的场景，可以采用 scan &amp; scroll api 来实现。 （4）cluster stats 驻留内存并无法水平扩展，超大规模集群可以考虑分拆成多个集群通过 tribe node 连接。 （5）想知道 heap 够不够，必须结合实际应用场景，并对集群的 heap 使用情况做持续的监控。 （6）根据监控数据理解内存需求，合理配置各类 circuit breaker，将内存溢出风险降低到最低 18、Elasticsearch 对于大数据量（上亿量级）的聚合如何实现？Elasticsearch 提供的首个近似聚合是 cardinality 度量。它提供一个字段的基数，即该字段的 distinct 或者 unique 值的数目。它是基于 HLL 算法的。HLL 会先对我们的输入作哈希运算，然后根据哈希运算的结果中的 bits 做概率估算从而得到基数。其特点是：可配置的精度，用来控制内存的使用（更精确 ＝ 更多内存）；小的数据集精度是非常高的；我们可以通过配置参数，来设置去重需要的固定内存使用量。无论数千还是数十亿的唯一值，内存使用量只与你配置的精确度相关。 19、在并发情况下，Elasticsearch 如果保证读写一致？（1）可以通过版本号使用乐观并发控制，以确保新版本不会被旧版本覆盖，由应用层来处理具体的冲突； （2）另外对于写操作，一致性级别支持 quorum&#x2F;one&#x2F;all，默认为 quorum，即只有当大多数分片可用时才允许写操作。但即使大多数可用，也可能存在因为网络等原因导致写入副本失败，这样该副本被认为故障，分片将会在一个不同的节点上重建。 （3）对于读操作，可以设置 replication 为 sync(默认)，这使得操作在主分片和副本分片都完成后才会返回；如果设置 replication 为 async 时，也可以通过设置搜索请求参数_preference 为 primary 来查询主分片，确保文档是最新版本。 20、如何监控 Elasticsearch 集群状态？Marvel 让你可以很简单的通过 Kibana 监控 Elasticsearch。你可以实时查看你的集群健康状态和性能，也可以分析过去的集群、索引和节点指标。 21、介绍下你们电商搜索的整体技术架构。 介绍一下你们的个性化搜索方案？基于 word2vec 和 Elasticsearch 实现个性化搜索 （1）基于 word2vec、Elasticsearch 和自定义的脚本插件，我们就实现了一个个性化的搜索服务，相对于原有的实现，新版的点击率和转化率都有大幅的提升； （2）基于 word2vec 的商品向量还有一个可用之处，就是可以用来实现相似商品的推荐； （3）使用 word2vec 来实现个性化搜索或个性化推荐是有一定局限性的，因为它只能处理用户点击历史这样的时序数据，而无法全面的去考虑用户偏好，这个还是有很大的改进和提升的空间； 是否了解字典树？常用字典数据结构如下所示： Trie 的核心思想是空间换时间，利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。它有 3 个基本性质： 1）根节点不包含字符，除根节点外每一个节点都只包含一个字符。 2）从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。 3）每个节点的所有子节点包含的字符都不相同。 （1）可以看到，trie 树每一层的节点数是 26^i 级别的。所以为了节省空间，我们还可以用动态链表，或者用数组来模拟动态。而空间的花费，不会超过单词数 × 单词长度。 （2）实现：对每个结点开一个字母集大小的数组，每个结点挂一个链表，使用左儿子右兄弟表示法记录这棵树； （3）对于中文的字典树，每个节点的子节点用一个哈希表存储，这样就不用浪费太大的空间，而且查询速度上可以保留哈希的复杂度 O(1)。 拼写纠错是如何实现的？（1）拼写纠错是基于编辑距离来实现；编辑距离是一种标准的方法，它用来表示经过插入、删除和替换操作从一个字符串转换到另外一个字符串的最小操作步数； （2）编辑距离的计算过程：比如要计算 batyu 和 beauty 的编辑距离，先创建一个 7×8 的表（batyu 长度为 5，coffee 长度为 6，各加 2），接着，在如下位置填入黑色数字。其他格的计算过程是取以下三个值的最小值： 如果最上方的字符等于最左方的字符，则为左上方的数字。否则为左上方的数字+1。（对于 3,3 来说为 0） 左方数字+1（对于 3,3 格来说为 2） 上方数字+1（对于 3,3 格来说为 2） 最终取右下角的值即为编辑距离的值 3。 对于拼写纠错，我们考虑构造一个度量空间（Metric Space），该空间内任何关系满足以下三条基本条件： d(x,y) &#x3D; 0 – 假如 x 与 y 的距离为 0，则 x&#x3D;y d(x,y) &#x3D; d(y,x) – x 到 y 的距离等同于 y 到 x 的距离 d(x,y) + d(y,z) &gt;&#x3D; d(x,z) – 三角不等式 （1）根据三角不等式，则满足与 query 距离在 n 范围内的另一个字符转 B，其与 A 的距离最大为 d+n，最小为 d-n。 （2）BK 树的构造就过程如下：每个节点有任意个子节点，每条边有个值表示编辑距离。所有子节点到父节点的边上标注 n 表示编辑距离恰好为 n。比如，我们有棵树父节点是”book”和两个子节点”cake”和”books”，”book”到”books”的边标号 1，”book”到”cake”的边上标号 4。从字典里构造好树后，无论何时你想插入新单词时，计算该单词与根节点的编辑距离，并且查找数值为 d(neweord, root)的边。递归得与各子节点进行比较，直到没有子节点，你就可以创建新的子节点并将新单词保存在那。比如，插入”boo”到刚才上述例子的树中，我们先检查根节点，查找 d(“book”, “boo”) &#x3D; 1 的边，然后检查标号为 1 的边的子节点，得到单词”books”。我们再计算距离 d(“books”, “boo”)&#x3D;2，则将新单词插在”books”之后，边标号为 2。 （3）查询相似词如下：计算单词与根节点的编辑距离 d，然后递归查找每个子节点标号为 d-n 到 d+n（包含）的边。假如被检查的节点与搜索单词的距离 d 小于 n，则返回该节点并继续查询。比如输入 cape 且最大容忍距离为 1，则先计算和根的编辑距离 d(“book”, “cape”)&#x3D;4，然后接着找和根节点之间编辑距离为 3 到 5 的，这个就找到了 cake 这个节点，计算 d(“cake”, “cape”)&#x3D;1，满足条件所以返回 cake，然后再找和 cake 节点编辑距离是 0 到 2 的，分别找到 cape 和 cart 节点，这样就得到 cape 这个满足条件的结果。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch Rest 查询","slug":"database/elasticsearch/elasticsearch-search","date":"2022-06-08T01:40:00.000Z","updated":"2023-12-25T05:56:09.161Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-search/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-search/","excerpt":"","text":"Elasticsearch 查询Elasticsearch 查询语句采用基于 RESTful 风格的接口封装成 JSON 格式的对象，称之为 Query DSL。Elasticsearch 查询分类大致分为全文查询、词项查询、复合查询、嵌套查询、位置查询、特殊查询。Elasticsearch 查询从机制分为两种，一种是根据用户输入的查询词，通过排序模型计算文档与查询词之间的相关度，并根据评分高低排序返回；另一种是过滤机制，只根据过滤条件对文档进行过滤，不计算评分，速度相对较快。 1. 全文查询ES 全文查询主要用于在全文字段上，主要考虑查询词与文档的相关性（Relevance）。 1.1. intervals queryintervals query 根据匹配词的顺序和近似度返回文档。 intervals query 使用匹配规则，这些规则应用于指定字段中的 term。 示例：下面示例搜索 query 字段，搜索值是 my favorite food，没有任何间隙；然后是 my_text 字段搜索匹配 hot water、cold porridge 的 term。 当 my_text 中的值为 my favorite food is cold porridge 时，会匹配成功，但是 when it&#39;s cold my favorite food is porridge 则匹配失败 1234567891011121314151617181920212223242526272829POST _search&#123; &quot;query&quot;: &#123; &quot;intervals&quot; : &#123; &quot;my_text&quot; : &#123; &quot;all_of&quot; : &#123; &quot;ordered&quot; : true, &quot;intervals&quot; : [ &#123; &quot;match&quot; : &#123; &quot;query&quot; : &quot;my favorite food&quot;, &quot;max_gaps&quot; : 0, &quot;ordered&quot; : true &#125; &#125;, &#123; &quot;any_of&quot; : &#123; &quot;intervals&quot; : [ &#123; &quot;match&quot; : &#123; &quot;query&quot; : &quot;hot water&quot; &#125; &#125;, &#123; &quot;match&quot; : &#123; &quot;query&quot; : &quot;cold porridge&quot; &#125; &#125; ] &#125; &#125; ] &#125; &#125; &#125; &#125;&#125; 1.2. match querymatch query 用于搜索单个字段，首先会针对查询语句进行解析（经过 analyzer），主要是对查询语句进行分词，分词后查询语句的任何一个词项被匹配，文档就会被搜到，默认情况下相当于对分词后词项进行 or 匹配操作。 match query 是执行全文搜索的标准查询，包括模糊匹配选项。 12345678910GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;customer_full_name&quot;: &#123; &quot;query&quot;: &quot;George Hubbard&quot; &#125; &#125; &#125;&#125; 等同于 or 匹配操作，如下： 1234567891011GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;customer_full_name&quot;: &#123; &quot;query&quot;: &quot;George Hubbard&quot;, &quot;operator&quot;: &quot;or&quot; &#125; &#125; &#125;&#125; match query 简写可以通过组合 &lt;field&gt; 和 query 参数来简化匹配查询语法。 示例： 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;message&quot;: &quot;this is a test&quot; &#125; &#125;&#125; match query 如何工作匹配查询是布尔类型。这意味着会对提供的文本进行分析，分析过程从提供的文本构造一个布尔查询。 operator 参数可以设置为 or 或 and 来控制布尔子句（默认为 or）。可以使用 minimum_should_match 参数设置要匹配的可选 should 子句的最小数量。 1234567891011GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;customer_full_name&quot;: &#123; &quot;query&quot;: &quot;George Hubbard&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 可以设置 analyzer 来控制哪个分析器将对文本执行分析过程。它默认为字段显式映射定义或默认搜索分析器。 lenient 参数可以设置为 true 以忽略由数据类型不匹配导致的异常，例如尝试使用文本查询字符串查询数字字段。默认为 false。 match query 的模糊查询fuzziness 允许基于被查询字段的类型进行模糊匹配。请参阅 Fuzziness 的配置。 在这种情况下可以设置 prefix_length 和 max_expansions 来控制模糊匹配。如果设置了模糊选项，查询将使用 top_terms_blended_freqs_$&#123;max_expansions&#125; 作为其重写方法，fuzzy_rewrite 参数允许控制查询将如何被重写。 默认情况下允许模糊倒转 (ab → ba)，但可以通过将 fuzzy_transpositions 设置为 false 来禁用。 1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;message&quot;: &#123; &quot;query&quot;: &quot;this is a testt&quot;, &quot;fuzziness&quot;: &quot;AUTO&quot; &#125; &#125; &#125;&#125; zero terms 查询如果使用的分析器像 stop 过滤器一样删除查询中的所有标记，则默认行为是不匹配任何文档。可以使用 zero_terms_query 选项来改变默认行为，它接受 none（默认）和 all （相当于 match_all 查询）。 123456789101112GET /_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;message&quot;: &#123; &quot;query&quot;: &quot;to be or not to be&quot;, &quot;operator&quot;: &quot;and&quot;, &quot;zero_terms_query&quot;: &quot;all&quot; &#125; &#125; &#125;&#125; 1.3. match_bool_prefix querymatch_bool_prefix query 分析其输入并根据这些词构造一个布尔查询。除了最后一个术语之外的每个术语都用于术语查询。最后一个词用于 prefix query。 示例： 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;match_bool_prefix&quot; : &#123; &quot;message&quot; : &quot;quick brown f&quot; &#125; &#125;&#125; 等价于 123456789101112GET /_search&#123; &quot;query&quot;: &#123; &quot;bool&quot; : &#123; &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;message&quot;: &quot;quick&quot; &#125;&#125;, &#123; &quot;term&quot;: &#123; &quot;message&quot;: &quot;brown&quot; &#125;&#125;, &#123; &quot;prefix&quot;: &#123; &quot;message&quot;: &quot;f&quot;&#125;&#125; ] &#125; &#125;&#125; match_bool_prefix query 和 match_phrase_prefix query 之间的一个重要区别是：match_phrase_prefix query 将其 term 匹配为短语，但 match_bool_prefix query 可以在任何位置匹配其 term。 上面的示例 match_bool_prefix query 查询可以匹配包含 quick brown fox 的字段，但它也可以快速匹配 brown fox。它还可以匹配包含 quick、brown 和以 f 开头的字段，出现在任何位置。 1.4. match_phrase querymatch_phrase query 即短语匹配，首先会把 query 内容分词，分词器可以自定义，同时文档还要满足以下两个条件才会被搜索到： 分词后所有词项都要出现在该字段中（相当于 and 操作）。 字段中的词项顺序要一致。 例如，有以下 3 个文档，使用 match_phrase 查询 “How are you”，只有前两个文档会被匹配： 1234567891011121314151617PUT demo/_create/1&#123; &quot;desc&quot;: &quot;How are you&quot; &#125;PUT demo/_create/2&#123; &quot;desc&quot;: &quot;How are you, Jack?&quot;&#125;PUT demo/_create/3&#123; &quot;desc&quot;: &quot;are you&quot;&#125;GET demo/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;desc&quot;: &quot;How are you&quot; &#125; &#125;&#125; 说明： 一个被认定为和短语 How are you 匹配的文档，必须满足以下这些要求： How、 are 和 you 需要全部出现在域中。 are 的位置应该比 How 的位置大 1 。 you 的位置应该比 How 的位置大 2 。 1.5. match_phrase_prefix querymatch_phrase_prefix query 和 match_phrase query 类似，只不过 match_phrase_prefix query 最后一个 term 会被作为前缀匹配。 12345678GET demo/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;desc&quot;: &quot;are yo&quot; &#125; &#125;&#125; 1.6. multi_match querymulti_match query 是 match query 的升级，用于搜索多个字段。 示例： 123456789101112GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: 34.98, &quot;fields&quot;: [ &quot;taxful_total_price&quot;, &quot;taxless_total_price&quot; ] &#125; &#125;&#125; multi_match query 的搜索字段可以使用通配符指定，示例如下： 123456789101112GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: 34.98, &quot;fields&quot;: [ &quot;taxful_*&quot;, &quot;taxless_total_price&quot; ] &#125; &#125;&#125; 同时，也可以用指数符指定搜索字段的权重。 示例：指定 taxful_total_price 字段的权重是 taxless_total_price 字段的 3 倍，命令如下： 123456789101112GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: 34.98, &quot;fields&quot;: [ &quot;taxful_total_price^3&quot;, &quot;taxless_total_price&quot; ] &#125; &#125;&#125; 1.7. combined_fields querycombined_fields query 支持搜索多个文本字段，就好像它们的内容已被索引到一个组合字段中一样。该查询会生成以 term 为中心的输入字符串视图：首先它将查询字符串解析为独立的 term，然后在所有字段中查找每个 term。当匹配结果可能跨越多个文本字段时，此查询特别有用，例如文章的标题、摘要和正文： 12345678910GET /_search&#123; &quot;query&quot;: &#123; &quot;combined_fields&quot; : &#123; &quot;query&quot;: &quot;database systems&quot;, &quot;fields&quot;: [ &quot;title&quot;, &quot;abstract&quot;, &quot;body&quot;], &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125; 字段前缀权重字段前缀权重根据组合字段模型进行计算。例如，如果 title 字段的权重为 2，则匹配度打分时会将 title 中的每个 term 形成的组合字段，按出现两次进行打分。 1.8. common_terms query 7.3.0 废弃 common_terms query 是一种在不牺牲性能的情况下替代停用词提高搜索准确率和召回率的方案。 查询中的每个词项都有一定的代价，以搜索“The brown fox”为例，query 会被解析成三个词项“the”“brown”和“fox”，每个词项都会到索引中执行一次查询。很显然包含“the”的文档非常多，相比其他词项，“the”的重要性会低很多。传统的解决方案是把“the”当作停用词处理，去除停用词之后可以减少索引大小，同时在搜索时减少对停用词的收缩。 虽然停用词对文档评分影响不大，但是当停用词仍然有重要意义的时候，去除停用词就不是完美的解决方案了。如果去除停用词，就无法区分“happy”和“not happy”, “The”“To be or not to be”就不会在索引中存在，搜索的准确率和召回率就会降低。 common_terms query 提供了一种解决方案，它把 query 分词后的词项分成重要词项（低频词项）和不重要的词项（高频词，也就是之前的停用词）。在搜索的时候，首先搜索和重要词项匹配的文档，这些文档是词项出现较少并且词项对其评分影响较大的文档。然后执行第二次查询，搜索对评分影响较小的高频词项，但是不计算所有文档的评分，而是只计算第一次查询已经匹配的文档得分。如果一个查询中只包含高频词，那么会通过 and 连接符执行一个单独的查询，换言之，会搜索所有的词项。 词项是高频词还是低频词是通过 cutoff frequency 来设置阀值的，取值可以是绝对频率（频率大于 1）或者相对频率（0 ～ 1）。common_terms query 最有趣之处在于它能自适应特定领域的停用词，例如，在视频托管网站上，诸如“clip”或“video”之类的高频词项将自动表现为停用词，无须保留手动列表。 例如，文档频率高于 0.1% 的词项将会被当作高频词项，词频之间可以用 low_freq_operator、high_freq_operator 参数连接。设置低频词操作符为“and”使所有的低频词都是必须搜索的，示例代码如下： 123456789101112GET books/_search&#123; &quot;query&quot;: &#123; &quot;common&quot;: &#123; &quot;body&quot;: &#123; &quot;query&quot;: &quot;nelly the elephant as a cartoon&quot;, &quot;cutoff_frequency&quot;: 0.001, &quot;low_freq_operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 上述操作等价于： 1234567891011121314151617GET books/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;term&quot;: &#123; &quot;body&quot;: &quot;nelly&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;body&quot;: &quot;elephant&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;body&quot;: &quot;cartoon&quot; &#125; &#125; ], &quot;should&quot;: [ &#123; &quot;term&quot;: &#123; &quot;body&quot;: &quot;the&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;body&quot;: &quot;as&quot; &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;body&quot;: &quot;a&quot; &#125; &#125; ] &#125; &#125;&#125; 1.9. query_string queryquery_string query 是与 Lucene 查询语句的语法结合非常紧密的一种查询，允许在一个查询语句中使用多个特殊条件关键字（如：AND | OR | NOT）对多个字段进行查询，建议熟悉 Lucene 查询语法的用户去使用。 用户可以使用 query_string query 来创建包含通配符、跨多个字段的搜索等复杂搜索。虽然通用，但查询是严格的，如果查询字符串包含任何无效语法，则会返回错误。 示例： 123456789GET /_search&#123; &quot;query&quot;: &#123; &quot;query_string&quot;: &#123; &quot;query&quot;: &quot;(new york city) OR (big apple)&quot;, &quot;default_field&quot;: &quot;content&quot; &#125; &#125;&#125; 1.10. simple_query_string querysimple_query_string query 是一种适合直接暴露给用户，并且具有非常完善的查询语法的查询语句，接受 Lucene 查询语法，解析过程中发生错误不会抛出异常。 虽然语法比 query_string query 更严格，但 simple_query_string query 不会返回无效语法的错误。相反，它会忽略查询字符串的任何无效部分。 示例： 12345678910GET /_search&#123; &quot;query&quot;: &#123; &quot;simple_query_string&quot; : &#123; &quot;query&quot;: &quot;\\&quot;fried eggs\\&quot; +(eggplant | potato) -frittata&quot;, &quot;fields&quot;: [&quot;title^5&quot;, &quot;body&quot;], &quot;default_operator&quot;: &quot;and&quot; &#125; &#125;&#125; simple_query_string 语义 +：等价于 AND 操作 |：等价于 OR 操作 -：相当于 NOT 操作 &quot;：包装一些标记以表示用于搜索的短语 *：词尾表示前缀查询 ( and )：表示优先级 ~N：词尾表示表示编辑距离（模糊性） ~N：在一个短语之后表示溢出量 注意：要使用上面的字符，请使用反斜杠 / 对其进行转义。 1.11. 全文查询完整示例12345678910111213141516171819202122232425262728293031323334353637383940414243#设置 position_increment_gapDELETE groupsPUT groups&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;names&quot;:&#123; &quot;type&quot;: &quot;text&quot;, &quot;position_increment_gap&quot;: 0 &#125; &#125; &#125;&#125;GET groups/_mappingPOST groups/_doc&#123; &quot;names&quot;: [ &quot;John Water&quot;, &quot;Water Smith&quot;]&#125;POST groups/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;names&quot;: &#123; &quot;query&quot;: &quot;Water Water&quot;, &quot;slop&quot;: 100 &#125; &#125; &#125;&#125;POST groups/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;names&quot;: &quot;Water Smith&quot; &#125; &#125;&#125;DELETE groups 2. 词项查询Term（词项）是表达语意的最小单位。搜索和利用统计语言模型进行自然语言处理都需要处理 Term。 全文查询在执行查询之前会分析查询字符串。 与全文查询不同，词项查询不会分词，而是将输入作为一个整体，在倒排索引中查找准确的词项。并且使用相关度计算公式为每个包含该词项的文档进行相关度计算。一言以概之：词项查询是对词项进行精确匹配。词项查询通常用于结构化数据，如数字、日期和枚举类型。 词项查询有以下类型： exists query fuzzy query ids query prefix query range query regexp query term query terms query type query wildcard query 2.1. exists queryexists query 会返回字段中至少有一个非空值的文档。 由于多种原因，文档字段可能不存在索引值： JSON 中的字段为 null 或 [] 该字段在 mapping 中配置了 &quot;index&quot; : false 字段值的长度超过了 mapping 中的 ignore_above 设置 字段值格式错误，并且在 mapping 中定义了 ignore_malformed 示例： 12345678GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;exists&quot;: &#123; &quot;field&quot;: &quot;email&quot; &#125; &#125;&#125; 以下文档会匹配上面的查询： &#123; &quot;user&quot; : &quot;jane&quot; &#125; 有 user 字段，且不为空。 &#123; &quot;user&quot; : &quot;&quot; &#125; 有 user 字段，值为空字符串。 &#123; &quot;user&quot; : &quot;-&quot; &#125; 有 user 字段，值不为空。 &#123; &quot;user&quot; : [ &quot;jane&quot; ] &#125; 有 user 字段，值不为空。 &#123; &quot;user&quot; : [ &quot;jane&quot;, null ] &#125; 有 user 字段，至少一个值不为空即可。 下面的文档都不会被匹配： &#123; &quot;user&quot; : null &#125; 虽然有 user 字段，但是值为空。 &#123; &quot;user&quot; : [] &#125; 虽然有 user 字段，但是值为空。 &#123; &quot;user&quot; : [null] &#125; 虽然有 user 字段，但是值为空。 &#123; &quot;foo&quot; : &quot;bar&quot; &#125; 没有 user 字段。 2.2. fuzzy query**fuzzy query**（模糊查询）返回包含与搜索词相似的词的文档。ES 使用 Levenshtein edit distance（Levenshtein 编辑距离）测量相似度或模糊度。 编辑距离是将一个术语转换为另一个术语所需的单个字符更改的数量。这些变化可能包括： 改变一个字符：（box -&gt; fox） 删除一个字符：（black -&gt; lack） 插入一个字符：（sic -&gt; sick） 反转两个相邻字符：（act → cat） 为了找到相似的词条，fuzzy query 会在指定的编辑距离内创建搜索词条的所有可能变体或扩展集。然后返回完全匹配任意扩展的文档。 123456789101112131415GET books/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;user.id&quot;: &#123; &quot;value&quot;: &quot;ki&quot;, &quot;fuzziness&quot;: &quot;AUTO&quot;, &quot;max_expansions&quot;: 50, &quot;prefix_length&quot;: 0, &quot;transpositions&quot;: true, &quot;rewrite&quot;: &quot;constant_score&quot; &#125; &#125; &#125;&#125; 注意：如果配置了 search.allow_expensive_queries ，则 fuzzy query 不能执行。 2.3. ids queryids query 根据 ID 返回文档。 此查询使用存储在 _id 字段中的文档 ID。 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;ids&quot; : &#123; &quot;values&quot; : [&quot;1&quot;, &quot;4&quot;, &quot;100&quot;] &#125; &#125;&#125; 2.4. prefix queryprefix query 用于查询某个字段中包含指定前缀的文档。 比如查询 user.id 中含有以 ki 为前缀的关键词的文档，那么含有 kind、kid 等所有以 ki 开头关键词的文档都会被匹配。 12345678910GET /_search&#123; &quot;query&quot;: &#123; &quot;prefix&quot;: &#123; &quot;user.id&quot;: &#123; &quot;value&quot;: &quot;ki&quot; &#125; &#125; &#125;&#125; 2.5. range queryrange query 即范围查询，用于匹配在某一范围内的数值型、日期类型或者字符串型字段的文档。比如搜索哪些书籍的价格在 50 到 100 之间、哪些书籍的出版时间在 2015 年到 2019 年之间。使用 range 查询只能查询一个字段，不能作用在多个字段上。 range 查询支持的参数有以下几种： **gt**：大于 **gte**：大于等于 **lt**：小于 **lte**：小于等于 **format**：如果字段是 Date 类型，可以设置日期格式化 **time_zone**：时区 **relation**：指示范围查询如何匹配范围字段的值。 **INTERSECTS (Default)**：匹配与查询字段值范围相交的文档。 **CONTAINS**：匹配完全包含查询字段值的文档。 **WITHIN**：匹配具有完全在查询范围内的范围字段值的文档。 示例：数值范围查询 1234567891011GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;taxful_total_price&quot;: &#123; &quot;gt&quot;: 10, &quot;lte&quot;: 50 &#125; &#125; &#125;&#125; 示例：日期范围查询 123456789101112GET kibana_sample_data_ecommerce/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;order_date&quot;: &#123; &quot;time_zone&quot;: &quot;+00:00&quot;, &quot;gte&quot;: &quot;2018-01-01T00:00:00&quot;, &quot;lte&quot;: &quot;now&quot; &#125; &#125; &#125;&#125; 2.6. regexp queryregexp query 返回与正则表达式相匹配的 term 所属的文档。 正则表达式是一种使用占位符字符匹配数据模式的方法，称为运算符。 示例：以下搜索返回 user.id 字段包含任何以 k 开头并以 y 结尾的文档。 .* 运算符匹配任何长度的任何字符，包括无字符。匹配项可以包括 ky、kay 和 kimchy。 1234567891011121314GET /_search&#123; &quot;query&quot;: &#123; &quot;regexp&quot;: &#123; &quot;user.id&quot;: &#123; &quot;value&quot;: &quot;k.*y&quot;, &quot;flags&quot;: &quot;ALL&quot;, &quot;case_insensitive&quot;: true, &quot;max_determinized_states&quot;: 10000, &quot;rewrite&quot;: &quot;constant_score&quot; &#125; &#125; &#125;&#125; 注意：如果配置了search.allow_expensive_queries ，则 regexp query 会被禁用。 2.7. term queryterm query 用来查找指定字段中包含给定单词的文档，term 查询不被解析，只有查询词和文档中的词精确匹配才会被搜索到，应用场景为查询人名、地名等需要精准匹配的需求。 示例： 123456789101112131415161718192021222324252627282930313233343536373839# 1. 创建一个索引DELETE my-index-000001PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;full_text&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125;# 2. 使用 &quot;Quick Brown Foxes!&quot; 关键字查 &quot;full_text&quot; 字段PUT my-index-000001/_doc/1&#123; &quot;full_text&quot;: &quot;Quick Brown Foxes!&quot;&#125;# 3. 使用 term 查询GET my-index-000001/_search?pretty&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;full_text&quot;: &quot;Quick Brown Foxes!&quot; &#125; &#125;&#125;# 因为 full_text 字段不再包含确切的 Term —— &quot;Quick Brown Foxes!&quot;，所以 term query 搜索不到任何结果# 4. 使用 match 查询GET my-index-000001/_search?pretty&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;full_text&quot;: &quot;Quick Brown Foxes!&quot; &#125; &#125;&#125;DELETE my-index-000001 :warning: 注意：应避免 term 查询对 text 字段使用查询。 默认情况下，Elasticsearch 针对 text 字段的值进行解析分词，这会使查找 text 字段值的精确匹配变得困难。 要搜索 text 字段值，需改用 match 查询。 2.8. terms queryterms query 与 term query 相同，但可以搜索多个值。 terms query 查询参数： **index**：索引名 **id**：文档 ID **path**：要从中获取字段值的字段的名称，即搜索关键字 **routing**（选填）：要从中获取 term 值的文档的自定义路由值。如果在索引文档时提供了自定义路由值，则此参数是必需的。 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041# 1. 创建一个索引DELETE my-index-000001PUT my-index-000001&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;color&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125;# 2. 写入一个文档PUT my-index-000001/_doc/1&#123; &quot;color&quot;: [ &quot;blue&quot;, &quot;green&quot; ]&#125;# 3. 写入另一个文档PUT my-index-000001/_doc/2&#123; &quot;color&quot;: &quot;blue&quot;&#125;# 3. 使用 terms queryGET my-index-000001/_search?pretty&#123; &quot;query&quot;: &#123; &quot;terms&quot;: &#123; &quot;color&quot;: &#123; &quot;index&quot;: &quot;my-index-000001&quot;, &quot;id&quot;: &quot;2&quot;, &quot;path&quot;: &quot;color&quot; &#125; &#125; &#125;&#125;DELETE my-index-000001 2.9. type query 7.0.0 后废弃 type query 用于查询具有指定类型的文档。 示例： 12345678GET /_search&#123; &quot;query&quot;: &#123; &quot;type&quot;: &#123; &quot;value&quot;: &quot;_doc&quot; &#125; &#125;&#125; 2.10. wildcard querywildcard query 即通配符查询，返回与通配符模式匹配的文档。 ? 用来匹配一个任意字符，* 用来匹配零个或者多个字符。 示例：以下搜索返回 user.id 字段包含以 ki 开头并以 y 结尾的术语的文档。这些匹配项可以包括 kiy、kity 或 kimchy。 123456789101112GET /_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;user.id&quot;: &#123; &quot;value&quot;: &quot;ki*y&quot;, &quot;boost&quot;: 1.0, &quot;rewrite&quot;: &quot;constant_score&quot; &#125; &#125; &#125;&#125; 注意：如果配置了search.allow_expensive_queries ，则wildcard query 会被禁用。 2.11. 词项查询完整示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879DELETE productsPUT products&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;&#125;POST /products/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1 &#125;&#125;&#123; &quot;productID&quot; : &quot;XHDK-A-1293-#fJ3&quot;,&quot;desc&quot;:&quot;iPhone&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2 &#125;&#125;&#123; &quot;productID&quot; : &quot;KDKE-B-9947-#kL5&quot;,&quot;desc&quot;:&quot;iPad&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3 &#125;&#125;&#123; &quot;productID&quot; : &quot;JODL-X-1937-#pV7&quot;,&quot;desc&quot;:&quot;MBP&quot; &#125;GET /productsPOST /products/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;desc&quot;: &#123; //&quot;value&quot;: &quot;iPhone&quot; &quot;value&quot;:&quot;iphone&quot; &#125; &#125; &#125;&#125;POST /products/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;desc.keyword&quot;: &#123; //&quot;value&quot;: &quot;iPhone&quot; //&quot;value&quot;:&quot;iphone&quot; &#125; &#125; &#125;&#125;POST /products/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;productID&quot;: &#123; &quot;value&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125;&#125;POST /products/_search&#123; //&quot;explain&quot;: true, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;productID.keyword&quot;: &#123; &quot;value&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125;&#125;POST /products/_search&#123; &quot;explain&quot;: true, &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;productID.keyword&quot;: &quot;XHDK-A-1293-#fJ3&quot; &#125; &#125; &#125; &#125;&#125; 3. 复合查询复合查询就是把一些简单查询组合在一起实现更复杂的查询需求，除此之外，复合查询还可以控制另外一个查询的行为。 3.1. bool querybool 查询可以把任意多个简单查询组合在一起，使用 must、should、must_not、filter 选项来表示简单查询之间的逻辑，每个选项都可以出现 0 次到多次，它们的含义如下： must 文档必须匹配 must 选项下的查询条件，相当于逻辑运算的 AND，且参与文档相关度的评分。 should 文档可以匹配 should 选项下的查询条件也可以不匹配，相当于逻辑运算的 OR，且参与文档相关度的评分。 must_not 与 must 相反，匹配该选项下的查询条件的文档不会被返回；需要注意的是，must_not 语句不会影响评分，它的作用只是将不相关的文档排除。 filter 和 must 一样，匹配 filter 选项下的查询条件的文档才会被返回，但是 filter 不评分，只起到过滤功能，与 must_not 相反。 假设要查询 title 中包含关键词 java，并且 price 不能高于 70，description 可以包含也可以不包含虚拟机的书籍，构造 bool 查询语句如下： 1234567891011121314151617181920212223242526272829303132GET books/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;status&quot;: 1 &#125; &#125;, &quot;must_not&quot;: &#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;gte&quot;: 70 &#125; &#125; &#125;, &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125;, &quot;should&quot;: [ &#123; &quot;match&quot;: &#123; &quot;description&quot;: &quot;虚拟机&quot; &#125; &#125; ], &quot;minimum_should_match&quot;: 1 &#125; &#125;&#125; 有关布尔查询更详细的信息参考 bool query（组合查询）详解。 3.2. boosting queryboosting 查询用于需要对两个查询的评分进行调整的场景，boosting 查询会把两个查询封装在一起并降低其中一个查询的评分。 boosting 查询包括 positive、negative 和 negative_boost 三个部分，positive 中的查询评分保持不变，negative 中的查询会降低文档评分，negative_boost 指明 negative 中降低的权值。如果我们想对 2015 年之前出版的书降低评分，可以构造一个 boosting 查询，查询语句如下： 1234567891011121314151617181920GET books/_search&#123; &quot;query&quot;: &#123; &quot;boosting&quot;: &#123; &quot;positive&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;python&quot; &#125; &#125;, &quot;negative&quot;: &#123; &quot;range&quot;: &#123; &quot;publish_time&quot;: &#123; &quot;lte&quot;: &quot;2015-01-01&quot; &#125; &#125; &#125;, &quot;negative_boost&quot;: 0.2 &#125; &#125;&#125; boosting 查询中指定了抑制因子为 0.2，publish_time 的值在 2015-01-01 之后的文档得分不变，publish_time 的值在 2015-01-01 之前的文档得分为原得分的 0.2 倍。 3.3. constant_score queryconstantscore query 包装一个 filter query，并返回匹配过滤器查询条件的文档，且它们的相关性评分都等于 _boost 参数值（可以理解为原有的基于 tf-idf 或 bm25 的相关分固定为 1.0，所以最终评分为 _1.0 * boost_，即等于 boost 参数值）。下面的查询语句会返回 title 字段中含有关键词 elasticsearch 的文档，所有文档的评分都是 1.8： 12345678910111213GET books/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;elasticsearch&quot; &#125; &#125;, &quot;boost&quot;: 1.8 &#125; &#125;&#125; 3.4. dis_max querydis_max query 与 bool query 有一定联系也有一定区别，dis_max query 支持多并发查询，可返回与任意查询条件子句匹配的任何文档类型。与 bool 查询可以将所有匹配查询的分数相结合使用的方式不同，dis_max 查询只使用最佳匹配查询条件的分数。请看下面的例子： 1234567891011121314151617181920GET books/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;tie_breaker&quot;: 0.7, &quot;boost&quot;: 1.2, &quot;queries&quot;: [&#123; &quot;term&quot;: &#123; &quot;age&quot;: 34 &#125; &#125;, &#123; &quot;term&quot;: &#123; &quot;age&quot;: 35 &#125; &#125; ] &#125; &#125;&#125; 3.5. function_score queryfunction_score query 可以修改查询的文档得分，这个查询在有些情况下非常有用，比如通过评分函数计算文档得分代价较高，可以改用过滤器加自定义评分函数的方式来取代传统的评分方式。 使用 function_score query，用户需要定义一个查询和一至多个评分函数，评分函数会对查询到的每个文档分别计算得分。 下面这条查询语句会返回 books 索引中的所有文档，文档的最大得分为 5，每个文档的得分随机生成，权重的计算模式为相乘模式。 12345678910111213GET books/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match all&quot;: &#123;&#125; &#125;, &quot;boost&quot;: &quot;5&quot;, &quot;random_score&quot;: &#123;&#125;, &quot;boost_mode&quot;: &quot;multiply&quot; &#125; &#125;&#125; 使用脚本自定义评分公式，这里把 price 值的十分之一开方作为每个文档的得分，查询语句如下： 123456789101112131415GET books/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;java&quot; &#125; &#125;, &quot;script_score&quot;: &#123; &quot;inline&quot;: &quot;Math.sqrt(doc[&#x27;price&#x27;].value/10)&quot; &#125; &#125; &#125;&#125; 关于 function_score 的更多详细内容请查看 Elasticsearch function_score 查询最强详解。 3.6. indices queryindices query 适用于需要在多个索引之间进行查询的场景，它允许指定一个索引名字列表和内部查询。indices query 中有 query 和 no_match_query 两部分，query 中用于搜索指定索引列表中的文档，no_match_query 中的查询条件用于搜索指定索引列表之外的文档。下面的查询语句实现了搜索索引 books、books2 中 title 字段包含关键字 javascript，其他索引中 title 字段包含 basketball 的文档，查询语句如下： 123456789101112131415161718GET books/_search&#123; &quot;query&quot;: &#123; &quot;indices&quot;: &#123; &quot;indices&quot;: [&quot;books&quot;, &quot;books2&quot;], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;javascript&quot; &#125; &#125;, &quot;no_match_query&quot;: &#123; &quot;term&quot;: &#123; &quot;title&quot;: &quot;basketball&quot; &#125; &#125; &#125; &#125;&#125; 4. 嵌套查询在 Elasticsearch 这样的分布式系统中执行全 SQL 风格的连接查询代价昂贵，是不可行的。相应地，为了实现水平规模地扩展，Elasticsearch 提供了以下两种形式的 join： nested query（嵌套查询） 文档中可能包含嵌套类型的字段，这些字段用来索引一些数组对象，每个对象都可以作为一条独立的文档被查询出来。 has_child query（有子查询）和 has_parent query（有父查询） 父子关系可以存在单个的索引的两个类型的文档之间。has_child 查询将返回其子文档能满足特定查询的父文档，而 has_parent 则返回其父文档能满足特定查询的子文档。 4.1. nested query文档中可能包含嵌套类型的字段，这些字段用来索引一些数组对象，每个对象都可以作为一条独立的文档被查询出来（用嵌套查询）。 123456789101112PUT /my_index&#123; &quot;mappings&quot;: &#123; &quot;type1&quot;: &#123; &quot;properties&quot;: &#123; &quot;obj1&quot;: &#123; &quot;type&quot;: &quot;nested&quot; &#125; &#125; &#125; &#125;&#125; 4.2. has_child query文档的父子关系创建索引时在映射中声明，这里以员工（employee）和工作城市（branch）为例，它们属于不同的类型，相当于数据库中的两张表，如果想把员工和他们工作的城市关联起来，需要告诉 Elasticsearch 文档之间的父子关系，这里 employee 是 child type，branch 是 parent type，在映射中声明，执行命令： 123456789PUT /company&#123; &quot;mappings&quot;: &#123; &quot;branch&quot;: &#123;&#125;, &quot;employee&quot;: &#123; &quot;parent&quot;: &#123; &quot;type&quot;: &quot;branch&quot; &#125; &#125; &#125;&#125; 使用 bulk api 索引 branch 类型下的文档，命令如下： 1234567POST company/branch/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;london&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;London Westminster&quot;,&quot;city&quot;: &quot;London&quot;,&quot;country&quot;: &quot;UK&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;liverpool&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Liverpool Central&quot;,&quot;city&quot;: &quot;Liverpool&quot;,&quot;country&quot;: &quot;UK&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: &quot;paris&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Champs Elysees&quot;,&quot;city&quot;: &quot;Paris&quot;,&quot;country&quot;: &quot;France&quot; &#125; 添加员工数据： 123456789POST company/employee/_bulk&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 1,&quot;parent&quot;:&quot;london&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Alice Smith&quot;,&quot;dob&quot;: &quot;1970-10-24&quot;,&quot;hobby&quot;: &quot;hiking&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 2,&quot;parent&quot;:&quot;london&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Mark Tomas&quot;,&quot;dob&quot;: &quot;1982-05-16&quot;,&quot;hobby&quot;: &quot;diving&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 3,&quot;parent&quot;:&quot;liverpool&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Barry Smith&quot;,&quot;dob&quot;: &quot;1979-04-01&quot;,&quot;hobby&quot;: &quot;hiking&quot; &#125;&#123; &quot;index&quot;: &#123; &quot;_id&quot;: 4,&quot;parent&quot;:&quot;paris&quot; &#125;&#125;&#123; &quot;name&quot;: &quot;Adrien Grand&quot;,&quot;dob&quot;: &quot;1987-05-11&quot;,&quot;hobby&quot;: &quot;horses&quot; &#125; 通过子文档查询父文档要使用 has_child 查询。例如，搜索 1980 年以后出生的员工所在的分支机构，employee 中 1980 年以后出生的有 Mark Thomas 和 Adrien Grand，他们分别在 london 和 paris，执行以下查询命令进行验证： 1234567891011GET company/branch/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;query&quot;: &#123; &quot;range&quot;: &#123; &quot;dob&quot;: &#123; &quot;gte&quot;: &quot;1980-01-01&quot; &#125; &#125; &#125; &#125; &#125;&#125; 搜索哪些机构中有名为 “Alice Smith” 的员工，因为使用 match 查询，会解析为 “Alice” 和 “Smith”，所以 Alice Smith 和 Barry Smith 所在的机构会被匹配，执行以下查询命令进行验证： 123456789101112GET company/branch/_search&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;score_mode&quot;: &quot;max&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Alice Smith&quot; &#125; &#125; &#125; &#125;&#125; 可以使用 min_children 指定子文档的最小个数。例如，搜索最少含有两个 employee 的机构，查询命令如下： 123456789101112GET company/branch/_search?pretty&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;employee&quot;, &quot;min_children&quot;: 2, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125; &#125; &#125;&#125; 4.3. has_parent query通过父文档查询子文档使用 has_parent 查询。比如，搜索哪些 employee 工作在 UK，查询命令如下： 1234567891011GET company/employee/_search&#123; &quot;query&quot;: &#123; &quot;has_parent&quot;: &#123; &quot;parent_type&quot;: &quot;branch&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;country&quot;: &quot;UK &#125; &#125; &#125; &#125;&#125; 5. 位置查询Elasticsearch 可以对地理位置点 geo_point 类型和地理位置形状 geo_shape 类型的数据进行搜索。为了学习方便，这里准备一些城市的地理坐标作为测试数据，每一条文档都包含城市名称和地理坐标这两个字段，这里的坐标点取的是各个城市中心的一个位置。首先把下面的内容保存到 geo.json 文件中： 123456789101112&#123;&quot;index&quot;:&#123; &quot;_index&quot;:&quot;geo&quot;,&quot;_type&quot;:&quot;city&quot;,&quot;_id&quot;:&quot;1&quot; &#125;&#125;&#123;&quot;name&quot;:&quot;北京&quot;,&quot;location&quot;:&quot;39.9088145109,116.3973999023&quot;&#125;&#123;&quot;index&quot;:&#123; &quot;_index&quot;:&quot;geo&quot;,&quot;_type&quot;:&quot;city&quot;,&quot;_id&quot;: &quot;2&quot; &#125;&#125;&#123;&quot;name&quot;:&quot;乌鲁木齐&quot;,&quot;location&quot;:&quot;43.8266300000,87.6168800000&quot;&#125;&#123;&quot;index&quot;:&#123; &quot;_index&quot;:&quot;geo&quot;,&quot;_type&quot;:&quot;city&quot;,&quot;_id&quot;:&quot;3&quot; &#125;&#125;&#123;&quot;name&quot;:&quot;西安&quot;,&quot;location&quot;:&quot;34.3412700000,108.9398400000&quot;&#125;&#123;&quot;index&quot;:&#123; &quot;_index&quot;:&quot;geo&quot;,&quot;_type&quot;:&quot;city&quot;,&quot;_id&quot;:&quot;4&quot; &#125;&#125;&#123;&quot;name&quot;:&quot;郑州&quot;,&quot;location&quot;:&quot;34.7447157466,113.6587142944&quot;&#125;&#123;&quot;index&quot;:&#123; &quot;_index&quot;:&quot;geo&quot;,&quot;_type&quot;:&quot;city&quot;,&quot;_id&quot;:&quot;5&quot; &#125;&#125;&#123;&quot;name&quot;:&quot;杭州&quot;,&quot;location&quot;:&quot;30.2294080260,120.1492309570&quot;&#125;&#123;&quot;index&quot;:&#123; &quot;_index&quot;:&quot;geo&quot;,&quot;_type&quot;:&quot;city&quot;,&quot;_id&quot;:&quot;6&quot; &#125;&#125;&#123;&quot;name&quot;:&quot;济南&quot;,&quot;location&quot;:&quot;36.6518400000,117.1200900000&quot;&#125; 创建一个索引并设置映射： 123456789101112131415PUT geo&#123; &quot;mappings&quot;: &#123; &quot;city&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125; &#125; &#125; &#125;&#125; 然后执行批量导入命令： 1curl -XPOST &quot;http://localhost:9200/_bulk?pretty&quot; --data-binary @geo.json 5.1. geo_distance querygeo_distance query 可以查找在一个中心点指定范围内的地理点文档。例如，查找距离天津 200km 以内的城市，搜索结果中会返回北京，命令如下： 12345678910111213141516171819GET geo/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;distance&quot;: &quot;200km&quot;, &quot;location&quot;: &#123; &quot;lat&quot;: 39.0851000000, &quot;lon&quot;: 117.1993700000 &#125; &#125; &#125; &#125; &#125;&#125; 按各城市离北京的距离排序： 1234567891011121314GET geo/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [&#123; &quot;_geo_distance&quot;: &#123; &quot;location&quot;: &quot;39.9088145109,116.3973999023&quot;, &quot;unit&quot;: &quot;km&quot;, &quot;order&quot;: &quot;asc&quot;, &quot;distance_type&quot;: &quot;plane&quot; &#125; &#125;]&#125; 其中 location 对应的经纬度字段；unit 为 km 表示将距离以 km 为单位写入到每个返回结果的 sort 键中；distance_type 为 plane 表示使用快速但精度略差的 plane 计算方式。 5.2. geo_bounding_box querygeo_bounding_box query 用于查找落入指定的矩形内的地理坐标。查询中由两个点确定一个矩形，然后在矩形区域内查询匹配的文档。 123456789101112131415161718192021222324GET geo/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;geo_bounding_box&quot;: &#123; &quot;location&quot;: &#123; &quot;top_left&quot;: &#123; &quot;lat&quot;: 38.4864400000, &quot;lon&quot;: 106.2324800000 &#125;, &quot;bottom_right&quot;: &#123; &quot;lat&quot;: 28.6820200000, &quot;lon&quot;: 115.8579400000 &#125; &#125; &#125; &#125; &#125; &#125;&#125; 5.3. geo_polygon querygeo_polygon query 用于查找在指定多边形内的地理点。例如，呼和浩特、重庆、上海三地组成一个三角形，查询位置在该三角形区域内的城市，命令如下： 1234567891011121314151617181920212223242526GET geo/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125; &#125;, &quot;filter&quot;: &#123; &quot;geo_polygon&quot;: &#123; &quot;location&quot;: &#123; &quot;points&quot;: [&#123; &quot;lat&quot;: 40.8414900000, &quot;lon&quot;: 111.7519900000 &#125;, &#123; &quot;lat&quot;: 29.5647100000, &quot;lon&quot;: 106.5507300000 &#125;, &#123; &quot;lat&quot;: 31.2303700000, &quot;lon&quot;: 121.4737000000 &#125;] &#125; &#125; &#125; &#125;&#125; 5.4. geo_shape querygeo_shape query 用于查询 geo_shape 类型的地理数据，地理形状之间的关系有相交、包含、不相交三种。创建一个新的索引用于测试，其中 location 字段的类型设为 geo_shape 类型。 123456789101112131415PUT geoshape&#123; &quot;mappings&quot;: &#123; &quot;city&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_shape&quot; &#125; &#125; &#125; &#125;&#125; 关于经纬度的顺序这里做一个说明，geo_point 类型的字段纬度在前经度在后，但是对于 geo_shape 类型中的点，是经度在前纬度在后，这一点需要特别注意。 把西安和郑州连成的线写入索引： 1234567891011POST geoshape/city/1&#123; &quot;name&quot;: &quot;西安-郑州&quot;, &quot;location&quot;: &#123; &quot;type&quot;: &quot;linestring&quot;, &quot;coordinates&quot;: [ [108.9398400000, 34.3412700000], [113.6587142944, 34.7447157466] ] &#125;&#125; 查询包含在由银川和南昌作为对角线上的点组成的矩形的地理形状，由于西安和郑州组成的直线落在该矩形区域内，因此可以被查询到。命令如下： 123456789101112131415161718192021222324GET geoshape/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;filter&quot;: &#123; &quot;geo_shape&quot;: &#123; &quot;location&quot;: &#123; &quot;shape&quot;: &#123; &quot;type&quot;: &quot;envelope&quot;, &quot;coordinates&quot;: [ [106.23248, 38.48644], [115.85794, 28.68202] ] &#125;, &quot;relation&quot;: &quot;within&quot; &#125; &#125; &#125; &#125; &#125;&#125; 6. 特殊查询6.1. more_like_this querymore_like_this query 可以查询和提供文本类似的文档，通常用于近似文本的推荐等场景。查询命令如下： 1234567891011GET books/_search&#123; &quot;query&quot;: &#123; &quot;more_like_ this&quot;: &#123; &quot;fields&quot;: [&quot;title&quot;, &quot;description&quot;], &quot;like&quot;: &quot;java virtual machine&quot;, &quot;min_term_freq&quot;: 1, &quot;max_query_terms&quot;: 12 &#125; &#125;&#125; 可选的参数及取值说明如下： fields 要匹配的字段，默认是 _all 字段。 like 要匹配的文本。 min_term_freq 文档中词项的最低频率，默认是 2，低于此频率的文档会被忽略。 max_query_terms query 中能包含的最大词项数目，默认为 25。 min_doc_freq 最小的文档频率，默认为 5。 max_doc_freq 最大文档频率。 min_word length 单词的最小长度。 max_word length 单词的最大长度。 stop_words 停用词列表。 analyzer 分词器。 minimum_should_match 文档应匹配的最小词项数，默认为 query 分词后词项数的 30%。 boost terms 词项的权重。 include 是否把输入文档作为结果返回。 boost 整个 query 的权重，默认为 1.0。 6.2. script queryElasticsearch 支持使用脚本进行查询。例如，查询价格大于 180 的文档，命令如下： 1234567891011GET books/_search&#123; &quot;query&quot;: &#123; &quot;script&quot;: &#123; &quot;script&quot;: &#123; &quot;inline&quot;: &quot;doc[&#x27;price&#x27;].value &gt; 180&quot;, &quot;lang&quot;: &quot;painless&quot; &#125; &#125; &#125;&#125; 6.3. percolate query一般情况下，我们是先把文档写入到 Elasticsearch 中，通过查询语句对文档进行搜索。percolate query 则是反其道而行之的做法，它会先注册查询条件，根据文档来查询 query。例如，在 my-index 索引中有一个 laptop 类型，文档有 price 和 name 两个字段，在映射中声明一个 percolator 类型的 query，命令如下： 12345678910111213141516PUT my-index&#123; &quot;mappings&quot;: &#123; &quot;laptop&quot;: &#123; &quot;properties&quot;: &#123; &quot;price&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125;, &quot;queries&quot;: &#123; &quot;properties&quot;: &#123; &quot;query&quot;: &#123; &quot;type&quot;: &quot;percolator&quot; &#125; &#125; &#125; &#125; &#125;&#125; 注册一个 bool query，bool query 中包含一个 range query，要求 price 字段的取值小于等于 10000，并且 name 字段中含有关键词 macbook： 123456789101112PUT /my-index/queries/1?refresh&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [&#123; &quot;range&quot;: &#123; &quot;price&quot;: &#123; &quot;lte&quot;: 10000 &#125; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;macbook&quot; &#125; &#125;] &#125; &#125;&#125; 通过文档查询 query： 12345678910111213GET /my-index/_search&#123; &quot;query&quot;: &#123; &quot;percolate&quot;: &#123; &quot;field&quot;: &quot;query&quot;, &quot;document_type&quot;: &quot;laptop&quot;, &quot;document&quot;: &#123; &quot;price&quot;: 9999, &quot;name&quot;: &quot;macbook pro on sale&quot; &#125; &#125; &#125;&#125; 文档符合 query 中的条件，返回结果中可以查到上文中注册的 bool query。percolate query 的这种特性适用于数据分类、数据路由、事件监控和预警等场景。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 性能优化","slug":"database/elasticsearch/elasticsearch-performance-optimization","date":"2022-06-08T01:35:00.000Z","updated":"2023-12-25T05:56:09.160Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-performance-optimization/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-performance-optimization/","excerpt":"","text":"Elasticsearch 性能优化Elasticsearch 是当前流行的企业级搜索引擎，设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。作为一个开箱即用的产品，在生产环境上线之后，我们其实不一定能确保其的性能和稳定性。如何根据实际情况提高服务的性能，其实有很多技巧。这章我们分享从实战经验中总结出来的 elasticsearch 性能优化，主要从硬件配置优化、索引优化设置、查询方面优化、数据结构优化、集群架构优化等方面讲解。 1. 硬件配置优化升级硬件设备配置一直都是提高服务能力最快速有效的手段，在系统层面能够影响应用性能的一般包括三个因素：CPU、内存和 IO，可以从这三方面进行 ES 的性能优化工作。 1.1. CPU 配置一般说来，CPU 繁忙的原因有以下几个： 线程中有无限空循环、无阻塞、正则匹配或者单纯的计算； 发生了频繁的 GC； 多线程的上下文切换； 大多数 Elasticsearch 部署往往对 CPU 要求不高。因此，相对其它资源，具体配置多少个（CPU）不是那么关键。你应该选择具有多个内核的现代处理器，常见的集群使用 2 到 8 个核的机器。如果你要在更快的 CPUs 和更多的核数之间选择，选择更多的核数更好。多个内核提供的额外并发远胜过稍微快一点点的时钟频率。 1.2. 内存配置如果有一种资源是最先被耗尽的，它可能是内存。排序和聚合都很耗内存，所以有足够的堆空间来应付它们是很重要的。即使堆空间是比较小的时候，也能为操作系统文件缓存提供额外的内存。因为 Lucene 使用的许多数据结构是基于磁盘的格式，Elasticsearch 利用操作系统缓存能产生很大效果。 64 GB 内存的机器是非常理想的，但是 32 GB 和 16 GB 机器也是很常见的。少于 8 GB 会适得其反（你最终需要很多很多的小机器），大于 64 GB 的机器也会有问题。 由于 ES 构建基于 lucene，而 lucene 设计强大之处在于 lucene 能够很好的利用操作系统内存来缓存索引数据，以提供快速的查询性能。lucene 的索引文件 segements 是存储在单文件中的，并且不可变，对于 OS 来说，能够很友好地将索引文件保持在 cache 中，以便快速访问；因此，我们很有必要将一半的物理内存留给 lucene；另一半的物理内存留给 ES（JVM heap）。 内存分配当机器内存小于 64G 时，遵循通用的原则，50% 给 ES，50% 留给 lucene。 当机器内存大于 64G 时，遵循以下原则： 如果主要的使用场景是全文检索，那么建议给 ES Heap 分配 4~32G 的内存即可；其它内存留给操作系统，供 lucene 使用（segments cache），以提供更快的查询性能。 如果主要的使用场景是聚合或排序，并且大多数是 numerics，dates，geo_points 以及 not_analyzed 的字符类型，建议分配给 ES Heap 分配 4~32G 的内存即可，其它内存留给操作系统，供 lucene 使用，提供快速的基于文档的聚类、排序性能。 如果使用场景是聚合或排序，并且都是基于 analyzed 字符数据，这时需要更多的 heap size，建议机器上运行多 ES 实例，每个实例保持不超过 50% 的 ES heap 设置（但不超过 32 G，堆内存设置 32 G 以下时，JVM 使用对象指标压缩技巧节省空间），50% 以上留给 lucene。 禁止 swap禁止 swap，一旦允许内存与磁盘的交换，会引起致命的性能问题。可以通过在 elasticsearch.yml 中 bootstrap.memory_lock: true，以保持 JVM 锁定内存，保证 ES 的性能。 GC 设置保持 GC 的现有设置，默认设置为：Concurrent-Mark and Sweep（CMS），别换成 G1 GC，因为目前 G1 还有很多 BUG。 保持线程池的现有设置，目前 ES 的线程池较 1.X 有了较多优化设置，保持现状即可；默认线程池大小等于 CPU 核心数。如果一定要改，按公式 ( ( CPU 核心数 * 3 ) &#x2F; 2 ) + 1 设置；不能超过 CPU 核心数的 2 倍；但是不建议修改默认配置，否则会对 CPU 造成硬伤。 1.3. 磁盘硬盘对所有的集群都很重要，对大量写入的集群更是加倍重要（例如那些存储日志数据的）。硬盘是服务器上最慢的子系统，这意味着那些写入量很大的集群很容易让硬盘饱和，使得它成为集群的瓶颈。 在经济压力能承受的范围下，尽量使用固态硬盘（SSD）。固态硬盘相比于任何旋转介质（机械硬盘，磁带等），无论随机写还是顺序写，都会对 IO 有较大的提升。 如果你正在使用 SSDs，确保你的系统 I&#x2F;O 调度程序是配置正确的。当你向硬盘写数据，I&#x2F;O 调度程序决定何时把数据实际发送到硬盘。大多数默认 *nix 发行版下的调度程序都叫做 cfq（完全公平队列）。 调度程序分配时间片到每个进程。并且优化这些到硬盘的众多队列的传递。但它是为旋转介质优化的：机械硬盘的固有特性意味着它写入数据到基于物理布局的硬盘会更高效。 这对 SSD 来说是低效的，尽管这里没有涉及到机械硬盘。但是，deadline 或者 noop 应该被使用。deadline 调度程序基于写入等待时间进行优化，noop 只是一个简单的 FIFO 队列。 这个简单的更改可以带来显著的影响。仅仅是使用正确的调度程序，我们看到了 500 倍的写入能力提升。 如果你使用旋转介质（如机械硬盘），尝试获取尽可能快的硬盘（高性能服务器硬盘，15k RPM 驱动器）。 使用 RAID0 是提高硬盘速度的有效途径，对机械硬盘和 SSD 来说都是如此。没有必要使用镜像或其它 RAID 变体，因为 Elasticsearch 在自身层面通过副本，已经提供了备份的功能，所以不需要利用磁盘的备份功能，同时如果使用磁盘备份功能的话，对写入速度有较大的影响。 最后，避免使用网络附加存储（NAS）。人们常声称他们的 NAS 解决方案比本地驱动器更快更可靠。除却这些声称，我们从没看到 NAS 能配得上它的大肆宣传。NAS 常常很慢，显露出更大的延时和更宽的平均延时方差，而且它是单点故障的。 2. 索引优化设置索引优化主要是在 Elasticsearch 的插入层面优化，Elasticsearch 本身索引速度其实还是蛮快的，具体数据，我们可以参考官方的 benchmark 数据。我们可以根据不同的需求，针对索引优化。 2.1. 批量提交当有大量数据提交的时候，建议采用批量提交（Bulk 操作）；此外使用 bulk 请求时，每个请求不超过几十 M，因为太大会导致内存使用过大。 比如在做 ELK 过程中，Logstash indexer 提交数据到 Elasticsearch 中，batch size 就可以作为一个优化功能点。但是优化 size 大小需要根据文档大小和服务器性能而定。 像 Logstash 中提交文档大小超过 20MB，Logstash 会将一个批量请求切分为多个批量请求。 如果在提交过程中，遇到 EsRejectedExecutionException 异常的话，则说明集群的索引性能已经达到极限了。这种情况，要么提高服务器集群的资源，要么根据业务规则，减少数据收集速度，比如只收集 Warn、Error 级别以上的日志。 2.2. 增加 Refresh 时间间隔为了提高索引性能，Elasticsearch 在写入数据的时候，采用延迟写入的策略，即数据先写到内存中，当超过默认 1 秒（index.refresh_interval）会进行一次写入操作，就是将内存中 segment 数据刷新到磁盘中，此时我们才能将数据搜索出来，所以这就是为什么 Elasticsearch 提供的是近实时搜索功能，而不是实时搜索功能。 如果我们的系统对数据延迟要求不高的话，我们可以通过延长 refresh 时间间隔，可以有效地减少 segment 合并压力，提高索引速度。比如在做全链路跟踪的过程中，我们就将 index.refresh_interval 设置为 30s，减少 refresh 次数。再如，在进行全量索引时，可以将 refresh 次数临时关闭，即 index.refresh_interval 设置为-1，数据导入成功后再打开到正常模式，比如 30s。 在加载大量数据时候可以暂时不用 refresh 和 repliccas，index.refresh_interval 设置为-1，index.number_of_replicas 设置为 0。 2.3. 修改 index_buffer_size 的设置索引缓冲的设置可以控制多少内存分配给索引进程。这是一个全局配置，会应用于一个节点上所有不同的分片上。 12indices.memory.index_buffer_size: 10%indices.memory.min_index_buffer_size: 48mb indices.memory.index_buffer_size 接受一个百分比或者一个表示字节大小的值。默认是 10%，意味着分配给节点的总内存的 10%用来做索引缓冲的大小。这个数值被分到不同的分片（shards）上。如果设置的是百分比，还可以设置 min_index_buffer_size （默认 48mb）和 max_index_buffer_size（默认没有上限）。 2.4. 修改 translog 相关的设置一是控制数据从内存到硬盘的操作频率，以减少硬盘 IO。可将 sync_interval 的时间设置大一些。默认为 5s。 1index.translog.sync_interval: 5s 也可以控制 tranlog 数据块的大小，达到 threshold 大小时，才会 flush 到 lucene 索引文件。默认为 512m。 1index.translog.flush_threshold_size: 512mb 2.5. 注意 _id 字段的使用_id 字段的使用，应尽可能避免自定义 _id，以避免针对 ID 的版本管理；建议使用 ES 的默认 ID 生成策略或使用数字类型 ID 做为主键。 2.6. 注意 _all 字段及 _source 字段的使用**_**all 字段及 _source 字段的使用，应该注意场景和需要，_all 字段包含了所有的索引字段，方便做全文检索，如果无此需求，可以禁用；_source 存储了原始的 document 内容，如果没有获取原始文档数据的需求，可通过设置 includes、excludes 属性来定义放入 _source 的字段。 2.7. 合理的配置使用 index 属性合理的配置使用 index 属性，analyzed 和 not_analyzed，根据业务需求来控制字段是否分词或不分词。只有 groupby 需求的字段，配置时就设置成 not_analyzed，以提高查询或聚类的效率。 2.8. 减少副本数量Elasticsearch 默认副本数量为 3 个，虽然这样会提高集群的可用性，增加搜索的并发数，但是同时也会影响写入索引的效率。 在索引过程中，需要把更新的文档发到副本节点上，等副本节点生效后在进行返回结束。使用 Elasticsearch 做业务搜索的时候，建议副本数目还是设置为 3 个，但是像内部 ELK 日志系统、分布式跟踪系统中，完全可以将副本数目设置为 1 个。 3. 查询方面优化Elasticsearch 作为业务搜索的近实时查询时，查询效率的优化显得尤为重要。 3.1. 路由优化当我们查询文档的时候，Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？它其实是通过下面这个公式来计算出来的。 1shard = hash(routing) % number_of_primary_shards routing 默认值是文档的 id，也可以采用自定义值，比如用户 ID。 不带 routing 查询在查询的时候因为不知道要查询的数据具体在哪个分片上，所以整个过程分为 2 个步骤： 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上。 聚合：协调节点搜集到每个分片上查询结果，再将查询的结果进行排序，之后给用户返回结果。 带 routing 查询查询的时候，可以直接根据 routing 信息定位到某个分配查询，不需要查询所有的分配，经过协调节点排序。 向上面自定义的用户查询，如果 routing 设置为 userid 的话，就可以直接查询出数据来，效率提升很多。 3.2. Filter VS Query尽可能使用过滤器上下文（Filter）替代查询上下文（Query） Query：此文档与此查询子句的匹配程度如何？ Filter：此文档和查询子句匹配吗？ Elasticsearch 针对 Filter 查询只需要回答「是」或者「否」，不需要像 Query 查询一样计算相关性分数，同时 Filter 结果可以缓存。 3.3. 深度翻页在使用 Elasticsearch 过程中，应尽量避免大翻页的出现。 正常翻页查询都是从 from 开始 size 条数据，这样就需要在每个分片中查询打分排名在前面的 from+size 条数据。协同节点收集每个分配的前 from+size 条数据。协同节点一共会受到 N*(from+size) 条数据，然后进行排序，再将其中 from 到 from+size 条数据返回出去。如果 from 或者 size 很大的话，导致参加排序的数量会同步扩大很多，最终会导致 CPU 资源消耗增大。 可以通过使用 Elasticsearch scroll 和 scroll-scan 高效滚动的方式来解决这样的问题。 也可以结合实际业务特点，文档 id 大小如果和文档创建时间是一致有序的，可以以文档 id 作为分页的偏移量，并将其作为分页查询的一个条件。 3.4. 脚本（script）合理使用我们知道脚本使用主要有 3 种形式，内联动态编译方式、_script 索引库中存储和文件脚本存储的形式；一般脚本的使用场景是粗排，尽量用第二种方式先将脚本存储在 _script 索引库中，起到提前编译，然后通过引用脚本 id，并结合 params 参数使用，即可以达到模型（逻辑）和数据进行了分离，同时又便于脚本模块的扩展与维护。具体 ES 脚本的深入内容请参考 Elasticsearch 脚本模块的详解。 4. 数据结构优化基于 Elasticsearch 的使用场景，文档数据结构尽量和使用场景进行结合，去掉没用及不合理的数据。 4.1. 尽量减少不需要的字段如果 Elasticsearch 用于业务搜索服务，一些不需要用于搜索的字段最好不存到 ES 中，这样即节省空间，同时在相同的数据量下，也能提高搜索性能。 避免使用动态值作字段，动态递增的 mapping，会导致集群崩溃；同样，也需要控制字段的数量，业务中不使用的字段，就不要索引。控制索引的字段数量、mapping 深度、索引字段的类型，对于 ES 的性能优化是重中之重。 以下是 ES 关于字段数、mapping 深度的一些默认设置： 123index.mapping.nested_objects.limit: 10000index.mapping.total_fields.limit: 1000index.mapping.depth.limit: 20 4.2. Nested Object vs Parent&#x2F;Child尽量避免使用 nested 或 parent&#x2F;child 的字段，能不用就不用；nested query 慢，parent&#x2F;child query 更慢，比 nested query 慢上百倍；因此能在 mapping 设计阶段搞定的（大宽表设计或采用比较 smart 的数据结构），就不要用父子关系的 mapping。 如果一定要使用 nested fields，保证 nested fields 字段不能过多，目前 ES 默认限制是 50。因为针对 1 个 document，每一个 nested field，都会生成一个独立的 document，这将使 doc 数量剧增，影响查询效率，尤其是 JOIN 的效率。 1index.mapping.nested_fields.limit: 50 对比 Nested Object Parent&#x2F;Child 优点 文档存储在一起，因此读取性高 父子文档可以独立更新，互不影响 缺点 更新父文档或子文档时需要更新整个文档 为了维护 join 关系，需要占用部分内存，读取性能较差 场景 子文档偶尔更新，查询频繁 子文档更新频繁 4.3. 选择静态映射，非必需时，禁止动态映射尽量避免使用动态映射，这样有可能会导致集群崩溃，此外，动态映射有可能会带来不可控制的数据类型，进而有可能导致在查询端出现相关异常，影响业务。 此外，Elasticsearch 作为搜索引擎时，主要承载 query 的匹配和排序的功能，那数据的存储类型基于这两种功能的用途分为两类，一是需要匹配的字段，用来建立倒排索引对 query 匹配用，另一类字段是用做粗排用到的特征字段，如 ctr、点击数、评论数等等。 5. 集群架构设计合理的部署 Elasticsearch 有助于提高服务的整体可用性。 5.1. 主节点、数据节点和协调节点分离Elasticsearch 集群在架构拓朴时，采用主节点、数据节点和负载均衡节点分离的架构，在 5.x 版本以后，又可将数据节点再细分为“Hot-Warm”的架构模式。 Elasticsearch 的配置文件中有 2 个参数，node.master 和 node.data。这两个参数搭配使用时，能够帮助提供服务器性能。 主（master）节点配置 node.master:true 和 node.data:false，该 node 服务器只作为一个主节点，但不存储任何索引数据。我们推荐每个集群运行 3 个专用的 master 节点来提供最好的弹性。使用时，你还需要将 discovery.zen.minimum_master_nodes setting 参数设置为 2，以免出现脑裂（split-brain）的情况。用 3 个专用的 master 节点，专门负责处理集群的管理以及加强状态的整体稳定性。因为这 3 个 master 节点不包含数据也不会实际参与搜索以及索引操作，在 JVM 上它们不用做相同的事，例如繁重的索引或者耗时，资源耗费很大的搜索。因此不太可能会因为垃圾回收而导致停顿。因此，master 节点的 CPU，内存以及磁盘配置可以比 data 节点少很多的。 数据（data）节点配置 node.master:false 和 node.data:true，该 node 服务器只作为一个数据节点，只用于存储索引数据，使该 node 服务器功能单一，只用于数据存储和数据查询，降低其资源消耗率。 在 Elasticsearch 5.x 版本之后，data 节点又可再细分为“Hot-Warm”架构，即分为热节点（hot node）和暖节点（warm node）。 hot 节点： hot 节点主要是索引节点（写节点），同时会保存近期的一些频繁被查询的索引。由于进行索引非常耗费 CPU 和 IO，即属于 IO 和 CPU 密集型操作，建议使用 SSD 的磁盘类型，保持良好的写性能；我们推荐部署最小化的 3 个 hot 节点来保证高可用性。根据近期需要收集以及查询的数据量，可以增加服务器数量来获得想要的性能。 将节点设置为 hot 类型需要 elasticsearch.yml 如下配置： 1node.attr.box_type: hot 如果是针对指定的 index 操作，可以通过 settings 设置 index.routing.allocation.require.box_type: hot 将索引写入 hot 节点。 warm 节点： 这种类型的节点是为了处理大量的，而且不经常访问的只读索引而设计的。由于这些索引是只读的，warm 节点倾向于挂载大量磁盘（普通磁盘）来替代 SSD。内存、CPU 的配置跟 hot 节点保持一致即可；节点数量一般也是大于等于 3 个。 将节点设置为 warm 类型需要 elasticsearch.yml 如下配置： 1node.attr.box_type: warm 同时，也可以在 elasticsearch.yml 中设置 index.codec:best_compression 保证 warm 节点的压缩配置。 当索引不再被频繁查询时，可通过 index.routing.allocation.require.box_type:warm，将索引标记为 warm，从而保证索引不写入 hot 节点，以便将 SSD 磁盘资源用在刀刃上。一旦设置这个属性，ES 会自动将索引合并到 warm 节点。 协调（coordinating）节点协调节点用于做分布式里的协调，将各分片或节点返回的数据整合后返回。该节点不会被选作主节点，也不会存储任何索引数据。该服务器主要用于查询负载均衡。在查询的时候，通常会涉及到从多个 node 服务器上查询数据，并将请求分发到多个指定的 node 服务器，并对各个 node 服务器返回的结果进行一个汇总处理，最终返回给客户端。在 ES 集群中，所有的节点都有可能是协调节点，但是，可以通过设置 node.master、node.data、node.ingest 都为 false 来设置专门的协调节点。需要较好的 CPU 和较高的内存。 node.master:false 和 node.data:true，该 node 服务器只作为一个数据节点，只用于存储索引数据，使该 node 服务器功能单一，只用于数据存储和数据查询，降低其资源消耗率。 node.master:true 和 node.data:false，该 node 服务器只作为一个主节点，但不存储任何索引数据，该 node 服务器将使用自身空闲的资源，来协调各种创建索引请求或者查询请求，并将这些请求合理分发到相关的 node 服务器上。 node.master:false 和 node.data:false，该 node 服务器即不会被选作主节点，也不会存储任何索引数据。该服务器主要用于查询负载均衡。在查询的时候，通常会涉及到从多个 node 服务器上查询数据，并将请求分发到多个指定的 node 服务器，并对各个 node 服务器返回的结果进行一个汇总处理，最终返回给客户端。 5.2. 关闭 data 节点服务器中的 http 功能针对 Elasticsearch 集群中的所有数据节点，不用开启 http 服务。将其中的配置参数这样设置，http.enabled:false，同时也不要安装 head, bigdesk, marvel 等监控插件，这样保证 data 节点服务器只需处理创建&#x2F;更新&#x2F;删除&#x2F;查询索引数据等操作。 http 功能可以在非数据节点服务器上开启，上述相关的监控插件也安装到这些服务器上，用于监控 Elasticsearch 集群状态等数据信息。这样做一来出于数据安全考虑，二来出于服务性能考虑。 5.3. 一台服务器上最好只部署一个 node一台物理服务器上可以启动多个 node 服务器节点（通过设置不同的启动 port），但一台服务器上的 CPU、内存、硬盘等资源毕竟有限，从服务器性能考虑，不建议一台服务器上启动多个 node 节点。 5.4. 集群分片设置ES 一旦创建好索引后，就无法调整分片的设置，而在 ES 中，一个分片实际上对应一个 lucene 索引，而 lucene 索引的读写会占用很多的系统资源，因此，分片数不能设置过大；所以，在创建索引时，合理配置分片数是非常重要的。一般来说，我们遵循一些原则： 控制每个分片占用的硬盘容量不超过 ES 的最大 JVM 的堆空间设置（一般设置不超过 32 G，参考上面的 JVM 内存设置原则），因此，如果索引的总容量在 500 G 左右，那分片大小在 16 个左右即可；当然，最好同时考虑原则 2。 考虑一下 node 数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数，很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了 1 个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以，一般都设置分片数不超过节点数的 3 倍。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch Rest API","slug":"database/elasticsearch/elasticsearch-rest-api","date":"2022-06-08T01:30:00.000Z","updated":"2023-12-25T05:56:09.161Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-rest-api/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-rest-api/","excerpt":"","text":"1. 索引 API1.1. 创建索引新建 Index，可以直接向 ES 服务器发出 PUT 请求。 语法格式： 123456789PUT /my_index&#123; &quot;settings&quot;: &#123; ... any settings ... &#125;, &quot;mappings&quot;: &#123; &quot;type_one&quot;: &#123; ... any mappings ... &#125;, &quot;type_two&quot;: &#123; ... any mappings ... &#125;, ... &#125;&#125; 示例： 123456789PUT /user&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: 3, &quot;number_of_replicas&quot;: 2 &#125; &#125;&#125; 服务器返回一个 JSON 对象，里面的 acknowledged 字段表示操作成功。 1&#123;&quot;acknowledged&quot;:true,&quot;shards_acknowledged&quot;:true,&quot;index&quot;:&quot;user&quot;&#125; 如果你想禁止自动创建索引，可以通过在 config/elasticsearch.yml 的每个节点下添加下面的配置： 1action.auto_create_index: false 1.2. 删除索引然后，我们可以通过发送 DELETE 请求，删除这个 Index。 1DELETE /user 删除多个索引 12DELETE /index_one,index_twoDELETE /index_* 1.3. 查看索引可以通过 GET 请求查看索引信息 123456789101112131415161718192021222324# 查看索引相关信息GET kibana_sample_data_ecommerce# 查看索引的文档总数GET kibana_sample_data_ecommerce/_count# 查看前10条文档，了解文档格式GET kibana_sample_data_ecommerce/_search# _cat indices API# 查看indicesGET /_cat/indices/kibana*?v&amp;s=index# 查看状态为绿的索引GET /_cat/indices?v&amp;health=green# 按照文档个数排序GET /_cat/indices?v&amp;s=docs.count:desc# 查看具体的字段GET /_cat/indices/kibana*?pri&amp;v&amp;h=health,index,pri,rep,docs.count,mt# 查看索引占用的内存GET /_cat/indices?v&amp;h=i,tm&amp;s=tm:desc 1.4. 索引别名ES 的索引别名就是给一个索引或者多个索引起的另一个名字，典型的应用场景是针对索引使用的平滑切换。 首先，创建索引 my_index，然后将别名 my_alias 指向它，示例如下： 12PUT /my_indexPUT /my_index/_alias/my_alias 也可以通过如下形式： 123456POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;my_index&quot;, &quot;alias&quot;: &quot;my_alias&quot; &#125;&#125; ]&#125; 也可以在一次请求中增加别名和移除别名混合使用： 1234567POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;my_index&quot;, &quot;alias&quot;: &quot;my_alias&quot; &#125;&#125; &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;my_index_v2&quot;, &quot;alias&quot;: &quot;my_alias&quot; &#125;&#125; ]&#125; 需要注意的是，如果别名与索引是一对一的，使用别名索引文档或者查询文档是可以的，但是如果别名和索引是一对多的，使用别名会发生错误，因为 ES 不知道把文档写入哪个索引中去或者从哪个索引中读取文档。 ES 索引别名有个典型的应用场景是平滑切换，更多细节可以查看 Elasticsearch（ES）索引零停机（无需重启）无缝平滑切换的方法。 1.5. 打开&#x2F;关闭索引通过在 POST 中添加 _close 或 _open 可以打开、关闭索引。 打开索引 1234# 打开索引POST kibana_sample_data_ecommerce/_open# 关闭索引POST kibana_sample_data_ecommerce/_close 2. 文档123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151############Create Document#############create document. 自动生成 _idPOST users/_doc&#123; &quot;user&quot; : &quot;Mike&quot;, &quot;post_date&quot; : &quot;2019-04-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Kibana&quot;&#125;#create document. 指定Id。如果id已经存在，报错PUT users/_doc/1?op_type=create&#123; &quot;user&quot; : &quot;Jack&quot;, &quot;post_date&quot; : &quot;2019-05-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;#create document. 指定 ID 如果已经存在，就报错PUT users/_create/1&#123; &quot;user&quot; : &quot;Jack&quot;, &quot;post_date&quot; : &quot;2019-05-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot;&#125;### Get Document by ID#Get the document by IDGET users/_doc/1### Index &amp; Update#Update 指定 ID (先删除，在写入)GET users/_doc/1PUT users/_doc/1&#123; &quot;user&quot; : &quot;Mike&quot;&#125;#GET users/_doc/1#在原文档上增加字段POST users/_update/1/&#123; &quot;doc&quot;:&#123; &quot;post_date&quot; : &quot;2019-05-15T14:12:12&quot;, &quot;message&quot; : &quot;trying out Elasticsearch&quot; &#125;&#125;### Delete by Id# 删除文档DELETE users/_doc/1### Bulk 操作#执行两次，查看每次的结果#执行第1次POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value1&quot; &#125;&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test2&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value3&quot; &#125;&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_index&quot; : &quot;test&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125;#执行第2次POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value1&quot; &#125;&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test2&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value3&quot; &#125;&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_index&quot; : &quot;test&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125;### mget 操作GET /_mget&#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot; &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot; &#125; ]&#125;#URI中指定indexGET /test/_mget&#123; &quot;docs&quot; : [ &#123; &quot;_id&quot; : &quot;1&quot; &#125;, &#123; &quot;_id&quot; : &quot;2&quot; &#125; ]&#125;GET /_mget&#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_source&quot; : false &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_source&quot; : [&quot;field3&quot;, &quot;field4&quot;] &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_source&quot; : &#123; &quot;include&quot;: [&quot;user&quot;], &quot;exclude&quot;: [&quot;user.location&quot;] &#125; &#125; ]&#125;### msearch 操作POST kibana_sample_data_ecommerce/_msearch&#123;&#125;&#123;&quot;query&quot; : &#123;&quot;match_all&quot; : &#123;&#125;&#125;,&quot;size&quot;:1&#125;&#123;&quot;index&quot; : &quot;kibana_sample_data_flights&quot;&#125;&#123;&quot;query&quot; : &#123;&quot;match_all&quot; : &#123;&#125;&#125;,&quot;size&quot;:2&#125;### 清除测试数据#清除数据DELETE usersDELETE testDELETE test2 2.1. 创建文档指定 ID语法格式： 1PUT /_index/_type/_create/_id 示例： 123456PUT /user/_doc/_create/1&#123; &quot;user&quot;: &quot;张三&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;数据库管理&quot;&#125; 注意：指定 Id，如果 id 已经存在，则报错 自动生成 ID新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。 语法格式： 1POST /_index/_type 示例： 123456POST /user/_doc&#123; &quot;user&quot;: &quot;张三&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;超级管理员&quot;&#125; 2.2. 删除文档语法格式： 1DELETE /_index/_doc/_id 示例： 1DELETE /user/_doc/1 2.3. 更新文档先删除，再写入语法格式： 1PUT /_index/_type/_id 示例： 123456PUT /user/_doc/1&#123; &quot;user&quot;: &quot;李四&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;超级管理员&quot;&#125; 在原文档上增加字段语法格式： 1POST /_index/_update/_id 示例： 123456POST /user/_update/1&#123; &quot;doc&quot;:&#123; &quot;age&quot; : &quot;30&quot; &#125;&#125; 2.4. 查询文档指定 ID 查询语法格式： 1GET /_index/_type/_id 示例： 1GET /user/_doc/1 结果： 1234567891011121314&#123; &quot;_index&quot;: &quot;user&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;_seq_no&quot;: 536248, &quot;_primary_term&quot;: 2, &quot;found&quot;: true, &quot;_source&quot;: &#123; &quot;user&quot;: &quot;张三&quot;, &quot;title&quot;: &quot;工程师&quot;, &quot;desc&quot;: &quot;数据库管理&quot; &#125;&#125; 返回的数据中，found 字段表示查询成功，_source 字段返回原始记录。 如果 id 不正确，就查不到数据，found 字段就是 false 查询所有记录使用 GET 方法，直接请求 /index/type/_search，就会返回所有记录。 123456789101112131415161718192021222324252627282930313233343536373839$ curl &#x27;localhost:9200/user/admin/_search?pretty&#x27;&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 3, &quot;successful&quot; : 3, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 2, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;admin&quot;, &quot;_id&quot; : &quot;WWuoDG8BHwECs7SiYn93&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;李四&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;系统管理&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;admin&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;张三&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;超级管理员&quot; &#125; &#125; ] &#125;&#125; 上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。 total：返回记录数，本例是 2 条。 max_score：最高的匹配程度，本例是1.0。 hits：返回的记录组成的数组。 返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。 2.5. 全文搜索ES 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。 1234$ curl -H &#x27;Content-Type: application/json&#x27; &#x27;localhost:9200/user/admin/_search?pretty&#x27; -d &#x27;&#123;&quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;管理&quot; &#125;&#125;&#125;&#x27; 上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含”软件”这个词。返回结果如下。 1234567891011121314151617181920212223242526272829303132333435363738&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 3, &quot;successful&quot; : 3, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 2, &quot;max_score&quot; : 0.38200712, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;admin&quot;, &quot;_id&quot; : &quot;WWuoDG8BHwECs7SiYn93&quot;, &quot;_score&quot; : 0.38200712, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;李四&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;系统管理&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;user&quot;, &quot;_type&quot; : &quot;admin&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.3487891, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;张三&quot;, &quot;title&quot; : &quot;工程师&quot;, &quot;desc&quot; : &quot;超级管理员&quot; &#125; &#125; ] &#125;&#125; Elastic 默认一次返回 10 条结果，可以通过size字段改变这个设置，还可以通过from字段，指定位移。 123456$ curl &#x27;localhost:9200/user/admin/_search&#x27; -d &#x27;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;管理&quot; &#125;&#125;, &quot;from&quot;: 1, &quot;size&quot;: 1&#125;&#x27; 上面代码指定，从位置 1 开始（默认是从位置 0 开始），只返回一条结果。 2.6. 逻辑运算如果有多个搜索关键字， Elastic 认为它们是or关系。 1234$ curl &#x27;localhost:9200/user/admin/_search&#x27; -d &#x27;&#123;&quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;desc&quot; : &quot;软件 系统&quot; &#125;&#125;&#125;&#x27; 上面代码搜索的是软件 or 系统。 如果要执行多个关键词的and搜索，必须使用布尔查询。 1234567891011$ curl -H &#x27;Content-Type: application/json&#x27; &#x27;localhost:9200/user/admin/_search?pretty&#x27; -d &#x27;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;管理&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;desc&quot;: &quot;超级&quot; &#125; &#125; ] &#125; &#125;&#125;&#x27; 2.7. 批量执行支持在一次 API 调用中，对不同的索引进行操作 支持四种类型操作 index create update delete 操作中单条操作失败，并不会影响其他操作。 返回结果包括了每一条操作执行的结果。 12345678POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value1&quot; &#125;&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test2&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value3&quot; &#125;&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_index&quot; : &quot;test&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125; 说明：上面的示例如果执行多次，执行结果都不一样。 2.8. 批量读取读多个索引 12345678910111213GET /_mget&#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot; &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot; &#125; ]&#125; 读一个索引 12345678910111213141516171819202122232425262728293031323334353637GET /test/_mget&#123; &quot;docs&quot; : [ &#123; &quot;_id&quot; : &quot;1&quot; &#125;, &#123; &quot;_id&quot; : &quot;2&quot; &#125; ]&#125;GET /_mget&#123; &quot;docs&quot; : [ &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_source&quot; : false &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_source&quot; : [&quot;field3&quot;, &quot;field4&quot;] &#125;, &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_source&quot; : &#123; &quot;include&quot;: [&quot;user&quot;], &quot;exclude&quot;: [&quot;user.location&quot;] &#125; &#125; ]&#125; 2.9. 批量查询12345POST kibana_sample_data_ecommerce/_msearch&#123;&#125;&#123;&quot;query&quot; : &#123;&quot;match_all&quot; : &#123;&#125;&#125;,&quot;size&quot;:1&#125;&#123;&quot;index&quot; : &quot;kibana_sample_data_flights&quot;&#125;&#123;&quot;query&quot; : &#123;&quot;match_all&quot; : &#123;&#125;&#125;,&quot;size&quot;:2&#125; 2.10. Request Body &amp; DSL1234567GET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true&#123; &quot;profile&quot;:&quot;true&quot;, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 分页123456789GET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true&#123; &quot;profile&quot;: &quot;true&quot;, &quot;from&quot;: 0, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 排序最好在数字型或日期型字段上排序 因为对于多值类型或分析过的字段排序，系统会选一个值，无法得知该值 1234567891011121314GET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true&#123; &quot;profile&quot;: &quot;true&quot;, &quot;sort&quot;: [ &#123; &quot;order_date&quot;: &quot;desc&quot; &#125; ], &quot;from&quot;: 1, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; _source 过滤如果 _source 没有存储，那就只返回匹配的文档的元数据 _source 支持使用通配符，如：_source[&quot;name*&quot;, &quot;desc*&quot;] 示例： 12345678910111213GET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true&#123; &quot;profile&quot;: &quot;true&quot;, &quot;_source&quot;: [ &quot;order_date&quot;, &quot;category.keyword&quot; ], &quot;from&quot;: 1, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 脚本字段123456789101112131415161718GET /kibana_sample_data_ecommerce/_search?ignore_unavailable=true&#123; &quot;profile&quot;: &quot;true&quot;, &quot;script_fields&quot;: &#123; &quot;new_field&quot;: &#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;:&quot;doc[&#x27;order_date&#x27;].value+&#x27; hello&#x27;&quot; &#125; &#125; &#125;, &quot;from&quot;: 1, &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 使用查询表达式 - Match123456789101112131415161718192021POST movies/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;last christmas&quot; &#125; &#125;&#125;POST movies/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;last christmas&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125; 短语搜索 - Match Phrase1234567891011POST movies/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;title&quot;:&#123; &quot;query&quot;: &quot;last christmas&quot; &#125; &#125; &#125;&#125; 3. 集群 API Elasticsearch 官方之 Cluster API 一些集群级别的 API 可能会在节点的子集上运行，这些节点可以用节点过滤器指定。例如，任务管理、节点统计和节点信息 API 都可以报告来自一组过滤节点而不是所有节点的结果。 节点过滤器以逗号分隔的单个过滤器列表的形式编写，每个过滤器从所选子集中添加或删除节点。每个过滤器可以是以下之一： _all：将所有节点添加到子集 _local：将本地节点添加到子集 _master：将当前主节点添加到子集 根据节点 ID 或节点名将匹配节点添加到子集 根据 IP 地址或主机名将匹配节点添加到子集 使用通配符，将节点名、地址名或主机名匹配的节点添加到子集 master:true, data:true, ingest:true, voting_only:true, ml:true 或 coordinating_only:true, 分别意味着将所有主节点、所有数据节点、所有摄取节点、所有仅投票节点、所有机器学习节点和所有协调节点添加到子集中。 master:false, data:false, ingest:false, voting_only:true, ml:false 或 coordinating_only:false, 分别意味着将所有主节点、所有数据节点、所有摄取节点、所有仅投票节点、所有机器学习节点和所有协调节点排除在子集外。 配对模式，使用 * 通配符，格式为 attrname:attrvalue，将所有具有自定义节点属性的节点添加到子集中，其名称和值与相应的模式匹配。自定义节点属性是通过 node.attr.attrname: attrvalue 形式在配置文件中设置的。 1234567891011121314151617181920212223# 如果没有给出过滤器，默认是查询所有节点GET /_nodes# 查询所有节点GET /_nodes/_all# 查询本地节点GET /_nodes/_local# 查询主节点GET /_nodes/_master# 根据名称查询节点（支持通配符）GET /_nodes/node_name_goes_hereGET /_nodes/node_name_goes_*# 根据地址查询节点（支持通配符）GET /_nodes/10.0.0.3,10.0.0.4GET /_nodes/10.0.0.*# 根据规则查询节点GET /_nodes/_all,master:falseGET /_nodes/data:true,ingest:trueGET /_nodes/coordinating_only:trueGET /_nodes/master:true,voting_only:false# 根据自定义属性查询节点（如：查询配置文件中含 node.attr.rack:2 属性的节点）GET /_nodes/rack:2GET /_nodes/ra*:2GET /_nodes/ra*:2* 3.1. 集群健康 API1234GET /_cluster/healthGET /_cluster/health?level=shardsGET /_cluster/health/kibana_sample_data_ecommerce,kibana_sample_data_flightsGET /_cluster/health/kibana_sample_data_flights?level=shards 3.2. 集群状态 API集群状态 API 返回表示整个集群状态的元数据。 1GET /_cluster/state 4. 节点 API Elasticsearch 官方之 cat Nodes API——返回有关集群节点的信息。 1234# 查看默认的字段GET /_cat/nodes?v=true# 查看指定的字段GET /_cat/nodes?v=true&amp;h=id,ip,port,v,m 5. 分片 API Elasticsearch 官方之 cat Shards API——shards 命令是哪些节点包含哪些分片的详细视图。它会告诉你它是主还是副本、文档数量、它在磁盘上占用的字节数以及它所在的节点。 123456# 查看默认的字段GET /_cat/shards# 根据名称查询分片（支持通配符）GET /_cat/shards/my-index-*# 查看指定的字段GET /_cat/shards?h=index,shard,prirep,state,unassigned.reason 6. 监控 APIElasticsearch 中集群相关的健康、统计等相关的信息都是围绕着 cat API 进行的。 通过 GET 请求发送 cat，下面列出了所有可用的 API： 123456789101112131415161718192021222324252627282930GET /_cat=^.^=/_cat/allocation/_cat/shards/_cat/shards/&#123;index&#125;/_cat/master/_cat/nodes/_cat/tasks/_cat/indices/_cat/indices/&#123;index&#125;/_cat/segments/_cat/segments/&#123;index&#125;/_cat/count/_cat/count/&#123;index&#125;/_cat/recovery/_cat/recovery/&#123;index&#125;/_cat/health/_cat/pending_tasks/_cat/aliases/_cat/aliases/&#123;alias&#125;/_cat/thread_pool/_cat/thread_pool/&#123;thread_pools&#125;/_cat/plugins/_cat/fielddata/_cat/fielddata/&#123;fields&#125;/_cat/nodeattrs/_cat/repositories/_cat/snapshots/&#123;repository&#125;/_cat/templates","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 集群","slug":"database/elasticsearch/elasticsearch-cluster","date":"2022-06-08T01:30:00.000Z","updated":"2023-12-25T05:56:09.159Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-cluster/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-cluster/","excerpt":"","text":"1. 集群1.1. 空集群如果我们启动了一个单独的节点，里面不包含任何的数据和索引，那我们的集群看起来就是一个包含空内容节点的集群。 一个运行中的 Elasticsearch 实例称为一个节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。 作为用户，我们可以将请求发送到集群中的任何节点，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。 1.2. 集群健康Elasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 集群健康 ， 它在 status 字段中展示为 green 、 yellow 或者 red 。 1GET /_cluster/health 在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容： 123456789101112&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 0, &quot;active_shards&quot;: 0, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0&#125; status 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下： **green**：所有的主分片和副本分片都正常运行。 **yellow**：所有的主分片都正常运行，但不是所有的副本分片都正常运行。 **red**：有主分片没能正常运行。 1.3. 添加索引我们往 Elasticsearch 添加数据时需要用到 索引 —— 保存相关数据的地方。索引实际上是指向一个或者多个物理分片的逻辑命名空间 。 一个 分片 是一个底层的 工作单元 ，它仅保存了全部数据中的一部分。现在我们只需知道一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。 Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。 一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。 技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。 一个副本分片只是一个主分片的拷贝。副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。 在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。 让我们在包含一个空节点的集群内创建名为 blogs 的索引。 索引在默认情况下会被分配 5 个主分片， 但是为了演示目的，我们将分配 3 个主分片和一份副本（每个主分片拥有一个副本分片）： 1234567PUT /blogs&#123; &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 3, &quot;number_of_replicas&quot; : 1 &#125;&#125; 我们的集群现在是 _拥有一个索引的单节点集群_。所有 3 个主分片都被分配在 Node 1 。 Figure 2. 拥有一个索引的单节点集群 如果我们现在查看集群健康，我们将看到如下内容： 1234567891011121314151617&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;yellow&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 1, &quot;number_of_data_nodes&quot;: 1, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 3, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 3, &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot;: 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 50&#125; 集群 status 值为 yellow 没有被分配到任何节点的副本数 集群的健康状况为 yellow 则表示全部 主 分片都正常运行（集群可以正常服务所有请求），但是 副本 分片没有全部处在正常状态。 实际上，所有 3 个副本分片都是 unassigned —— 它们都没有被分配到任何节点。 在同一个节点上既保存原始数据又保存副本是没有意义的，因为一旦失去了那个节点，我们也将丢失该节点上的所有副本数据。 当前我们的集群是正常运行的，但是在硬件故障时有丢失数据的风险。 1.4. 添加故障转移当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。 为了测试第二个节点启动后的情况，你可以在同一个目录内，完全依照启动第一个节点的方式来启动一个新节点（参考安装并运行 Elasticsearch）。多个节点可以共享同一个目录。 当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的 cluster.name 配置，它就会自动发现集群并加入到其中。 但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。 如果启动了第二个节点，我们的集群将会拥有两个节点的集群——所有主分片和副本分片都已被分配。 Figure 3. 拥有两个节点的集群——所有主分片和副本分片都已被分配 当第二个节点加入到集群后，3 个 副本分片 将会分配到这个节点上——每个主分片对应一个副本分片。 这意味着当集群内任何一个节点出现问题时，我们的数据都完好无损。 所有新近被索引的文档都将会保存在主分片上，然后被并行的复制到对应的副本分片上。这就保证了我们既可以从主分片又可以从副本分片上获得文档。 cluster-health 现在展示的状态为 green ，这表示所有 6 个分片（包括 3 个主分片和 3 个副本分片）都在正常运行。 1234567891011121314151617&#123; &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;status&quot;: &quot;green&quot;, &quot;timed_out&quot;: false, &quot;number_of_nodes&quot;: 2, &quot;number_of_data_nodes&quot;: 2, &quot;active_primary_shards&quot;: 3, &quot;active_shards&quot;: 6, &quot;relocating_shards&quot;: 0, &quot;initializing_shards&quot;: 0, &quot;unassigned_shards&quot;: 0, &quot;delayed_unassigned_shards&quot;: 0, &quot;number_of_pending_tasks&quot;: 0, &quot;number_of_in_flight_fetch&quot;: 0, &quot;task_max_waiting_in_queue_millis&quot;: 0, &quot;active_shards_percent_as_number&quot;: 100&#125; 集群 status 值为 green 我们的集群现在不仅仅是正常运行的，并且还处于 始终可用 的状态。 1.5. 水平扩容怎样为我们的正在增长中的应用程序按需扩容呢？ 当启动了第三个节点，我们的集群将拥有三个节点的集群——为了分散负载而对分片进行重新分配。 Figure 4. 拥有三个节点的集群——为了分散负载而对分片进行重新分配 Node 1 和 Node 2 上各有一个分片被迁移到了新的 Node 3 节点，现在每个节点上都拥有 2 个分片，而不是之前的 3 个。 这表示每个节点的硬件资源（CPU, RAM, I&#x2F;O）将被更少的分片所共享，每个分片的性能将会得到提升。 分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 我们这个拥有 6 个分片（3 个主分片和 3 个副本分片）的索引可以最大扩容到 6 个节点，每个节点上存在一个分片，并且每个分片拥有所在节点的全部资源。 1.6. 更多的扩容但是如果我们想要扩容超过 6 个节点怎么办呢？ 主分片的数目在索引创建时就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。 在运行中的集群上是可以动态调整副本分片数目的，我们可以按需伸缩集群。让我们把副本数从默认的 1 增加到 2 ： 1234PUT /blogs/_settings&#123; &quot;number_of_replicas&quot; : 2&#125; blogs 索引现在拥有 9 个分片：3 个主分片和 6 个副本分片。 这意味着我们可以将集群扩容到 9 个节点，每个节点上一个分片。相比原来 3 个节点时，集群搜索性能可以提升 3 倍。 Figure 5. 将参数 number_of_replicas 调大到 2 当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。 但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去 2 个节点的情况下不丢失任何数据。 1.7. 应对故障我们之前说过 Elasticsearch 可以应对节点故障，接下来让我们尝试下这个功能。 如果我们关闭第一个节点，这时集群的状态为关闭了一个节点后的集群。 Figure 6. 关闭了一个节点后的集群 我们关闭的节点是一个主节点。而集群必须拥有一个主节点来保证正常工作，所以发生的第一件事情就是选举一个新的主节点： Node 2 。 在我们关闭 Node 1 的同时也失去了主分片 1 和 2 ，并且在缺失主分片的时候索引也不能正常工作。 如果此时来检查集群的状况，我们看到的状态将会为 red ：不是所有主分片都在正常工作。 幸运的是，在其它节点上存在着这两个主分片的完整副本， 所以新的主节点立即将这些分片在 Node 2 和 Node 3 上对应的副本分片提升为主分片， 此时集群的状态将会为 yellow 。 这个提升主分片的过程是瞬间发生的，如同按下一个开关一般。 为什么我们集群状态是 yellow 而不是 green 呢？ 虽然我们拥有所有的三个主分片，但是同时设置了每个主分片需要对应 2 份副本分片，而此时只存在一份副本分片。 所以集群不能为 green 的状态，不过我们不必过于担心：如果我们同样关闭了 Node 2 ，我们的程序 依然 可以保持在不丢任何数据的情况下运行，因为 Node 3 为每一个分片都保留着一份副本。 如果我们重新启动 Node 1 ，集群可以将缺失的副本分片再次进行分配，那么集群的状态也将如 Figure 5. 将参数 number_of_replicas 调大到 2 所示。 如果 Node 1 依然拥有着之前的分片，它将尝试去重用它们，同时仅从主分片复制发生了修改的数据文件。 到目前为止，你应该对分片如何使得 Elasticsearch 进行水平扩容以及数据保障等知识有了一定了解。 接下来我们将讲述关于分片生命周期的更多细节。 2. 分片 为什么搜索是 近 实时的？ 为什么文档的 CRUD (创建-读取-更新-删除) 操作是 实时 的? Elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据? 为什么删除文档不会立刻释放空间？ refresh, flush, 和 optimize API 都做了什么, 你什么情况下应该使用他们？ 2.1. 使文本可被搜索必须解决的第一个挑战是如何使文本可被搜索。 传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值(这里指单词)的能力。 倒排索引包含一个有序列表，列表包含所有文档出现过的不重复个体，或称为 词项 ，对于每一个词项，包含了它所有曾出现过文档的列表。 123456Term | Doc 1 | Doc 2 | Doc 3 | ...------------------------------------brown | X | | X | ...fox | X | X | X | ...quick | X | X | | ...the | X | | X | ... 当讨论倒排索引时，我们会谈到 文档 标引，因为历史原因，倒排索引被用来对整个非结构化文本文档进行标引。 Elasticsearch 中的 文档 是有字段和值的结构化 JSON 文档。事实上，在 JSON 文档中， 每个被索引的字段都有自己的倒排索引。 这个倒排索引相比特定词项出现过的文档列表，会包含更多其它信息。它会保存每一个词项出现过的文档总数， 在对应的文档中一个具体词项出现的总次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度，等等。这些统计信息允许 Elasticsearch 决定哪些词比其它词更重要，哪些文档比其它文档更重要。 为了能够实现预期功能，倒排索引需要知道集合中的 所有 文档，这是需要认识到的关键问题。 早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 一旦新的索引就绪，旧的就会被其替换，这样最近的变化便可以被检索到。 2.2. 不变性倒排索引被写入磁盘后是 不可改变 的:它永远不会修改。 不变性有重要的价值： 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。 其它缓存(像 filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。 写入单个大的倒排索引允许数据被压缩，减少磁盘 I&#x2F;O 和 需要被缓存到内存的索引的使用量。 当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。 2.3. 动态更新索引下一个需要被解决的问题是怎样在保留不变性的前提下实现倒排索引的更新？答案是: 用更多的索引。 通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到—从最早的开始—查询完后再对结果进行合并。 Elasticsearch 基于 Lucene, 这个 java 库引入了 按段搜索 的概念。 每一 段 本身都是一个倒排索引， 但 索引 在 Lucene 中除表示所有 段 的集合外， 还增加了 提交点 的概念 — 一个列出了所有已知段的文件，就像在 Figure 16, “一个 Lucene 索引包含一个提交点和三个段” 中描绘的那样。 如 Figure 17, “一个在内存缓存中包含新文档的 Lucene 索引” 所示，新的文档首先被添加到内存索引缓存中，然后写入到一个基于磁盘的段，如 Figure 18, “在一次提交后，一个新的段被添加到提交点而且缓存被清空。” 所示。 Figure 16. 一个 Lucene 索引包含一个提交点和三个段 被混淆的概念是，一个 Lucene 索引 我们在 Elasticsearch 称作 分片 。 一个 Elasticsearch 索引 是分片的集合。 当 Elasticsearch 在索引中搜索的时候， 他发送查询到每一个属于索引的分片(Lucene 索引)，然后像 执行分布式检索 提到的那样，合并每个分片的结果到一个全局的结果集。 逐段搜索会以如下流程进行工作： 新文档被收集到内存索引缓存， 见 Figure 17, “一个在内存缓存中包含新文档的 Lucene 索引” 。 不时地, 缓存被 提交 ： 一个新的段—一个追加的倒排索引—被写入磁盘。 一个新的包含新段名字的 提交点 被写入磁盘。 磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。 新的段被开启，让它包含的文档可见以被搜索。 内存缓存被清空，等待接收新的文档。 Figure 17. 一个在内存缓存中包含新文档的 Lucene 索引 Figure 18. 在一次提交后，一个新的段被添加到提交点而且缓存被清空。 当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。 这种方式可以用相对较低的成本将新文档添加到索引。 2.4. 删除和更新段是不可改变的，所以既不能从把文档从旧的段中移除，也不能修改旧的段来进行反映文档的更新。 取而代之的是，每个提交点会包含一个 .del 文件，文件中会列出这些被删除文档的段信息。 当一个文档被 “删除” 时，它实际上只是在 .del 文件中被 标记 删除。一个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。 文档更新也是类似的操作方式：当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。 可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。 在 段合并 , 我们展示了一个被删除的文档是怎样被文件系统移除的。 2.5. 近实时搜索随着按段（per-segment）搜索的发展，一个新的文档从索引到可被搜索的延迟显著降低了。新文档在几分钟之内即可被检索，但这样还是不够快。 磁盘在这里成为了瓶颈。提交（Commiting）一个新的段到磁盘需要一个 fsync 来确保段被物理性地写入磁盘，这样在断电的时候就不会丢失数据。 但是 fsync 操作代价很大; 如果每次索引一个文档都去执行一次的话会造成很大的性能问题。 我们需要的是一个更轻量的方式来使一个文档可被搜索，这意味着 fsync 要从整个过程中被移除。 在 Elasticsearch 和磁盘之间是文件系统缓存。 像之前描述的一样， 在内存索引缓冲区（ Figure 19, “在内存缓冲区中包含了新文档的 Lucene 索引” ）中的文档会被写入到一个新的段中（ Figure 20, “缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交” ）。 但是这里新段会被先写入到文件系统缓存—这一步代价会比较低，稍后再被刷新到磁盘—这一步代价比较高。不过只要文件已经在缓存中， 就可以像其它文件一样被打开和读取了。 Figure 19. 在内存缓冲区中包含了新文档的 Lucene 索引 Lucene 允许新段被写入和打开—使其包含的文档在未进行一次完整提交时便对搜索可见。 这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。 Figure 20. 缓冲区的内容已经被写入一个可被搜索的段中，但还没有进行提交 2.6. refresh API在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是 近 实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。 这些行为可能会对新用户造成困惑: 他们索引了一个文档然后尝试搜索它，但却没有搜到。这个问题的解决办法是用 refresh API 执行一次手动刷新: 12POST /_refreshPOST /blogs/_refresh 刷新（Refresh）所有的索引 只刷新（Refresh） blogs 索引 尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产环境下每次索引一个文档都去手动刷新。 相反，你的应用需要意识到 Elasticsearch 的近实时的性质，并接受它的不足。 并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件， 你可能想优化索引速度而不是近实时搜索， 可以通过设置 refresh_interval ， 降低每个索引的刷新频率： 123456PUT /my_logs&#123; &quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;30s&quot; &#125;&#125; 每 30 秒刷新 my_logs 索引。 refresh_interval 可以在既存索引上进行动态更新。 在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来： 12345PUT /my_logs/_settings&#123; &quot;refresh_interval&quot;: -1 &#125;PUT /my_logs/_settings&#123; &quot;refresh_interval&quot;: &quot;1s&quot; &#125; 关闭自动刷新。 每秒自动刷新。 refresh_interval 需要一个 持续时间 值， 例如 1s （1 秒） 或 2m （2 分钟）。 一个绝对值 1 表示的是 1 毫秒 –无疑会使你的集群陷入瘫痪。 2.7. 持久化变更如果没有用 fsync 把数据从文件系统缓存刷（flush）到硬盘，我们不能保证数据在断电甚至是程序正常退出之后依然存在。为了保证 Elasticsearch 的可靠性，需要确保数据变化被持久化到磁盘。 在 动态更新索引，我们说一次完整的提交会将段刷到磁盘，并写入一个包含所有段列表的提交点。Elasticsearch 在启动或重新打开一个索引的过程中使用这个提交点来判断哪些段隶属于当前分片。 即使通过每秒刷新（refresh）实现了近实时搜索，我们仍然需要经常进行完整提交来确保能从失败中恢复。但在两次提交之间发生变化的文档怎么办？我们也不希望丢失掉这些数据。 Elasticsearch 增加了一个 translog ，或者叫事务日志，在每一次对 Elasticsearch 进行操作时均进行了日志记录。通过 translog ，整个流程看起来是下面这样： 一个文档被索引之后，就会被添加到内存缓冲区，并且 追加到了 translog ，正如 Figure 21, “新的文档被添加到内存缓冲区并且被追加到了事务日志” 描述的一样。 Figure 21. 新的文档被添加到内存缓冲区并且被追加到了事务日志 刷新（refresh）使分片处于 Figure 22, “刷新（refresh）完成后, 缓存被清空但是事务日志不会” 描述的状态，分片每秒被刷新（refresh）一次： 这些在内存缓冲区的文档被写入到一个新的段中，且没有进行 fsync 操作。 这个段被打开，使其可被搜索。 内存缓冲区被清空。 Figure 22. 刷新（refresh）完成后, 缓存被清空但是事务日志不会 这个进程继续工作，更多的文档被添加到内存缓冲区和追加到事务日志（见 Figure 23, “事务日志不断积累文档” ）。 Figure 23. 事务日志不断积累文档 每隔一段时间—例如 translog 变得越来越大—索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行（见 Figure 24, “在刷新（flush）之后，段被全量提交，并且事务日志被清空” ）： 所有在内存缓冲区的文档都被写入一个新的段。 缓冲区被清空。 一个提交点被写入硬盘。 文件系统缓存通过 fsync 被刷新（flush）。 老的 translog 被删除。 translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 Elasticsearch 启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放 translog 中所有在最后一次提交后发生的变更操作。 translog 也被用来提供实时 CRUD 。当你试着通过 ID 查询、更新、删除一个文档，它会在尝试从相应的段中检索之前， 首先检查 translog 任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。 Figure 24. 在刷新（flush）之后，段被全量提交，并且事务日志被清空 2.8. flush API这个执行一个提交并且截断 translog 的行为在 Elasticsearch 被称作一次 flush 。 分片每 30 分钟被自动刷新（flush），或者在 translog 太大的时候也会刷新。请查看 translog 文档 来设置，它可以用来 控制这些阈值： flush API 可以被用来执行一个手工的刷新（flush）: 12POST /blogs/_flushPOST /_flush?wait_for_ongoing 刷新（flush） blogs 索引。 刷新（flush）所有的索引并且并且等待所有刷新在返回前完成。 你很少需要自己手动执行 flush 操作；通常情况下，自动刷新就足够了。 这就是说，在重启节点或关闭索引之前执行 flush 有益于你的索引。当 Elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，所以如果日志越短，恢复越快。 translog 的目的是保证操作不会丢失。这引出了这个问题： Translog 有多安全？ 在文件被 fsync 到磁盘前，被写入的文件在重启之后就会丢失。默认 translog 是每 5 秒被 fsync 刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)。这个过程在主分片和复制分片都会发生。最终， 基本上，这意味着在整个请求被 fsync 到主分片和复制分片的 translog 之前，你的客户端不会得到一个 200 OK 响应。 在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小（特别是 bulk 导入，它在一次请求中平摊了大量文档的开销）。 但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。比如，写入的数据被缓存到内存中，再每 5 秒执行一次 fsync 。 这个行为可以通过设置 durability 参数为 async 来启用： 12345PUT /my_index/_settings&#123; &quot;index.translog.durability&quot;: &quot;async&quot;, &quot;index.translog.sync_interval&quot;: &quot;5s&quot;&#125; 这个选项可以针对索引单独设置，并且可以动态进行修改。如果你决定使用异步 translog 的话，你需要 保证 在发生 crash 时，丢失掉 sync_interval 时间段的数据也无所谓。请在决定前知晓这个特性。 如果你不确定这个行为的后果，最好是使用默认的参数（ &quot;index.translog.durability&quot;: &quot;request&quot; ）来避免数据丢失。 2.9. 段合并由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。 每一个段都会消耗文件句柄、内存和 cpu 运行周期。更重要的是，每个搜索请求都必须轮流检查每个段；所以段越多，搜索也就越慢。 Elasticsearch 通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。 段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。 启动段合并不需要你做任何事。进行索引和搜索时会自动进行。这个流程像在 Figure 25, “两个提交了的段和一个未提交的段正在被合并到一个更大的段” 中提到的一样工作： 1、 当索引的时候，刷新（refresh）操作会创建新的段并将段打开以供搜索使用。 2、 合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中。这并不会中断索引和搜索。 Figure 25. 两个提交了的段和一个未提交的段正在被合并到一个更大的段 Figure 26, “一旦合并结束，老的段被删除” 说明合并完成时的活动： 新的段被刷新（flush）到了磁盘。 ** 写入一个包含新段且排除旧的和较小的段的新提交点。 新的段被打开用来搜索。 老的段被删除。 Figure 26. 一旦合并结束，老的段被删除 合并大的段需要消耗大量的 I&#x2F;O 和 CPU 资源，如果任其发展会影响搜索性能。Elasticsearch 在默认情况下会对合并流程进行资源限制，所以搜索仍然 有足够的资源很好地执行。 2.10. optimize APIoptimize API 大可看做是 强制合并 API。它会将一个分片强制合并到 max_num_segments 参数指定大小的段数目。 这样做的意图是减少段的数量（通常减少到一个），来提升搜索性能。 optimize API 不应该 被用在一个活跃的索引————一个正积极更新的索引。后台合并流程已经可以很好地完成工作。 optimizing 会阻碍这个进程。不要干扰它！ 在特定情况下，使用 optimize API 颇有益处。例如在日志这种用例下，每天、每周、每月的日志被存储在一个索引中。 老的索引实质上是只读的；它们也并不太可能会发生变化。 在这种情况下，使用 optimize 优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速： 1POST /logstash-2014-10/_optimize?max_num_segments=1 合并索引中的每个分片为一个单独的段 请注意，使用 optimize API 触发段合并的操作不会受到任何资源上的限制。这可能会消耗掉你节点上全部的 I&#x2F;O 资源, 使其没有余裕来处理搜索请求，从而有可能使集群失去响应。 如果你想要对索引执行 optimize，你需要先使用分片分配（查看 迁移旧索引）把索引移到一个安全的节点，再执行。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 分析器","slug":"database/elasticsearch/elasticsearch-analyzer","date":"2022-06-08T01:25:00.000Z","updated":"2023-12-25T05:56:09.158Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-analyzer/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-analyzer/","excerpt":"","text":"Elasticsearch 分析器在 ES 中，不管是索引任务还是搜索工作，都需要使用 analyzer（分析器）。分析器，分为内置分析器和自定义的分析器。 分析器进一步由字符过滤器（Character Filters）、分词器（Tokenizer）和词元过滤器（Token Filters）三部分组成。它的执行顺序如下： character filters -&gt; tokenizer -&gt; token filters 1. 字符过滤器（Character Filters）character filter 的输入是原始的文本 text，如果配置了多个，它会按照配置的顺序执行，目前 ES 自带的 character filter 主要由如下 3 类： html strip character filter：从文本中剥离 HTML 元素，并用其解码值替换 HTML 实体（如，将 ＆amp; 替换为 **_＆_**）。 mapping character filter：自定义一个 map 映射，可以进行一些自定义的替换，如常用的大写变小写也可以在该环节设置。 pattern replace character filter：使用 java 正则表达式来匹配应替换为指定替换字符串的字符，此外，替换字符串可以引用正则表达式中的捕获组。 1.1. HTML strip character filterHTML strip 如下示例： 12345678GET /_analyze&#123; &quot;tokenizer&quot;: &quot;keyword&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot; ], &quot;text&quot;: &quot;&lt;p&gt;I&amp;apos;m so &lt;b&gt;happy&lt;/b&gt;!&lt;/p&gt;&quot;&#125; 经过 html_strip 字符过滤器处理后，输出如下： 1[ \\nI&#x27;m so happy!\\n ] 1.2. Mapping character filterMapping character filter 接收键和值映射（key &#x3D;&gt; value）作为配置参数，每当在预处理过程中遇到与键值映射中的键相同的字符串时，就会使用该键对应的值去替换它。 原始文本中的字符串和键值映射中的键的匹配是贪心的，在对给定的文本进行预处理过程中如果配置的键值映射存在包含关系，会优先匹配最长键。同样也可以用空字符串进行替换。 mapping char_filter 不像 html_strip 那样拆箱即可用，必须先进行配置才能使用，它有两个属性可以配置： 参数名称 参数说明 mappings 一组映射，每个元素的格式为 _key &#x3D;&gt; value_。 mappings_path 一个相对或者绝对的文件路径，指向一个每行包含一个 key &#x3D;&gt;value 映射的 UTF-8 编码文本映射文件。 mapping char_filter 示例如下： 12345678910111213141516171819202122GET /_analyze&#123; &quot;tokenizer&quot;: &quot;keyword&quot;, &quot;char_filter&quot;: [ &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;٠ =&gt; 0&quot;, &quot;١ =&gt; 1&quot;, &quot;٢ =&gt; 2&quot;, &quot;٣ =&gt; 3&quot;, &quot;٤ =&gt; 4&quot;, &quot;٥ =&gt; 5&quot;, &quot;٦ =&gt; 6&quot;, &quot;٧ =&gt; 7&quot;, &quot;٨ =&gt; 8&quot;, &quot;٩ =&gt; 9&quot; ] &#125; ], &quot;text&quot;: &quot;My license plate is ٢٥٠١٥&quot;&#125; 分析结果如下： 1[ My license plate is 25015 ] 1.3. Pattern Replace character filterPattern Replace character filter 支持如下三个参数： 参数名称 参数说明 pattern 必填参数，一个 java 的正则表达式。 replacement 替换字符串，可以使用 $1 ... $9 语法来引用捕获组。 flags Java 正则表达式的标志，具体参考 java 的 java.util.regex.Pattern 类的标志属性。 如将输入的 text 中大于一个的空格都转变为一个空格，在 settings 时，配置示例如下： 12345678&quot;char_filter&quot;: &#123; &quot;multi_space_2_one&quot;: &#123; &quot;pattern&quot;: &quot;[ ]+&quot;, &quot;type&quot;: &quot;pattern_replace&quot;, &quot;replacement&quot;: &quot; &quot; &#125;, ...&#125; 2. 分词器（Tokenizer）tokenizer 即分词器，也是 analyzer 最重要的组件，它对文本进行分词；一个 analyzer 必需且只可包含一个 tokenizer。 ES 自带默认的分词器是 standard tokenizer，标准分词器提供基于语法的分词（基于 Unicode 文本分割算法），并且适用于大多数语言。 此外有很多第三方的分词插件，如中文分词界最经典的 ik 分词器，它对应的 tokenizer 分为 ik_smart 和 ik_max_word，一个是智能分词（针对搜索侧），一个是全切分词（针对索引侧）。 ES 默认提供的分词器 standard 对中文分词不优化，效果差，一般会安装第三方中文分词插件，通常首先 elasticsearch-analysis-ik 插件，它其实是 ik 针对的 ES 的定制版。 2.1. elasticsearch-plugin 使用在安装 elasticsearch-analysis-ik 第三方之前，我们首先要了解 es 的插件管理工具 elasticsearch-plugin 的使用。 现在的 elasticsearch 安装完后，在安装目录的 bin 目录下会存在 elasticsearch-plugin 命令工具，用它来对 es 插件进行管理。 1bin/elasticsearch-plugin 其实该命令的是软连接，原始路径是： 1libexec/bin/elasticsearch-plugin 再进一步看脚本代码，你会发现，它是通过 elasticsearch-cli 执行 libexec/lib/tools/plugin-cli/elasticsearch-plugin-cli-x.x.x.jar。 但一般使用者了解 elasticsearch-plugin 命令使用就可： 12345678# 安装指定的插件到当前 ES 节点中elasticsearch-plugin install &#123;plugin_url&#125;# 显示当前 ES 节点已经安装的插件列表elasticsearch-plugin list# 删除已安装的插件elasticsearch-plugin remove &#123;plugin_name&#125; 在安装插件时，要保证安装的插件与 ES 版本一致。 2.2. elasticsearch-analysis-ik 安装在确定要安装的 ik 版本之后，执行如下命令： 1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v&#123;X.X.X&#125;/elasticsearch-analysis-ik-&#123;X.X.X&#125;.zip 执行完安装命令后，我们会发现在 plugins 中多了 analysis-ik 目录，这里面主要存放的是源码 jar 包，此外，在 config 文件里也多了 analysis-ik 目录，里面主要是 ik 相关的配置，如 IKAnalyzer.cfg.xml 配置、词典文件等。 123# 两个新增目录路径libexec/plugins/analysis-ik/libexec/config/analysis-ik/ 2.3. elasticsearch-analysis-ik 使用ES 5.X 版本开始安装完的 elasticsearch-analysis-ik 提供了两个分词器，分别对应名称是 ik_max_word 和 **ik_smart**，ik_max_word 是索引侧的分词器，走全切模式，ik_smart 是搜索侧的分词器，走智能分词，属于搜索模式。 索引 mapping 设置安装完 elasticsearch-analysis-ik 后，我们可以指定索引及指定字段设置可用的分析器（analyzer），示例如下： 12345678910111213141516171819202122232425262728&#123; &quot;qa&quot;: &#123; &quot;mappings&quot;: &#123; &quot;qa&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;properties&quot;: &#123; &quot;question&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: true, &quot;similarity&quot;: &quot;BM25&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_smart&quot; &#125;, &quot;answer&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: false, &quot;similarity&quot;: &quot;BM25&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_smart&quot; &#125;, ... &#125; &#125; &#125; &#125;&#125; 如上示例中，analyzer 指定 ik_max_word，即索引侧使用 ik 全切模式，search_analyzer 设置 ik_smart，即搜索侧使用 ik 智能分词模式。 查看 ik 分词结果es 提供了查看分词结果的 api **analyze**，具体示例如下： 12345GET &#123;index&#125;/_analyze&#123; &quot;analyzer&quot; : &quot;ik_smart&quot;, &quot;text&quot; : &quot;es 中文分词器安装&quot;&#125; 输出如下： 1234567891011121314151617181920212223242526272829303132&#123; &quot;tokens&quot;: [ &#123; &quot;token&quot;: &quot;es&quot;, &quot;start_offset&quot;: 0, &quot;end_offset&quot;: 2, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 0 &#125;, &#123; &quot;token&quot;: &quot;中文&quot;, &quot;start_offset&quot;: 3, &quot;end_offset&quot;: 5, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 1 &#125;, &#123; &quot;token&quot;: &quot;分词器&quot;, &quot;start_offset&quot;: 5, &quot;end_offset&quot;: 8, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 2 &#125;, &#123; &quot;token&quot;: &quot;安装&quot;, &quot;start_offset&quot;: 8, &quot;end_offset&quot;: 10, &quot;type&quot;: &quot;CN_WORD&quot;, &quot;position&quot;: 3 &#125; ]&#125; elasticsearch-analysis-ik 自定义词典elasticsearch-analysis-ik 本质是 ik 分词器，使用者根据实际需求可以扩展自定义的词典，具体主要分为如下 2 大类，每类又分为本地配置和远程配置 2 种： 自定义扩展词典； 自定义扩展停用词典； elasticsearch-analysis-ik 配置文件为 IKAnalyzer.cfg.xml，它位于 libexec/config/analysis-ik 目录下，具体配置结构如下： 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;!-- &lt;entry key=&quot;remote_ext_dict&quot;&gt;words_location&lt;/entry&gt; --&gt; &lt;!--用户可以在这里配置远程扩展停止词字典--&gt; &lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;&lt;/properties&gt; 当然，如果开发者认为 ik 默认的词表有问题，也可以进行调整，文件都在 libexec/config/analysis-ik 下，如 main.dic 为主词典，stopword.dic 为停用词表。 3. 词元过滤器（Token Filters）token filters 叫词元过滤器，或词项过滤器，对 tokenizer 分出的词进行过滤处理。常用的有转小写、停用词处理、同义词处理等等。一个 analyzer 可包含 0 个或多个词项过滤器，按配置顺序进行过滤。 以同义词过滤器的使用示例，具体如下： 1234567891011121314151617181920212223242526PUT /test_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;synonym&quot;: &#123; &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;my_stop&quot;, &quot;synonym&quot; ] &#125; &#125;, &quot;filter&quot;: &#123; &quot;my_stop&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;bar&quot; ] &#125;, &quot;synonym&quot;: &#123; &quot;type&quot;: &quot;synonym&quot;, &quot;lenient&quot;: true, &quot;synonyms&quot;: [ &quot;foo, bar =&gt; baz&quot; ] &#125; &#125; &#125; &#125; &#125;&#125; 3.1. 同义词Elasticsearch 同义词通过专有的同义词过滤器（synonym token filter）来进行工作，它允许在分析（analysis）过程中方便地处理同义词，一般是通过配置文件配置同义词。此外，同义词可以再建索引时（index-time synonyms）或者检索时（search-time synonyms）使用。 同义词（synonym）配置语法如上例子所示，es 同义词配置的 filter 语法具体如下选项： **_type_**：指定 synonym，表示同义词 filter； **_synonyms_path_**：指定同义词配置文件路径； **expand**：该参数决定映射行为的模式，默认为 true，表示扩展模式，具体示例如下： 当 expand == true 时， 1ipod, i-pod, i pod 等价于： 1ipod, i-pod, i pod =&gt; ipod, i-pod, i pod 当 expand == false 时， 1ipod, i-pod, i pod 仅映射第一个单词，等价于： 1ipod, i-pod, i pod =&gt; ipod **_lenient_**：如果值为 true 时，遇到那些无法解析的同义词规则时，忽略异常。默认为 false。 同义词文档格式elasticsearch 的同义词有如下两种形式： 单向同义词： 1ipod, i-pod, i pod =&gt; ipod 双向同义词： 1马铃薯, 土豆, potato 单向同义词不管索引还是检索时，箭头左侧的词都会映射成箭头右侧的词； 双向同义词是索引时，都建立同义词的倒排索引，检索时，同义词之间都会进行倒排索引的匹配。 同义词的文档化时，需要注意的是，同一个词在不同的同义词关系中出现时，其它同义词之间不具有传递性，这点需要注意。 假设如上示例中，如果“马铃薯”和其它两个同义词分成两行写： 12马铃薯,土豆马铃薯,potato 此时，elasticsearch 中不会将“土豆”和“potato”视为同义词关系，所以多个同义词要写在一起，这往往是开发中经常容易疏忽的点。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 安装","slug":"database/elasticsearch/elasticsearch-install","date":"2022-06-08T01:25:00.000Z","updated":"2023-12-25T05:56:09.159Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-install/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-install/","excerpt":"","text":"官方下载地址 1. Elasticsearch 配置ES 的默认配置文件为 config/elasticsearch.yml 基本配置说明如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465cluster.name: elasticsearch#配置es的集群名称，默认是elasticsearch，es会自动发现在同一网段下的es，如果在同一网段下有多个集群，就可以用这个属性来区分不同的集群。node.name: &#x27;Franz Kafka&#x27;#节点名，默认随机指定一个name列表中名字，该列表在es的jar包中config文件夹里name.txt文件中，其中有很多作者添加的有趣名字。node.master: true#指定该节点是否有资格被选举成为node，默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master。node.data: true#指定该节点是否存储索引数据，默认为true。index.number_of_shards: 5#设置默认索引分片个数，默认为5片。index.number_of_replicas: 1#设置默认索引副本个数，默认为1个副本。path.conf: /path/to/conf#设置配置文件的存储路径，默认是es根目录下的config文件夹。path.data: /path/to/data#设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开，例：#path.data: /path/to/data1,/path/to/data2path.work: /path/to/work#设置临时文件的存储路径，默认是es根目录下的work文件夹。path.logs: /path/to/logs#设置日志文件的存储路径，默认是es根目录下的logs文件夹path.plugins: /path/to/plugins#设置插件的存放路径，默认是es根目录下的plugins文件夹bootstrap.mlockall: true#设置为true来锁住内存。因为当jvm开始swapping时es的效率会降低，所以要保证它不swap，可以把#ES_MIN_MEM和ES_MAX_MEM两个环境变量设置成同一个值，并且保证机器有足够的内存分配给es。同时也要#允许elasticsearch的进程可以锁住内存，linux下可以通过`ulimit -l unlimited`命令。network.bind_host: 192.168.0.1#设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0。network.publish_host: 192.168.0.1#设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。network.host: 192.168.0.1#这个参数是用来同时设置bind_host和publish_host上面两个参数。transport.tcp.port: 9300#设置节点间交互的tcp端口，默认是9300。transport.tcp.compress: true#设置是否压缩tcp传输时的数据，默认为false，不压缩。http.port: 9200#设置对外服务的http端口，默认为9200。http.max_content_length: 100mb#设置内容的最大容量，默认100mbhttp.enabled: false#是否使用http协议对外提供服务，默认为true，开启。gateway.type: local#gateway的类型，默认为local即为本地文件系统，可以设置为本地文件系统，分布式文件系统，hadoop的#HDFS，和amazon的s3服务器，其它文件系统的设置方法下次再详细说。gateway.recover_after_nodes: 1#设置集群中N个节点启动时进行数据恢复，默认为1。gateway.recover_after_time: 5m#设置初始化数据恢复进程的超时时间，默认是5分钟。gateway.expected_nodes: 2#设置这个集群中节点的数量，默认为2，一旦这N个节点启动，就会立即进行数据恢复。cluster.routing.allocation.node_initial_primaries_recoveries: 4#初始化数据恢复时，并发恢复线程的个数，默认为4。cluster.routing.allocation.node_concurrent_recoveries: 2#添加删除节点或负载均衡时并发恢复线程的个数，默认为4。indices.recovery.max_size_per_sec: 0#设置数据恢复时限制的带宽，如入100mb，默认为0，即无限制。indices.recovery.concurrent_streams: 5#设置这个参数来限制从其它分片恢复数据时最大同时打开并发流的个数，默认为5。discovery.zen.minimum_master_nodes: 1#设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4）discovery.zen.ping.timeout: 3s#设置集群中自动发现其它节点时ping连接超时时间，默认为3秒，对于比较差的网络环境可以高点的值来防止自动发现时出错。discovery.zen.ping.multicast.enabled: false#设置是否打开多播发现节点，默认是true。discovery.zen.ping.unicast.hosts: [&#x27;host1&#x27;, &#x27;host2:port&#x27;, &#x27;host3[portX-portY]&#x27;]#设置集群中master节点的初始列表，可以通过这些节点来自动发现新加入集群的节点。 2. Elasticsearch FAQ2.1. elasticsearch 不允许以 root 权限来运行问题：在 Linux 环境中，elasticsearch 不允许以 root 权限来运行。 如果以 root 身份运行 elasticsearch，会提示这样的错误： 1can not run elasticsearch as root 解决方法：使用非 root 权限账号运行 elasticsearch 12345678# 创建用户组groupadd elk# 创建新用户，-g elk 设置其用户组为 elk，-p elk 设置其密码为 elkuseradd elk -g elk -p elk# 更改 /opt 文件夹及内部文件的所属用户及组为 elk:elkchown -R elk:elk /opt # 假设你的 elasticsearch 安装在 opt 目录下# 切换账号su elk 2.2. vm.max_map_count 不低于 262144问题：vm.max_map_count 表示虚拟内存大小，它是一个内核参数。elasticsearch 默认要求 vm.max_map_count 不低于 262144。 1max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决方法： 你可以执行以下命令，设置 vm.max_map_count ，但是重启后又会恢复为原值。 1sysctl -w vm.max_map_count=262144 持久性的做法是在 /etc/sysctl.conf 文件中修改 vm.max_map_count 参数： 12echo &quot;vm.max_map_count=262144&quot; &gt; /etc/sysctl.confsysctl -p 注意 如果运行环境为 docker 容器，可能会限制执行 sysctl 来修改内核参数。 这种情况下，你只能选择直接修改宿主机上的参数了。 2.3. nofile 不低于 65536问题： nofile 表示进程允许打开的最大文件数。elasticsearch 进程要求可以打开的最大文件数不低于 65536。 1max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 解决方法： 在 /etc/security/limits.conf 文件中修改 nofile 参数： 12echo &quot;* soft nofile 65536&quot; &gt; /etc/security/limits.confecho &quot;* hard nofile 131072&quot; &gt; /etc/security/limits.conf 2.4. nproc 不低于 2048问题： nproc 表示最大线程数。elasticsearch 要求最大线程数不低于 2048。 1max number of threads [1024] for user [user] is too low, increase to at least [2048] 解决方法： 在 /etc/security/limits.conf 文件中修改 nproc 参数： 12echo &quot;* soft nproc 2048&quot; &gt; /etc/security/limits.confecho &quot;* hard nproc 4096&quot; &gt; /etc/security/limits.conf","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 搜索 聚合","slug":"database/elasticsearch/elasticsearch-aggregations","date":"2022-06-08T01:20:00.000Z","updated":"2023-12-25T05:56:09.158Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-aggregations/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-aggregations/","excerpt":"","text":"Elasticsearch 聚合Elasticsearch 是一个分布式的全文搜索引擎，索引和搜索是 Elasticsearch 的基本功能。事实上，Elasticsearch 的聚合（Aggregations）功能也十分强大，允许在数据上做复杂的分析统计。Elasticsearch 提供的聚合分析功能主要有指标聚合(metrics aggregations)、桶聚合(bucket aggregations)、管道聚合(pipeline aggregations) 和 矩阵聚合(matrix aggregations) 四大类，管道聚合和矩阵聚合官方说明是在试验阶段，后期会完全更改或者移除，这里不再对管道聚合和矩阵聚合进行讲解。 1. 聚合的具体结构所有的聚合，无论它们是什么类型，都遵从以下的规则。 使用查询中同样的 JSON 请求来定义它们，而且你是使用键 aggregations 或者是 aggs 来进行标记。需要给每个聚合起一个名字，指定它的类型以及和该类型相关的选项。 它们运行在查询的结果之上。和查询不匹配的文档不会计算在内，除非你使用 global 聚集将不匹配的文档囊括其中。 可以进一步过滤查询的结果，而不影响聚集。 以下是聚合的基本结构： 12345678910&quot;aggregations&quot; : &#123; &lt;!-- 最外层的聚合键，也可以缩写为 aggs --&gt; &quot;&lt;aggregation_name&gt;&quot; : &#123; &lt;!-- 聚合的自定义名字 --&gt; &quot;&lt;aggregation_type&gt;&quot; : &#123; &lt;!-- 聚合的类型，指标相关的，如 max、min、avg、sum，桶相关的 terms、filter 等 --&gt; &lt;aggregation_body&gt; &lt;!-- 聚合体：对哪些字段进行聚合，可以取字段的值，也可以是脚本计算的结果 --&gt; &#125; [,&quot;meta&quot; : &#123; [&lt;meta_data_body&gt;] &#125; ]? &lt;!-- 元 --&gt; [,&quot;aggregations&quot; : &#123; [&lt;sub_aggregation&gt;]+ &#125; ]? &lt;!-- 在聚合里面在定义子聚合 --&gt; &#125; [,&quot;&lt;aggregation_name_2&gt;&quot; : &#123; ... &#125; ]* &lt;!-- 聚合的自定义名字 2 --&gt;&#125; 在最上层有一个 aggregations 的键，可以缩写为 aggs。 在下面一层，需要为聚合指定一个名字。可以在请求的返回中看到这个名字。在同一个请求中使用多个聚合时，这一点非常有用，它让你可以很容易地理解每组结果的含义。 最后，必须要指定聚合的类型。 关于聚合分析的值来源，可以取字段的值，也可以是脚本计算的结果。 但是用脚本计算的结果时，需要注意脚本的性能和安全性；尽管多数聚集类型允许使用脚本，但是脚本使得聚集变得缓慢，因为脚本必须在每篇文档上运行。为了避免脚本的运行，可以在索引阶段进行计算。 此外，脚本也可以被人可能利用进行恶意代码攻击，尽量使用沙盒（sandbox）内的脚本语言。 示例：查询所有球员的平均年龄是多少，并对球员的平均薪水加 188（也可以理解为每名球员加 188 后的平均薪水）。 1234567891011121314151617POST /player/_search?size=0&#123; &quot;aggs&quot;: &#123; &quot;avg_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125;, &quot;avg_salary_188&quot;: &#123; &quot;avg&quot;: &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;doc.salary.value + 188&quot; &#125; &#125; &#125; &#125;&#125; 2. 指标聚合指标聚合（又称度量聚合）主要从不同文档的分组中提取统计数据，或者，从来自其他聚合的文档桶来提取统计数据。 这些统计数据通常来自数值型字段，如最小或者平均价格。用户可以单独获取每项统计数据，或者也可以使用 stats 聚合来同时获取它们。更高级的统计数据，如平方和或者是标准差，可以通过 extended stats 聚合来获取。 2.1. Max AggregationMax Aggregation 用于最大值统计。例如，统计 sales 索引中价格最高的是哪本书，并且计算出对应的价格的 2 倍值，查询语句如下： 123456789101112131415161718GET /sales/_search?size=0&#123; &quot;aggs&quot; : &#123; &quot;max_price&quot; : &#123; &quot;max&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125;, &quot;max_price_2&quot; : &#123; &quot;max&quot; : &#123; &quot;field&quot; : &quot;price&quot;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;_value * 2.0&quot; &#125; &#125; &#125; &#125;&#125; 指定的 field，在脚本中可以用 _value 取字段的值。 聚合结果如下： 1234567891011&#123; ... &quot;aggregations&quot;: &#123; &quot;max_price&quot;: &#123; &quot;value&quot;: 188.0 &#125;, &quot;max_price_2&quot;: &#123; &quot;value&quot;: 376.0 &#125; &#125;&#125; 2.2. Min AggregationMin Aggregation 用于最小值统计。例如，统计 sales 索引中价格最低的是哪本书，查询语句如下： 12345678910GET /sales/_search?size=0&#123; &quot;aggs&quot; : &#123; &quot;min_price&quot; : &#123; &quot;min&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125;&#125; 聚合结果如下： 12345678&#123; ... &quot;aggregations&quot;: &#123; &quot;min_price&quot;: &#123; &quot;value&quot;: 18.0 &#125; &#125;&#125; 2.3. Avg AggregationAvg Aggregation 用于计算平均值。例如，统计 exams 索引中考试的平均分数，如未存在分数，默认为 60 分，查询语句如下： 1234567891011GET /exams/_search?size=0&#123; &quot;aggs&quot; : &#123; &quot;avg_grade&quot; : &#123; &quot;avg&quot; : &#123; &quot;field&quot; : &quot;grade&quot;, &quot;missing&quot;: 60 &#125; &#125; &#125;&#125; 如果指定字段没有值，可以通过 missing 指定默认值；若未指定默认值，缺失该字段值的文档将被忽略（计算）。 聚合结果如下： 12345678&#123; ... &quot;aggregations&quot;: &#123; &quot;avg_grade&quot;: &#123; &quot;value&quot;: 78.0 &#125; &#125;&#125; 除了常规的平均值聚合计算外，elasticsearch 还提供了加权平均值的聚合计算，详情参见 Elasticsearch 指标聚合之 Weighted Avg Aggregation。 2.4. Sum AggregationSum Aggregation 用于计算总和。例如，统计 sales 索引中 type 字段中匹配 hat 的价格总和，查询语句如下： 123456789101112131415GET /exams/_search?size=0&#123; &quot;query&quot; : &#123; &quot;constant_score&quot; : &#123; &quot;filter&quot; : &#123; &quot;match&quot; : &#123; &quot;type&quot; : &quot;hat&quot; &#125; &#125; &#125; &#125;, &quot;aggs&quot; : &#123; &quot;hat_prices&quot; : &#123; &quot;sum&quot; : &#123; &quot;field&quot; : &quot;price&quot; &#125; &#125; &#125;&#125; 聚合结果如下： 12345678&#123; ... &quot;aggregations&quot;: &#123; &quot;hat_prices&quot;: &#123; &quot;value&quot;: 567.0 &#125; &#125;&#125; 2.5. Value Count AggregationValue Count Aggregation 可按字段统计文档数量。例如，统计 books 索引中包含 author 字段的文档数量，查询语句如下： 12345678GET /books/_search?size=0&#123; &quot;aggs&quot; : &#123; &quot;doc_count&quot; : &#123; &quot;value_count&quot; : &#123; &quot;field&quot; : &quot;author&quot; &#125; &#125; &#125;&#125; 聚合结果如下： 12345678&#123; ... &quot;aggregations&quot;: &#123; &quot;doc_count&quot;: &#123; &quot;value&quot;: 5 &#125; &#125;&#125; 2.6. Cardinality AggregationCardinality Aggregation 用于基数统计，其作用是先执行类似 SQL 中的 distinct 操作，去掉集合中的重复项，然后统计去重后的集合长度。例如，在 books 索引中对 language 字段进行 cardinality 操作可以统计出编程语言的种类数，查询语句如下： 1234567891011GET /books/_search?size=0&#123; &quot;aggs&quot; : &#123; &quot;all_lan&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;language&quot; &#125; &#125;, &quot;title_cnt&quot; : &#123; &quot;cardinality&quot; : &#123; &quot;field&quot; : &quot;title.keyword&quot; &#125; &#125; &#125;&#125; 假设 title 字段为文本类型（text），去重时需要指定 keyword，表示把 title 作为整体去重，即不分词统计。 聚合结果如下： 1234567891011&#123; ... &quot;aggregations&quot;: &#123; &quot;all_lan&quot;: &#123; &quot;value&quot;: 8 &#125;, &quot;title_cnt&quot;: &#123; &quot;value&quot;: 18 &#125; &#125;&#125; 2.7. Stats AggregationStats Aggregation 用于基本统计，会一次返回 count、max、min、avg 和 sum 这 5 个指标。例如，在 exams 索引中对 grade 字段进行分数相关的基本统计，查询语句如下： 12345678GET /exams/_search?size=0&#123; &quot;aggs&quot; : &#123; &quot;grades_stats&quot; : &#123; &quot;stats&quot; : &#123; &quot;field&quot; : &quot;grade&quot; &#125; &#125; &#125;&#125; 聚合结果如下： 123456789101112&#123; ... &quot;aggregations&quot;: &#123; &quot;grades_stats&quot;: &#123; &quot;count&quot;: 2, &quot;min&quot;: 50.0, &quot;max&quot;: 100.0, &quot;avg&quot;: 75.0, &quot;sum&quot;: 150.0 &#125; &#125;&#125; 2.8. Extended Stats AggregationExtended Stats Aggregation 用于高级统计，和基本统计功能类似，但是会比基本统计多出以下几个统计结果，sum_of_squares（平方和）、variance（方差）、std_deviation（标准差）、std_deviation_bounds（平均值加&#x2F;减两个标准差的区间）。在 exams 索引中对 grade 字段进行分数相关的高级统计，查询语句如下： 12345678GET /exams/_search?size=0&#123; &quot;aggs&quot; : &#123; &quot;grades_stats&quot; : &#123; &quot;extended_stats&quot; : &#123; &quot;field&quot; : &quot;grade&quot; &#125; &#125; &#125;&#125; 聚合结果如下： 12345678910111213141516171819&#123; ... &quot;aggregations&quot;: &#123; &quot;grades_stats&quot;: &#123; &quot;count&quot;: 2, &quot;min&quot;: 50.0, &quot;max&quot;: 100.0, &quot;avg&quot;: 75.0, &quot;sum&quot;: 150.0, &quot;sum_of_squares&quot;: 12500.0, &quot;variance&quot;: 625.0, &quot;std_deviation&quot;: 25.0, &quot;std_deviation_bounds&quot;: &#123; &quot;upper&quot;: 125.0, &quot;lower&quot;: 25.0 &#125; &#125; &#125;&#125; 2.9. Percentiles AggregationPercentiles Aggregation 用于百分位统计。百分位数是一个统计学术语，如果将一组数据从大到小排序，并计算相应的累计百分位，某一百分位所对应数据的值就称为这一百分位的百分位数。默认情况下，累计百分位为 [ 1, 5, 25, 50, 75, 95, 99 ]。以下例子给出了在 latency 索引中对 load_time 字段进行加载时间的百分位统计，查询语句如下： 1234567891011GET latency/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot; : &#123; &quot;load_time_outlier&quot; : &#123; &quot;percentiles&quot; : &#123; &quot;field&quot; : &quot;load_time&quot; &#125; &#125; &#125;&#125; 需要注意的是，如上的 load_time 字段必须是数字类型。 聚合结果如下： 12345678910111213141516&#123; ... &quot;aggregations&quot;: &#123; &quot;load_time_outlier&quot;: &#123; &quot;values&quot; : &#123; &quot;1.0&quot;: 5.0, &quot;5.0&quot;: 25.0, &quot;25.0&quot;: 165.0, &quot;50.0&quot;: 445.0, &quot;75.0&quot;: 725.0, &quot;95.0&quot;: 945.0, &quot;99.0&quot;: 985.0 &#125; &#125; &#125;&#125; 百分位的统计也可以指定 percents 参数指定百分位，如下： 123456789101112GET latency/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot; : &#123; &quot;load_time_outlier&quot; : &#123; &quot;percentiles&quot; : &#123; &quot;field&quot; : &quot;load_time&quot;, &quot;percents&quot;: [60, 80, 95] &#125; &#125; &#125;&#125; 2.10. Percentiles Ranks AggregationPercentiles Ranks Aggregation 与 Percentiles Aggregation 统计恰恰相反，就是想看当前数值处在什么范围内（百分位）， 假如你查一下当前值 500 和 600 所处的百分位，发现是 90.01 和 100，那么说明有 90.01 % 的数值都在 500 以内，100 % 的数值在 600 以内。 123456789101112GET latency/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot; : &#123; &quot;load_time_ranks&quot; : &#123; &quot;percentile_ranks&quot; : &#123; &quot;field&quot; : &quot;load_time&quot;, &quot;values&quot; : [500, 600] &#125; &#125; &#125;&#125; 同样 load_time 字段必须是数字类型。 返回结果大概类似如下： 1234567891011&#123; ... &quot;aggregations&quot;: &#123; &quot;load_time_ranks&quot;: &#123; &quot;values&quot; : &#123; &quot;500.0&quot;: 90.01, &quot;600.0&quot;: 100.0 &#125; &#125; &#125;&#125; 可以设置 keyed 参数为 true，将对应的 values 作为桶 key 一起返回，默认是 false。 12345678910111213GET latency/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;load_time_ranks&quot;: &#123; &quot;percentile_ranks&quot;: &#123; &quot;field&quot;: &quot;load_time&quot;, &quot;values&quot;: [500, 600], &quot;keyed&quot;: true &#125; &#125; &#125;&#125; 返回结果如下： 1234567891011121314151617&#123; ... &quot;aggregations&quot;: &#123; &quot;load_time_ranks&quot;: &#123; &quot;values&quot;: [ &#123; &quot;key&quot;: 500.0, &quot;value&quot;: 90.01 &#125;, &#123; &quot;key&quot;: 600.0, &quot;value&quot;: 100.0 &#125; ] &#125; &#125;&#125; 3. 桶聚合bucket 可以理解为一个桶，它会遍历文档中的内容，凡是符合某一要求的就放入一个桶中，分桶相当于 SQL 中的 group by。从另外一个角度，可以将指标聚合看成单桶聚合，即把所有文档放到一个桶中，而桶聚合是多桶型聚合，它根据相应的条件进行分组。 种类 描述&#x2F;场景 词项聚合（Terms Aggregation） 用于分组聚合，让用户得知文档中每个词项的频率，它返回每个词项出现的次数。 差异词项聚合（Significant Terms Aggregation） 它会返回某个词项在整个索引中和在查询结果中的词频差异，这有助于我们发现搜索场景中有意义的词。 过滤器聚合（Filter Aggregation） 指定过滤器匹配的所有文档到单个桶（bucket），通常这将用于将当前聚合上下文缩小到一组特定的文档。 多过滤器聚合（Filters Aggregation） 指定多个过滤器匹配所有文档到多个桶（bucket）。 范围聚合（Range Aggregation） 范围聚合，用于反映数据的分布情况。 日期范围聚合（Date Range Aggregation） 专门用于日期类型的范围聚合。 IP 范围聚合（IP Range Aggregation） 用于对 IP 类型数据范围聚合。 直方图聚合（Histogram Aggregation） 可能是数值，或者日期型，和范围聚集类似。 时间直方图聚合（Date Histogram Aggregation） 时间直方图聚合，常用于按照日期对文档进行统计并绘制条形图。 空值聚合（Missing Aggregation） 空值聚合，可以把文档集中所有缺失字段的文档分到一个桶中。 地理点范围聚合（Geo Distance Aggregation） 用于对地理点（geo point）做范围统计。 3.1. Terms AggregationTerms Aggregation 用于词项的分组聚合。最为经典的用例是获取 X 中最频繁（top frequent）的项目，其中 X 是文档中的某个字段，如用户的名称、标签或分类。由于 terms 聚集统计的是每个词条，而不是整个字段值，因此通常需要在一个非分析型的字段上运行这种聚集。原因是, 你期望“big data”作为词组统计，而不是“big”单独统计一次，“data”再单独统计一次。 用户可以使用 terms 聚集，从分析型字段（如内容）中抽取最为频繁的词条。还可以使用这种信息来生成一个单词云。 12345678910&#123; &quot;aggs&quot;: &#123; &quot;profit_terms&quot;: &#123; &quot;terms&quot;: &#123; // terms 聚合 关键字 &quot;field&quot;: &quot;profit&quot;, ...... &#125; &#125; &#125;&#125; 在 terms 分桶的基础上，还可以对每个桶进行指标统计，也可以基于一些指标或字段值进行排序。示例如下： 123456789101112131415161718192021222324252627&#123; &quot;aggs&quot;: &#123; &quot;item_terms&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;item_id&quot;, &quot;size&quot;: 1000, &quot;order&quot;:[&#123; &quot;gmv_stat&quot;: &quot;desc&quot; &#125;,&#123; &quot;gmv_180d&quot;: &quot;desc&quot; &#125;] &#125;, &quot;aggs&quot;: &#123; &quot;gmv_stat&quot;: &#123; &quot;sum&quot;: &#123; &quot;field&quot;: &quot;gmv&quot; &#125; &#125;, &quot;gmv_180d&quot;: &#123; &quot;sum&quot;: &#123; &quot;script&quot;: &quot;doc[&#x27;gmv_90d&#x27;].value*2&quot; &#125; &#125; &#125; &#125; &#125;&#125; 返回的结果如下： 1234567891011121314151617181920212223242526272829303132&#123; ... &quot;aggregations&quot;: &#123; &quot;hospital_id_agg&quot;: &#123; &quot;doc_count_error_upper_bound&quot;: 0, &quot;sum_other_doc_count&quot;: 260, &quot;buckets&quot;: [ &#123; &quot;key&quot;: 23388, &quot;doc_count&quot;: 18, &quot;gmv_stat&quot;: &#123; &quot;value&quot;: 176220 &#125;, &quot;gmv_180d&quot;: &#123; &quot;value&quot;: 89732 &#125; &#125;, &#123; &quot;key&quot;: 96117, &quot;doc_count&quot;: 16, &quot;gmv_stat&quot;: &#123; &quot;value&quot;: 129306 &#125;, &quot;gmv_180d&quot;: &#123; &quot;value&quot;: 56988 &#125; &#125;, ... ] &#125; &#125;&#125; 默认情况下返回按文档计数从高到低的前 10 个分组，可以通过 size 参数指定返回的分组数。 3.2. Filter AggregationFilter Aggregation 是过滤器聚合，可以把符合过滤器中的条件的文档分到一个桶中，即是单分组聚合。 1234567891011121314&#123; &quot;aggs&quot;: &#123; &quot;age_terms&quot;: &#123; &quot;filter&quot;: &#123;&quot;match&quot;:&#123;&quot;gender&quot;:&quot;F&quot;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;avg_age&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;age&quot; &#125; &#125; &#125; &#125; &#125;&#125; 3.3. Filters AggregationFilters Aggregation 是多过滤器聚合，可以把符合多个过滤条件的文档分到不同的桶中，即每个分组关联一个过滤条件，并收集所有满足自身过滤条件的文档。 12345678910111213&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;messages&quot;: &#123; &quot;filters&quot;: &#123; &quot;filters&quot;: &#123; &quot;errors&quot;: &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;error&quot; &#125; &#125;, &quot;warnings&quot;: &#123; &quot;match&quot;: &#123; &quot;body&quot;: &quot;warning&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 在这个例子里，我们分析日志信息。聚合会创建两个关于日志数据的分组，一个收集包含错误信息的文档，另一个收集包含告警信息的文档。而且每个分组会按月份划分。 123456789101112131415&#123; ... &quot;aggregations&quot;: &#123; &quot;messages&quot;: &#123; &quot;buckets&quot;: &#123; &quot;errors&quot;: &#123; &quot;doc_count&quot;: 1 &#125;, &quot;warnings&quot;: &#123; &quot;doc_count&quot;: 2 &#125; &#125; &#125; &#125;&#125; 3.4. Range AggregationRange Aggregation 范围聚合是一个基于多组值来源的聚合，可以让用户定义一系列范围，每个范围代表一个分组。在聚合执行的过程中，从每个文档提取出来的值都会检查每个分组的范围，并且使相关的文档落入分组中。注意，范围聚合的每个范围内包含 from 值但是排除 to 值。 123456789101112131415161718192021222324252627&#123; &quot;aggs&quot;: &#123; &quot;age_range&quot;: &#123; &quot;range&quot;: &#123; &quot;field&quot;: &quot;age&quot;, &quot;ranges&quot;: [&#123; &quot;to&quot;: 25 &#125;, &#123; &quot;from&quot;: 25, &quot;to&quot;: 35 &#125;, &#123; &quot;from&quot;: 35 &#125;] &#125;, &quot;aggs&quot;: &#123; &quot;bmax&quot;: &#123; &quot;max&quot;: &#123; &quot;field&quot;: &quot;balance&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 返回结果如下： 1234567891011121314151617181920212223242526272829303132&#123; ... &quot;aggregations&quot;: &#123; &quot;age_range&quot;: &#123; &quot;buckets&quot;: [&#123; &quot;key&quot;: &quot;*-25.0&quot;, &quot;to&quot;: 25, &quot;doc_count&quot;: 225, &quot;bmax&quot;: &#123; &quot;value&quot;: 49587 &#125; &#125;, &#123; &quot;key&quot;: &quot;25.0-35.0&quot;, &quot;from&quot;: 25, &quot;to&quot;: 35, &quot;doc_count&quot;: 485, &quot;bmax&quot;: &#123; &quot;value&quot;: 49795 &#125; &#125;, &#123; &quot;key&quot;: &quot;35.0-*&quot;, &quot;from&quot;: 35, &quot;doc_count&quot;: 290, &quot;bmax&quot;: &#123; &quot;value&quot;: 49989 &#125; &#125;] &#125; &#125;&#125;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 搜索 排序","slug":"database/elasticsearch/elasticsearch-sort","date":"2022-06-08T01:15:00.000Z","updated":"2023-12-25T05:56:09.161Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-sort/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-sort/","excerpt":"","text":"Elasticsearch 排序在 Elasticsearch 中，默认排序是按照相关性的评分（_score）进行降序排序，也可以按照字段的值排序、多级排序、多值字段排序、基于 geo（地理位置）排序以及自定义脚本排序，除此之外，对于相关性的评分也可以用 rescore 二次、三次打分，它可以限定重新打分的窗口大小（window size），并针对作用范围内的文档修改其得分，从而达到精细化控制结果相关性的目的。 1. 默认相关性排序在 Elasticsearch 中，默认情况下，文档是按照相关性得分倒序排列的，其对应的相关性得分字段用 _score 来表示，它是浮点数类型，_score 评分越高，相关性越高。评分模型的选择可以通过 similarity 参数在映射中指定。 相似度算法可以按字段指定，只需在映射中为不同字段选定即可，如果要修改已有字段的相似度算法，只能通过为数据重新建立索引来达到目的。 1.1. TF-IDF 模型Elasticsearch 在 5.4 版本以前，text 类型的字段，默认采用基于 tf-idf 的向量空间模型。 在开始计算得分之时，Elasticsearch 使用了被搜索词条的频率以及它有多常见来影响得分。一个简短的解释是，一个词条出现在某个文档中的次数越多，它就越相关；但是，如果该词条出现在不同的文档的次数越多，它就越不相关。这一点被称为 TF-IDF，TF 是词频（term frequency），IDF 是逆文档频率（inverse document frequency）。 考虑给一篇文档打分的首要方式，是统计一个词条在文本中出现的次数。举个例子，如果在用户的区域搜索关于 Elasticsearch 的 get-together，用户希望频繁提及 Elasticsearch 的分组被优先展示出来。 12&quot;We will discuss Elasticsearch at the next Big Data group.&quot;&quot;Tuesday the Elasticsearch team will gather to answer questions about Elasticsearch.&quot; 第一个句子提到 Elasticsearch 一次，而第二个句子提到 Elasticsearch 两次，所以包含第二句话的文档应该比包含第一句话的文档拥有更高的得分。如果我们要按照数量来讨论，第一句话的词频（TF）是 1，而第二句话的词频将是 2。 逆文档频率比文档词频稍微复杂一点。这个听上去很酷炫的描述意味着，如果一个分词（通常是单词，但不一定是）在索引的不同文档中出现越多的次数，那么它就越不重要。使用如下例子更容易解释这一点。 123&quot;We use Elasticsearch to power the search for our website.&quot;&quot;The developers like Elasticsearch so far.&quot;&quot;The scoring of documents is calculated by the scoring formula.&quot; 如上述例子，需要理解以下几点： 词条 “Elasticsearch” 的文档频率是 2（因为它出现在两篇文档中）。文档频率的逆源自得分乘以 1&#x2F;DF，这里 DF 是该词条的文档频率。这就意味着，由于词条拥有更高的文档频率，它的权重就会降低。 词条 “the” 的文档频率是 3，因为它出现在所有的三篇文档中。请注意，尽管 “the” 在最后一篇文档中出现了两次，它的文档频率还是 3。这是因为，逆文档频率只检查一个词条是否出现在某文档中，而不检查它出现多少次。那个应该是词频所关心的事情。 逆文档频率是一个重要的因素，用于平衡词条的词频。举个例子，考虑有一个用户搜索词条 “the score”，单词 the 几乎出现在每个普通的英语文本中，如果它不被均衡一下，单词 the 的频率要完全淹没单词 score 的频率。逆文档频率 IDF 均衡了 the 这种常见词的相关性影响，所以实际的相关性得分将会对查询的词条有一个更准确的描述。 一旦词频 TF 和逆文档频率 IDF 计算完成，就可以使用 TF-IDF 公式来计算文档的得分。 1.2. BM25 模型Elasticsearch 在 5.4 版本之后，针对 text 类型的字段，默认采用的是 BM25 评分模型，而不是基于 tf-idf 的向量空间模型，评分模型的选择可以通过 similarity 参数在映射中指定。 2. 字段的值排序在 Elasticsearch 中按照字段的值排序，可以利用 sort 参数实现。 12345678GET books/_search&#123; &quot;sort&quot;: &#123; &quot;price&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;&#125; 返回结果如下： 12345678910111213141516171819202122232425262728293031&#123; &quot;took&quot;: 132, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 10, &quot;successful&quot;: 10, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 749244, &quot;max_score&quot;: null, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;books&quot;, &quot;_type&quot;: &quot;book&quot;, &quot;_id&quot;: &quot;8456479&quot;, &quot;_score&quot;: null, &quot;_source&quot;: &#123; &quot;id&quot;: 8456479, &quot;price&quot;: 1580.00, ... &#125;, &quot;sort&quot;: [ 1580.00 ] &#125;, ... ] &#125;&#125; 从如上返回结果，可以看出，max_score 和 _score 字段都返回 null，返回字段多出 sort 字段，包含排序字段的分值。计算 _score 的花销巨大，如果不根据相关性排序，记录 _score 是没有意义的。如果无论如何都要计算 _score，可以将 track_scores 参数设置为 true。 3. 多字段排序如果我们想要结合使用 price、date 和 _score 进行查询，并且匹配的结果首先按照价格排序，然后按照日期排序，最后按照相关性排序，具体示例如下： 123456789101112131415161718192021222324252627GET books/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;java&quot; &#125; &#125;, &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;user_id&quot;: 4868438 &#125; &#125; &#125; &#125;, &quot;sort&quot;: [&#123; &quot;price&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;, &#123; &quot;date&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;, &#123; &quot;_score&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125; 排序条件的顺序是很重要的。结果首先按第一个条件排序，仅当结果集的第一个 sort 值完全相同时才会按照第二个条件进行排序，以此类推。 多级排序并不一定包含 _score。你可以根据一些不同的字段进行排序，如地理距离或是脚本计算的特定值。 4. 多值字段的排序一种情形是字段有多个值的排序，需要记住这些值并没有固有的顺序；一个多值的字段仅仅是多个值的包装，这时应该选择哪个进行排序呢？ 对于数字或日期，你可以将多值字段减为单值，这可以通过使用 min、max、avg 或是 sum 排序模式。例如你可以按照每个 date 字段中的最早日期进行排序，通过以下方法： 123456&quot;sort&quot;: &#123; &quot;dates&quot;: &#123; &quot;order&quot;: &quot;asc&quot;, &quot;mode&quot;: &quot;min&quot; &#125;&#125; 5. 地理位置上的距离排序es 的地理位置排序使用 _geo_distance 来进行距离排序，如下示例： 12345678910111213141516&#123; &quot;sort&quot; : [ &#123; &quot;_geo_distance&quot; : &#123; &quot;es_location_field&quot; : [116.407526, 39.904030], &quot;order&quot; : &quot;asc&quot;, &quot;unit&quot; : &quot;km&quot;, &quot;mode&quot; : &quot;min&quot;, &quot;distance_type&quot; : &quot;plane&quot; &#125; &#125; ], &quot;query&quot; : &#123; ...... &#125;&#125; _geo_distance 的选项具体如下： 如上的 es_location_field 指的是 es 存储经纬度数据的字段名。 **_order**：指定按距离升序或降序，分别对应 **asc** 和 **desc_**。 **_unit**：计算距离值的单位，默认是 **m**，表示米（meters），其它可选项有 **mi、cm、mm、NM、km、ft、yd** 和 **in_**。 **_mode**：针对数组数据（多个值）时，指定的取值模式，可选值有 **min、max、sum、avg** 和 **median_**，当排序采用升序时，默认为 _min_；排序采用降序时，默认为 _max_。 **_distance_type**：用来设置如何计算距离，它的可选项有 **sloppy_arc、arc** 和 **plane**，默认为 sloppy_arc_，_arc 它相对更精确些，但速度会明显下降，_plane 则是计算快，但是长距离计算相对不准确。 **_ignore_unmapped**：未映射字段时，是否忽略处理，可选项有 **true** 和 **false_**；默认为 _false_，表示如果未映射字段，查询将引发异常；若设置 _true_，将忽略未映射的字段，并且不匹配此查询的任何文档。 **_validation_method**：指定检验经纬度数据的方式，可选项有 **IGNORE_MALFORMED、COERCE** 和 **STRICT**；_IGNORE_MALFORMED 表示可接受纬度或经度无效的地理点，即忽略数据；COERCE 表示另外尝试并推断正确的地理坐标；STRICT 为默认值，表示遇到不正确的地理坐标直接抛出异常。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 搜索 高亮","slug":"database/elasticsearch/elasticsearch-highlight","date":"2022-06-08T01:10:00.000Z","updated":"2023-12-25T05:56:09.159Z","comments":true,"path":"2022/06/08/database/elasticsearch/elasticsearch-highlight/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elasticsearch-highlight/","excerpt":"","text":"Elasticsearch 高亮搜索及显示Elasticsearch 的高亮（highlight）可以让您从搜索结果中的一个或多个字段中获取突出显示的摘要，以便向用户显示查询匹配的位置。当您请求突出显示（即高亮）时，响应结果的 highlight 字段中包括高亮的字段和高亮的片段。Elasticsearch 默认会用 &lt;em&gt;&lt;/em&gt; 标签标记关键字。 1. 高亮参数ES 提供了如下高亮参数： 参数 说明 boundary_chars 包含每个边界字符的字符串。默认为,! ?\\ \\ n。 boundary_max_scan 扫描边界字符的距离。默认为 20。 boundary_scanner 指定如何分割突出显示的片段，支持 chars、sentence、word 三种方式。 boundary_scanner_locale 用来设置搜索和确定单词边界的本地化设置，此参数使用语言标记的形式（“en-US”, “fr-FR”, “ja-JP”） encoder 表示代码段应该是 HTML 编码的:默认(无编码)还是 HTML (HTML-转义代码段文本，然后插入高亮标记) fields 指定检索高亮显示的字段。可以使用通配符来指定字段。例如，可以指定 comment*来获取以 comment开头的所有文本和关键字字段的高亮显示。 force_source 根据源高亮显示。默认值为 false。 fragmenter 指定文本应如何在突出显示片段中拆分:支持参数 simple 或者 span。 fragment_offset 控制要开始突出显示的空白。仅在使用 fvh highlighter 时有效。 fragment_size 字符中突出显示的片段的大小。默认为 100。 highlight_query 突出显示搜索查询之外的其他查询的匹配项。这在使用重打分查询时特别有用，因为默认情况下高亮显示不会考虑这些问题。 matched_fields 组合多个匹配结果以突出显示单个字段，对于使用不同方式分析同一字符串的多字段。所有的 matched_fields 必须将 term_vector 设置为 with_positions_offsets，但是只有将匹配项组合到的字段才会被加载，因此只有将 store 设置为 yes 才能使该字段受益。只适用于 fvh highlighter。 no_match_size 如果没有要突出显示的匹配片段，则希望从字段开头返回的文本量。默认为 0(不返回任何内容)。 number_of_fragments 返回的片段的最大数量。如果片段的数量设置为 0，则不会返回任何片段。相反，突出显示并返回整个字段内容。当需要突出显示短文本(如标题或地址)，但不需要分段时，使用此配置非常方便。如果 number_of_fragments 为 0，则忽略 fragment_size。默认为 5。 order 设置为 score 时，按分数对突出显示的片段进行排序。默认情况下，片段将按照它们在字段中出现的顺序输出(order:none)。将此选项设置为 score 将首先输出最相关的片段。每个高亮应用自己的逻辑来计算相关性得分。 phrase_limit 控制文档中所考虑的匹配短语的数量。防止 fvh highlighter 分析太多的短语和消耗太多的内存。提高限制会增加查询时间并消耗更多内存。默认为 256。 pre_tags 与 post_tags 一起使用，定义用于突出显示文本的 HTML 标记。默认情况下，突出显示的文本被包装在和标记中。指定为字符串数组。 post_tags 与 pre_tags 一起使用，定义用于突出显示文本的 HTML 标记。默认情况下，突出显示的文本被包装在和标记中。指定为字符串数组。 require_field_match 默认情况下，只突出显示包含查询匹配的字段。将 require_field_match 设置为 false 以突出显示所有字段。默认值为 true。 tags_schema 设置为使用内置标记模式的样式。 type 使用的高亮模式，可选项为**_unified、plain或fvh_**。默认为 _unified_。 2. 自定义高亮片段如果我们想使用自定义标签，在高亮属性中给需要高亮的字段加上 pre_tags 和 post_tags 即可。例如，搜索 title 字段中包含关键词 javascript 的书籍并使用自定义 HTML 标签高亮关键词，查询语句如下： 1234567891011121314GET /books/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;javascript&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;title&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;strong&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/strong&gt;&quot;] &#125; &#125; &#125;&#125; 3. 多字段高亮关于搜索高亮，还需要掌握如何设置多字段搜索高亮。比如，搜索 title 字段的时候，我们期望 description 字段中的关键字也可以高亮，这时候就需要把 require_field_match 属性的取值设置为 fasle。require_field_match 的默认值为 true，只会高亮匹配的字段。多字段高亮的查询语句如下： 12345678910111213GET /books/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;javascript&quot; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;require_field_match&quot;: false, &quot;fields&quot;: &#123; &quot;title&quot;: &#123;&#125;, &quot;description&quot;: &#123;&#125; &#125; &#125;&#125; 4. 高亮性能分析Elasticsearch 提供了三种高亮器，分别是默认的 highlighter 高亮器、postings-highlighter 高亮器和 fast-vector-highlighter 高亮器。 默认的 highlighter 是最基本的高亮器。highlighter 高亮器实现高亮功能需要对 _source 中保存的原始文档进行二次分析，其速度在三种高亮器里最慢，优点是不需要额外的存储空间。 postings-highlighter 高亮器实现高亮功能不需要二次分析，但是需要在字段的映射中设置 index_options 参数的取值为 offsets，即保存关键词的偏移量，速度快于默认的 highlighter 高亮器。例如，配置 comment 字段使用 postings-highlighter 高亮器，映射如下： 12345678910111213PUT /example&#123; &quot;mappings&quot;: &#123; &quot;doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;comment&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index_options&quot;: &quot;offsets&quot; &#125; &#125; &#125; &#125;&#125; fast-vector-highlighter 高亮器实现高亮功能速度最快，但是需要在字段的映射中设置 term_vector 参数的取值为 with_positions_offsets，即保存关键词的位置和偏移信息，占用的存储空间最大，是典型的空间换时间的做法。例如，配置 comment 字段使用 fast-vector-highlighter 高亮器，映射如下： 12345678910111213PUT /example&#123; &quot;mappings&quot;: &#123; &quot;doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;comment&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets&quot; &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"ElasticSearch 索引","slug":"database/elasticsearch/elacticsearch-index","date":"2022-06-08T01:00:00.000Z","updated":"2023-12-25T05:56:09.158Z","comments":true,"path":"2022/06/08/database/elasticsearch/elacticsearch-index/","link":"","permalink":"https://blog.ifan.host/2022/06/08/database/elasticsearch/elacticsearch-index/","excerpt":"","text":"1. 索引管理Elasticsearch 索引管理主要包括如何进行索引的创建、索引的删除、副本的更新、索引读写权限、索引别名的配置等等内容。 1.1. 索引删除ES 索引删除操作向 ES 集群的 http 接口发送指定索引的 delete http 请求即可，可以通过 curl 命令，具体如下： 1curl -X DELETE http://&#123;host&#125;:&#123;port&#125;/&#123;index&#125; 如果删除成功，它会返回如下信息，具体示例如下： 1curl -X DELETE http://127.0.0.1:9200/my_index?pretty 为了返回的信息便于读取，增加了 pretty 参数： 123&#123; &quot;acknowledged&quot; : true&#125; 1.2. 索引别名ES 的索引别名就是给一个索引或者多个索引起的另一个名字，典型的应用场景是针对索引使用的平滑切换。 首先，创建索引 my_index，然后将别名 my_alias 指向它，示例如下： 12PUT /my_indexPUT /my_index/_alias/my_alias 也可以通过如下形式： 123456POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;my_index&quot;, &quot;alias&quot;: &quot;my_alias&quot; &#125;&#125; ]&#125; 也可以在一次请求中增加别名和移除别名混合使用： 1234567POST /_aliases&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;my_index&quot;, &quot;alias&quot;: &quot;my_alias&quot; &#125;&#125; &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;my_index_v2&quot;, &quot;alias&quot;: &quot;my_alias&quot; &#125;&#125; ]&#125; 需要注意的是，如果别名与索引是一对一的，使用别名索引文档或者查询文档是可以的，但是如果别名和索引是一对多的，使用别名会发生错误，因为 ES 不知道把文档写入哪个索引中去或者从哪个索引中读取文档。 2. SettingsElasticsearch 索引的配置项主要分为静态配置属性和动态配置属性，静态配置属性是索引创建后不能修改，而动态配置属性则可以随时修改。 ES 索引设置的 api 为 **__settings_**，完整的示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;1&quot;, &quot;number_of_replicas&quot;: &quot;1&quot;, &quot;refresh_interval&quot;: &quot;60s&quot;, &quot;analysis&quot;: &#123; &quot;filter&quot;: &#123; &quot;tsconvert&quot;: &#123; &quot;type&quot;: &quot;stconvert&quot;, &quot;convert_type&quot;: &quot;t2s&quot;, &quot;delimiter&quot;: &quot;,&quot; &#125;, &quot;synonym&quot;: &#123; &quot;type&quot;: &quot;synonym&quot;, &quot;synonyms_path&quot;: &quot;analysis/synonyms.txt&quot; &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;ik_max_word_synonym&quot;: &#123; &quot;filter&quot;: [ &quot;synonym&quot;, &quot;tsconvert&quot;, &quot;standard&quot;, &quot;lowercase&quot;, &quot;stop&quot; ], &quot;tokenizer&quot;: &quot;ik_max_word&quot; &#125;, &quot;ik_smart_synonym&quot;: &#123; &quot;filter&quot;: [ &quot;synonym&quot;, &quot;standard&quot;, &quot;lowercase&quot;, &quot;stop&quot; ], &quot;tokenizer&quot;: &quot;ik_smart&quot; &#125; &#125;, &quot;mapping&quot;: &#123; &quot;coerce&quot;: &quot;false&quot;, &quot;ignore_malformed&quot;: &quot;false&quot; &#125;, &quot;indexing&quot;: &#123; &quot;slowlog&quot;: &#123; &quot;threshold&quot;: &#123; &quot;index&quot;: &#123; &quot;warn&quot;: &quot;2s&quot;, &quot;info&quot;: &quot;1s&quot; &#125; &#125; &#125; &#125;, &quot;provided_name&quot;: &quot;hospital_202101070533&quot;, &quot;query&quot;: &#123; &quot;default_field&quot;: &quot;timestamp&quot;, &quot;parse&quot;: &#123; &quot;allow_unmapped_fields&quot;: &quot;false&quot; &#125; &#125;, &quot;requests&quot;: &#123; &quot;cache&quot;: &#123; &quot;enable&quot;: &quot;true&quot; &#125; &#125;, &quot;search&quot;: &#123; &quot;slowlog&quot;: &#123; &quot;threshold&quot;: &#123; &quot;fetch&quot;: &#123; &quot;warn&quot;: &quot;1s&quot;, &quot;info&quot;: &quot;200ms&quot; &#125;, &quot;query&quot;: &#123; &quot;warn&quot;: &quot;1s&quot;, &quot;info&quot;: &quot;500ms&quot; &#125; &#125; &#125; &#125; &#125; &#125;&#125; 2.1. 固定属性 **_index.creation_date_**：顾名思义索引的创建时间戳。 **_index.uuid_**：索引的 uuid 信息。 **_index.version.created_**：索引的版本号。 2.2. 索引静态配置 **_index.number_of_shards**：索引的主分片数，默认值是 **5**。这个配置在索引创建后不能修改；在 es 层面，可以通过 **es.index.max_number_of_shards** 属性设置索引最大的分片数，默认为 **1024_**。 **_index.codec**：数据存储的压缩算法，默认值为 **LZ4**，可选择值还有 **best_compression_**，它比 LZ4 可以获得更好的压缩比（即占据较小的磁盘空间，但存储性能比 LZ4 低）。 **_index.routing_partition_size_**：路由分区数，如果设置了该参数，其路由算法为：( hash(_routing) + hash(_id) % index.routing_parttion_size ) % number_of_shards。如果该值不设置，则路由算法为 hash(_routing) % number_of_shardings，_routing 默认值为 _id。 静态配置里，有重要的部分是配置分析器（config analyzers）。 index.analysis ：分析器最外层的配置项，内部主要分为 char_filter、tokenizer、filter 和 analyzer。 **_char_filter_**：定义新的字符过滤器件。 **_tokenizer_**：定义新的分词器。 **_filter_**：定义新的 token filter，如同义词 filter。 **_analyzer_**：配置新的分析器，一般是 char_filter、tokenizer 和一些 token filter 的组合。 2.3. 索引动态配置 **_index.number_of_replicas**：索引主分片的副本数，默认值是 **1_**，该值必须大于等于 0，这个配置可以随时修改。 **_index.refresh_interval**：执行新索引数据的刷新操作频率，该操作使对索引的最新更改对搜索可见，默认为 **1s**。也可以设置为 **-1_** 以禁用刷新。 3. Mapping 详解在 Elasticsearch 中，**Mapping**（映射），用来定义一个文档以及其所包含的字段如何被存储和索引，可以在映射中事先定义字段的数据类型、字段的权重、分词器等属性，就如同在关系型数据库中创建数据表时会设置字段的类型。 Mapping 会把 json 文档映射成 Lucene 所需要的扁平格式 一个 Mapping 属于一个索引的 Type 每个文档都属于一个 Type 一个 Type 有一个 Mapping 定义 7.0 开始，不需要在 Mapping 定义中指定 type 信息 3.1. 映射分类在 Elasticsearch 中，映射可分为静态映射和动态映射。在关系型数据库中写入数据之前首先要建表，在建表语句中声明字段的属性，在 Elasticsearch 中，则不必如此，Elasticsearch 最重要的功能之一就是让你尽可能快地开始探索数据，文档写入 Elasticsearch 中，它会根据字段的类型自动识别，这种机制称为动态映射，而静态映射则是写入数据之前对字段的属性进行手工设置。 静态映射静态映射是在创建索引时手工指定索引映射。静态映射和 SQL 中在建表语句中指定字段属性类似。相比动态映射，通过静态映射可以添加更详细、更精准的配置信息。 如何定义一个 Mapping 12345678PUT /books&#123; &quot;mappings&quot;: &#123; &quot;type_one&quot;: &#123; ... any mappings ... &#125;, &quot;type_two&quot;: &#123; ... any mappings ... &#125;, ... &#125;&#125; 动态映射动态映射是一种偷懒的方式，可直接创建索引并写入文档，文档中字段的类型是 Elasticsearch 自动识别的，不需要在创建索引的时候设置字段的类型。在实际项目中，如果遇到的业务在导入数据之前不确定有哪些字段，也不清楚字段的类型是什么，使用动态映射非常合适。当 Elasticsearch 在文档中碰到一个以前没见过的字段时，它会利用动态映射来决定该字段的类型，并自动把该字段添加到映射中，根据字段的取值自动推测字段类型的规则见下表： JSON 格式的数据 自动推测的字段类型 null 没有字段被添加 true or false boolean 类型 浮点类型数字 float 类型 数字 long 类型 JSON 对象 object 类型 数组 由数组中第一个非空值决定 string 有可能是 date 类型（若开启日期检测）、double 或 long 类型、text 类型、keyword 类型 下面举一个例子认识动态 mapping，在 Elasticsearch 中创建一个新的索引并查看它的 mapping，命令如下： 12PUT booksGET books/_mapping 此时 books 索引的 mapping 是空的，返回结果如下： 12345&#123; &quot;books&quot;: &#123; &quot;mappings&quot;: &#123;&#125; &#125;&#125; 再往 books 索引中写入一条文档，命令如下： 123456PUT books/it/1&#123; &quot;id&quot;: 1, &quot;publish_date&quot;: &quot;2019-11-10&quot;, &quot;name&quot;: &quot;master Elasticsearch&quot;&#125; 文档写入完成之后，再次查看 mapping，返回结果如下： 1234567891011121314151617181920212223&#123; &quot;books&quot;: &#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;publish_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125; &#125;&#125; 使用动态 mapping 要结合实际业务需求来综合考虑，如果将 Elasticsearch 当作主要的数据存储使用，并且希望出现未知字段时抛出异常来提醒你注意这一问题，那么开启动态 mapping 并不适用。在 mapping 中可以通过 dynamic 设置来控制是否自动新增字段，接受以下参数： **true**：默认值为 true，自动添加字段。 **false**：忽略新的字段。 **strict**：严格模式，发现新的字段抛出异常。 3.2. 基础类型 类型 关键字 字符串类型 string、text、keyword 数字类型 long、integer、short、byte、double、float、half_float、scaled_float 日期类型 date 布尔类型 boolean 二进制类型 binary 范围类型 range 3.3. 复杂类型 类型 关键字 数组类型 array 对象类型 object 嵌套类型 nested 3.4. 特殊类型 类型 关键字 地理类型 geo_point 地理图形类型 geo_shape IP 类型 ip 范围类型 completion 令牌计数类型 token_count 附件类型 attachment 抽取类型 percolator 3.5. Mapping 属性Elasticsearch 的 mapping 中的字段属性非常多，具体如下表格： 属性名 描述 type 字段类型，常用的有 text、integer 等等。 index 当前字段是否被作为索引。可选值为 **_true_**，默认为 true。 store 是否存储指定字段，可选值为 true norms 是否使用归一化因子，可选值为 true index_options 索引选项控制添加到倒排索引（Inverted Index）的信息，这些信息用于搜索（Search）和高亮显示：**_docs：只索引文档编号(Doc Number)；freqs：索引文档编号和词频率（term frequency）；positions：索引文档编号，词频率和词位置（序号）；offsets_**：索引文档编号，词频率，词偏移量（开始和结束位置）和词位置（序号）。默认情况下，被分析的字符串（analyzed string）字段使用 _positions_，其他字段默认使用 docs_。此外，需要注意的是 index_option 是 elasticsearch 特有的设置属性；临近搜索和短语查询时，_index_option 必须设置为 _offsets_，同时高亮也可使用 postings highlighter。 term_vector 索引选项控制词向量相关信息：**_no：默认值，表示不存储词向量相关信息；yes：只存储词向量信息；with_positions：存储词项和词项位置；with_offsets：存储词项和字符偏移位置；with_positions_offsets**：存储词项、词项位置、字符偏移位置。_term_vector 是 lucene 层面的索引设置。 similarity 指定文档相似度算法（也可以叫评分模型）：**_BM25_**：ES 5 之后的默认设置。 copy_to 复制到自定义 _all 字段，值是数组形式，即表明可以指定多个自定义的字段。 analyzer 指定索引和搜索时的分析器，如果同时指定 search_analyzer 则搜索时会优先使用 _search_analyzer_。 search_analyzer 指定搜索时的分析器，搜索时的优先级最高。 null_value 用于需要对 Null 值实现搜索的场景，只有 Keyword 类型支持此配置。 4. 索引查询4.1. 多个 index、多个 type 查询Elasticsearch 的搜索 api 支持一个索引（index）的多个类型（type）查询以及多个索引（index）的查询。 例如，我们可以搜索 twitter 索引下面所有匹配条件的所有类型中文档，如下： 1GET /twitter/_search?q=user:shay 我们也可以搜索一个索引下面指定多个 type 下匹配条件的文档，如下： 1GET /twitter/tweet,user/_search?q=user:banon 我们也可以搜索多个索引下匹配条件的文档，如下： 1GET /twitter,elasticsearch/_search?q=tag:wow 此外我们也可以搜索所有索引下匹配条件的文档，用_all 表示所有索引，如下： 1GET /_all/_search?q=tag:wow 甚至我们可以搜索所有索引及所有 type 下匹配条件的文档，如下： 1GET /_search?q=tag:wow 4.2. URI 搜索Elasticsearch 支持用 uri 搜索，可用 get 请求里面拼接相关的参数，并用 curl 相关的命令就可以进行测试。 如下有一个示例： 1GET twitter/_search?q=user:kimchy 如下是上一个请求的相应实体： 12345678910111213141516171819202122232425262728&#123; &quot;timed_out&quot;: false, &quot;took&quot;: 62, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 1, &quot;max_score&quot;: 1.3862944, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;twitter&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;0&quot;, &quot;_score&quot;: 1.3862944, &quot;_source&quot;: &#123; &quot;user&quot;: &quot;kimchy&quot;, &quot;date&quot;: &quot;2009-11-15T14:12:12&quot;, &quot;message&quot;: &quot;trying out Elasticsearch&quot;, &quot;likes&quot;: 0 &#125; &#125; ] &#125;&#125; URI 中允许的参数： 名称 描述 q 查询字符串，映射到 query_string 查询 df 在查询中未定义字段前缀时使用的默认字段 analyzer 查询字符串时指定的分词器 analyze_wildcard 是否允许通配符和前缀查询，默认设置为 false batched_reduce_size 应在协调节点上一次减少的分片结果数。如果请求中潜在的分片数量很大，则应将此值用作保护机制，以减少每个搜索请求的内存开销 default_operator 默认使用的匹配运算符，可以是AND或者OR，默认是OR lenient 如果设置为 true，将会忽略由于格式化引起的问题（如向数据字段提供文本），默认为 false explain 对于每个 hit，包含了具体如何计算得分的解释 _source 请求文档内容的参数，默认 true；设置 false 的话，不返回_source 字段，可以使用_source_include和_source_exclude参数分别指定返回字段和不返回的字段 stored_fields 指定每个匹配返回的文档中的存储字段，多个用逗号分隔。不指定任何值将导致没有字段返回 sort 排序方式，可以是fieldName、fieldName:asc或者fieldName:desc的形式。fieldName 可以是文档中的实际字段，也可以是诸如_score 字段，其表示基于分数的排序。此外可以指定多个 sort 参数（顺序很重要） track_scores 当排序时，若设置 true，返回每个命中文档的分数 track_total_hits 是否返回匹配条件命中的总文档数，默认为 true timeout 设置搜索的超时时间，默认无超时时间 terminate_after 在达到查询终止条件之前，指定每个分片收集的最大文档数。如果设置，则在响应中多了一个 terminated_early 的布尔字段，以指示查询执行是否实际上已终止。默认为 no terminate_after from 从第几条（索引以 0 开始）结果开始返回，默认为 0 size 返回命中的文档数，默认为 10 search_type 搜索的方式，可以是dfs_query_then_fetch或query_then_fetch。默认为query_then_fetch allow_partial_search_results 是否可以返回部分结果。如设置为 false，表示如果请求产生部分结果，则设置为返回整体故障；默认为 true，表示允许请求在超时或部分失败的情况下获得部分结果 4.3. 查询流程在 Elasticsearch 中，查询是一个比较复杂的执行模式，因为我们不知道那些 document 会被匹配到，任何一个 shard 上都有可能，所以一个 search 请求必须查询一个索引或多个索引里面的所有 shard 才能完整的查询到我们想要的结果。 找到所有匹配的结果是查询的第一步，来自多个 shard 上的数据集在分页返回到客户端之前会被合并到一个排序后的 list 列表，由于需要经过一步取 top N 的操作，所以 search 需要进过两个阶段才能完成，分别是 query 和 fetch。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"}]},{"title":"Java Spring Boot 注解 profile 区分环境","slug":"language/java/framework/springboot/profile","date":"2022-06-08T01:00:00.000Z","updated":"2023-12-25T05:56:09.174Z","comments":true,"path":"2022/06/08/language/java/framework/springboot/profile/","link":"","permalink":"https://blog.ifan.host/2022/06/08/language/java/framework/springboot/profile/","excerpt":"","text":"配置文件约定 application.yaml 公共配置 application-dev.yaml 开发环境配置 application-test.yaml 测试环境配置 application-prod.yaml 生产环境配置 可以在 application.yaml 配置文件中激活 123spring: profiles: activate: prod 也可以在一个 yaml 文件中完成所有的 profile 配置 123456789101112131415spring: profiles: activate: - prod - dev--- # dev 配置spring: profiles: dev# 简略配置---# dev 配置spring: profiles: test# 简略配置 在代码中区分环境修饰类12345678910@Configuration@Profile(&quot;production&quot;)public class JndiDataConfig &#123; @Bean(destroyMethod=&quot;&quot;) public DataSource dataSource() throws Exception &#123; Context ctx = new InitialContext(); return (DataSource) ctx.lookup(&quot;java:comp/env/jdbc/datasource&quot;); &#125;&#125; 修饰注解12345@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Profile(&quot;production&quot;)public @interface Production &#123;&#125; 修饰方法1234567891011121314151617181920@Configurationpublic class AppConfig &#123; @Bean(&quot;dataSource&quot;) @Profile(&quot;development&quot;) public DataSource standaloneDataSource() &#123; return new EmbeddedDatabaseBuilder() .setType(EmbeddedDatabaseType.HSQL) .addScript(&quot;classpath:com/bank/config/sql/schema.sql&quot;) .addScript(&quot;classpath:com/bank/config/sql/test-data.sql&quot;) .build(); &#125; @Bean(&quot;dataSource&quot;) @Profile(&quot;production&quot;) public DataSource jndiDataSource() throws Exception &#123; Context ctx = new InitialContext(); return (DataSource) ctx.lookup(&quot;java:comp/env/jdbc/datasource&quot;); &#125;&#125; 激活使用插件1spring-boot:run -Drun.profiles=prod Jar 激活1java -jar -Dspring.profiles.active=prod *.jar 在Java中使用1System.setProperty(&quot;spring.profiles.active&quot;, &quot;test&quot;); 1234AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext();ctx.getEnvironment().setActiveProfiles(&quot;development&quot;);ctx.register(SomeConfig.class, StandaloneDataConfig.class, JndiDataConfig.class);ctx.refresh();","categories":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/categories/Java/"},{"name":"Spring Boot","slug":"Java/Spring-Boot","permalink":"https://blog.ifan.host/categories/Java/Spring-Boot/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.ifan.host/tags/Spring-Boot/"}]},{"title":"MongoDB 聚合","slug":"database/mongodb/mongo-pipeline","date":"2022-06-07T01:15:00.000Z","updated":"2023-12-25T05:56:09.162Z","comments":true,"path":"2022/06/07/database/mongodb/mongo-pipeline/","link":"","permalink":"https://blog.ifan.host/2022/06/07/database/mongodb/mongo-pipeline/","excerpt":"","text":"MongoDB 的聚合操作聚合操作处理数据记录并返回计算结果。聚合操作将来自多个 document 的值分组，并可以对分组的数据执行各种操作以返回单个结果。 MongoDB 提供了三种执行聚合的方式：聚合管道，map-reduce 函数和单一目的聚合方法。 PipelinePipeline 简介MongoDB 的聚合框架以数据处理管道（Pipeline）的概念为模型。 MongoDB 通过 db.collection.aggregate() 方法支持聚合操作。并提供了 aggregate 命令来执行 pipeline。 MongoDB Pipeline 由多个阶段（stages）组成。每个阶段在 document 通过 pipeline 时都会对其进行转换。pipeline 阶段不需要为每个输入 document 都生成一个输出 document。例如，某些阶段可能会生成新 document 或过滤 document。 同一个阶段可以在 pipeline 中出现多次，但 $out、$merge,和 $geoNear 阶段除外。所有可用 pipeline 阶段可以参考：Aggregation Pipeline Stages。 第一阶段：$match 阶段按状态字段过滤 document，然后将状态等于 A 的那些 document 传递到下一阶段。 第二阶段：$group 阶段按 cust_id 字段对 document 进行分组，以计算每个唯一 cust_id 的金额总和。 最基本的管道阶段提供过滤器，其操作类似于查询和 document 转换（修改输出 document 形式）。 其他管道操作提供了用于按特定字段对 document 进行分组和排序的工具，以及用于汇总数组（包括 document 数组）内容的工具。另外，管道阶段可以将运算符用于诸如计算平均值或连接字符串之类的任务。 聚合管道也可以在分片 collection 上操作。 Pipeline 优化投影优化Pipeline 可以确定是否仅需要 document 中必填字段即可获得结果。 Pipeline 串行优化（$project、$unset、$addFields、$set） + $match 串行优化 对于包含投影阶段（$project 或 $unset 或 $addFields 或 $set），且后续跟随着 $match 阶段的 Pipeline ，MongoDB 会将所有 $match 阶段中不需要在投影阶段中计算出的值的过滤器，移动一个在投影阶段之前的新 $match 阶段。 如果 Pipeline 包含多个投影阶段 和 &#x2F; 或 $match 阶段，则 MongoDB 将为每个 $match 阶段执行此优化，将每个 $match 过滤器移动到该过滤器不依赖的所有投影阶段之前。 【示例】Pipeline 串行优化示例 优化前： 1234567891011121314&#123; $addFields: &#123; maxTime: &#123; $max: &quot;$times&quot; &#125;, minTime: &#123; $min: &quot;$times&quot; &#125;&#125; &#125;,&#123; $project: &#123; _id: 1, name: 1, times: 1, maxTime: 1, minTime: 1, avgTime: &#123; $avg: [&quot;$maxTime&quot;, &quot;$minTime&quot;] &#125;&#125; &#125;,&#123; $match: &#123; name: &quot;Joe Schmoe&quot;, maxTime: &#123; $lt: 20 &#125;, minTime: &#123; $gt: 5 &#125;, avgTime: &#123; $gt: 7 &#125;&#125; &#125; 优化后： 1234567891011&#123; $match: &#123; name: &quot;Joe Schmoe&quot; &#125; &#125;,&#123; $addFields: &#123; maxTime: &#123; $max: &quot;$times&quot; &#125;, minTime: &#123; $min: &quot;$times&quot; &#125;&#125; &#125;,&#123; $match: &#123; maxTime: &#123; $lt: 20 &#125;, minTime: &#123; $gt: 5 &#125; &#125; &#125;,&#123; $project: &#123; _id: 1, name: 1, times: 1, maxTime: 1, minTime: 1, avgTime: &#123; $avg: [&quot;$maxTime&quot;, &quot;$minTime&quot;] &#125;&#125; &#125;,&#123; $match: &#123; avgTime: &#123; $gt: 7 &#125; &#125; &#125; 说明： &#123; name: &quot;Joe Schmoe&quot; &#125; 不需要计算任何投影阶段的值，所以可以放在最前面。 &#123; avgTime: &#123; $gt: 7 &#125; &#125; 依赖 $project 阶段的 avgTime 字段，所以不能移动。 maxTime 和 minTime 字段被 $addFields 阶段所依赖，但自身不依赖其他，所以会新建一个 $match 阶段，并将其置于 $project 阶段之前。 Pipeline 并行优化如果可能，优化阶段会将 Pipeline 阶段合并到其前身。通常，合并发生在任意序列重新排序优化之后。 $sort + $limit当 $sort 在 $limit 之前时，如果没有中间阶段修改文档数量（例如 $unwind、$group），则优化程序可以将 $limit 合并到 $sort 中。如果有管道阶段更改了 $sort 和 $limit 阶段之间的文档数，则 MongoDB 不会将 $limit 合并到 $sort 中。 【示例】$sort + $limit 优化前： 123&#123; $sort : &#123; age : -1 &#125; &#125;,&#123; $project : &#123; age : 1, status : 1, name : 1 &#125; &#125;,&#123; $limit: 5 &#125; 优化后： 1234567891011121314&#123; &quot;$sort&quot; : &#123; &quot;sortKey&quot; : &#123; &quot;age&quot; : -1 &#125;, &quot;limit&quot; : NumberLong(5) &#125;&#125;,&#123; &quot;$project&quot; : &#123; &quot;age&quot; : 1, &quot;status&quot; : 1, &quot;name&quot; : 1 &#125;&#125; $limit + $limit如果一个 $limit 紧随另一个 $limit，那么它们可以合并为一。 优化前： 12&#123; $limit: 100 &#125;,&#123; $limit: 10 &#125; 优化后： 123&#123; $limit: 10&#125; $skip + $skip如果一个 $skip 紧随另一个 $skip，那么它们可以合并为一。 优化前： 12&#123; $skip: 5 &#125;,&#123; $skip: 2 &#125; 优化后： 123&#123; $skip: 7&#125; $match + $match如果一个 $skip 紧随另一个 $skip ，那么它们可以通过 $and 合并为一。 优化前： 12&#123; $match: &#123; year: 2014 &#125; &#125;,&#123; $match: &#123; status: &quot;A&quot; &#125; &#125; 优化后： 12345&#123; $match: &#123; $and: [&#123; year: 2014 &#125;, &#123; status: &#x27;A&#x27; &#125;] &#125;&#125; $lookup + $unwind如果一个 $unwind 紧随另一个 $lookup，并且 $unwind 在 $lookup 的 as 字段上运行时，优化程序可以将 $unwind 合并到 $lookup 阶段。这样可以避免创建较大的中间文档。 优化前： 123456789&#123; $lookup: &#123; from: &quot;otherCollection&quot;, as: &quot;resultingArray&quot;, localField: &quot;x&quot;, foreignField: &quot;y&quot; &#125;&#125;,&#123; $unwind: &quot;$resultingArray&quot;&#125; 优化后： 123456789&#123; $lookup: &#123; from: &quot;otherCollection&quot;, as: &quot;resultingArray&quot;, localField: &quot;x&quot;, foreignField: &quot;y&quot;, unwinding: &#123; preserveNullAndEmptyArrays: false &#125; &#125;&#125; Pipeline 限制结果集中的每个文档均受 BSON 文档大小限制（当前为 16 MB） Pipeline 的内存限制为 100 MB。 Map-Reduce 聚合 pipeline 比 map-reduce 提供更好的性能和更一致的接口。 Map-reduce 是一种数据处理范式，用于将大量数据汇总为有用的聚合结果。为了执行 map-reduce 操作，MongoDB 提供了 mapReduce 数据库命令。 在上面的操作中，MongoDB 将 map 阶段应用于每个输入 document（即 collection 中与查询条件匹配的 document）。 map 函数分发出多个键-值对。对于具有多个值的那些键，MongoDB 应用 reduce 阶段，该阶段收集并汇总聚合的数据。然后，MongoDB 将结果存储在 collection 中。可选地，reduce 函数的输出可以通过 finalize 函数来进一步汇总聚合结果。 MongoDB 中的所有 map-reduce 函数都是 JavaScript，并在 mongod 进程中运行。 Map-reduce 操作将单个 collection 的 document 作为输入，并且可以在开始 map 阶段之前执行任意排序和限制。 mapReduce 可以将 map-reduce 操作的结果作为 document 返回，也可以将结果写入 collection。 单一目的聚合方法MongoDB 支持一下单一目的的聚合操作： db.collection.estimatedDocumentCount() db.collection.count() db.collection.distinct() 所有这些操作都汇总了单个 collection 中的 document。尽管这些操作提供了对常见聚合过程的简单访问，但是它们相比聚合 pipeline 和 map-reduce，缺少灵活性和丰富的功能性。 SQL 和 MongoDB 聚合对比MongoDB pipeline 提供了许多等价于 SQL 中常见聚合语句的操作。 下表概述了常见的 SQL 聚合语句或函数和 MongoDB 聚合操作的映射表： SQL Terms, Functions, and Concepts MongoDB Aggregation Operators WHERE $match GROUP BY $group HAVING $match SELECT $project ORDER BY $sort LIMIT $limit SUM() $sum COUNT() $sum$sortByCount JOIN $lookup SELECT INTO NEW_TABLE $out MERGE INTO TABLE $merge (Available starting in MongoDB 4.2) UNION ALL $unionWith (Available starting in MongoDB 4.4) 【示例】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101db.orders.insertMany([ &#123; _id: 1, cust_id: &#x27;Ant O. Knee&#x27;, ord_date: new Date(&#x27;2020-03-01&#x27;), price: 25, items: [ &#123; sku: &#x27;oranges&#x27;, qty: 5, price: 2.5 &#125;, &#123; sku: &#x27;apples&#x27;, qty: 5, price: 2.5 &#125;, ], status: &#x27;A&#x27;, &#125;, &#123; _id: 2, cust_id: &#x27;Ant O. Knee&#x27;, ord_date: new Date(&#x27;2020-03-08&#x27;), price: 70, items: [ &#123; sku: &#x27;oranges&#x27;, qty: 8, price: 2.5 &#125;, &#123; sku: &#x27;chocolates&#x27;, qty: 5, price: 10 &#125;, ], status: &#x27;A&#x27;, &#125;, &#123; _id: 3, cust_id: &#x27;Busby Bee&#x27;, ord_date: new Date(&#x27;2020-03-08&#x27;), price: 50, items: [ &#123; sku: &#x27;oranges&#x27;, qty: 10, price: 2.5 &#125;, &#123; sku: &#x27;pears&#x27;, qty: 10, price: 2.5 &#125;, ], status: &#x27;A&#x27;, &#125;, &#123; _id: 4, cust_id: &#x27;Busby Bee&#x27;, ord_date: new Date(&#x27;2020-03-18&#x27;), price: 25, items: [&#123; sku: &#x27;oranges&#x27;, qty: 10, price: 2.5 &#125;], status: &#x27;A&#x27;, &#125;, &#123; _id: 5, cust_id: &#x27;Busby Bee&#x27;, ord_date: new Date(&#x27;2020-03-19&#x27;), price: 50, items: [&#123; sku: &#x27;chocolates&#x27;, qty: 5, price: 10 &#125;], status: &#x27;A&#x27;, &#125;, &#123; _id: 6, cust_id: &#x27;Cam Elot&#x27;, ord_date: new Date(&#x27;2020-03-19&#x27;), price: 35, items: [ &#123; sku: &#x27;carrots&#x27;, qty: 10, price: 1.0 &#125;, &#123; sku: &#x27;apples&#x27;, qty: 10, price: 2.5 &#125;, ], status: &#x27;A&#x27;, &#125;, &#123; _id: 7, cust_id: &#x27;Cam Elot&#x27;, ord_date: new Date(&#x27;2020-03-20&#x27;), price: 25, items: [&#123; sku: &#x27;oranges&#x27;, qty: 10, price: 2.5 &#125;], status: &#x27;A&#x27;, &#125;, &#123; _id: 8, cust_id: &#x27;Don Quis&#x27;, ord_date: new Date(&#x27;2020-03-20&#x27;), price: 75, items: [ &#123; sku: &#x27;chocolates&#x27;, qty: 5, price: 10 &#125;, &#123; sku: &#x27;apples&#x27;, qty: 10, price: 2.5 &#125;, ], status: &#x27;A&#x27;, &#125;, &#123; _id: 9, cust_id: &#x27;Don Quis&#x27;, ord_date: new Date(&#x27;2020-03-20&#x27;), price: 55, items: [ &#123; sku: &#x27;carrots&#x27;, qty: 5, price: 1.0 &#125;, &#123; sku: &#x27;apples&#x27;, qty: 10, price: 2.5 &#125;, &#123; sku: &#x27;oranges&#x27;, qty: 10, price: 2.5 &#125;, ], status: &#x27;A&#x27;, &#125;, &#123; _id: 10, cust_id: &#x27;Don Quis&#x27;, ord_date: new Date(&#x27;2020-03-23&#x27;), price: 25, items: [&#123; sku: &#x27;oranges&#x27;, qty: 10, price: 2.5 &#125;], status: &#x27;A&#x27;, &#125;,]) SQL 和 MongoDB 聚合方式对比： MongoDB 聚合操作MongoDB 中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似 sql 语句中的 count(*)。 管道整个聚合运算过程称为管道，它是由多个步骤组成，每个管道 接受一系列文档（原始数据）； 每个步骤对这些文档进行一系列运算； 结果文档输出给下一个步骤； 聚合操作的基本格式 123pipeline = [$stage1, $stage1, ..., $stageN];db.&lt;集合&gt;.aggregate(pipeline, &#123;options&#125;); 聚合步骤 步骤 作用 SQL 等价运算符 $match 过滤 WHERE $project 投影 AS $sort 排序 ORDER BY $group 分组 GROUP BY $skip &#x2F; $limit 结果限制 SKIP &#x2F; LIMIT $lookup 左外连接 LEFT OUTER JOIN $unwind 展开数组 N&#x2F;A $graphLookup 图搜索 N&#x2F;A $facet &#x2F; $bucket 分面搜索 N&#x2F;A 【示例】 12345&gt; db.collection.insertMany([&#123;&quot;title&quot;:&quot;MongoDB Overview&quot;,&quot;description&quot;:&quot;MongoDB is no sql database&quot;,&quot;by_user&quot;:&quot;collection&quot;,&quot;tagsr&quot;:[&quot;mongodb&quot;,&quot;database&quot;,&quot;NoSQL&quot;],&quot;likes&quot;:&quot;100&quot;&#125;,&#123;&quot;title&quot;:&quot;NoSQL Overview&quot;,&quot;description&quot;:&quot;No sql database is very fast&quot;,&quot;by_user&quot;:&quot;collection&quot;,&quot;tagsr&quot;:[&quot;mongodb&quot;,&quot;database&quot;,&quot;NoSQL&quot;],&quot;likes&quot;:&quot;10&quot;&#125;,&#123;&quot;title&quot;:&quot;Neo4j Overview&quot;,&quot;description&quot;:&quot;Neo4j is no sql database&quot;,&quot;by_user&quot;:&quot;Neo4j&quot;,&quot;tagsr&quot;:[&quot;neo4j&quot;,&quot;database&quot;,&quot;NoSQL&quot;],&quot;likes&quot;:&quot;750&quot;&#125;])&gt; db.collection.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, num_tutorial : &#123;$sum : 1&#125;&#125;&#125;])&#123; &quot;_id&quot; : null, &quot;num_tutorial&quot; : 3 &#125;&#123; &quot;_id&quot; : &quot;Neo4j&quot;, &quot;num_tutorial&quot; : 1 &#125;&#123; &quot;_id&quot; : &quot;collection&quot;, &quot;num_tutorial&quot; : 2 &#125; 下表展示了一些聚合的表达式: 表达式 描述 实例 $sum 计算总和。 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, num_tutorial : &#123;$sum : &quot;$likes&quot;&#125;&#125;&#125;]) $avg 计算平均值 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, num_tutorial : &#123;$avg : &quot;$likes&quot;&#125;&#125;&#125;]) $min 获取集合中所有文档对应值得最小值。 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, num_tutorial : &#123;$min : &quot;$likes&quot;&#125;&#125;&#125;]) $max 获取集合中所有文档对应值得最大值。 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, num_tutorial : &#123;$max : &quot;$likes&quot;&#125;&#125;&#125;]) $push 在结果文档中插入值到一个数组中。 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, url : &#123;$push: &quot;$url&quot;&#125;&#125;&#125;]) $addToSet 在结果文档中插入值到一个数组中，但不创建副本。 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, url : &#123;$addToSet : &quot;$url&quot;&#125;&#125;&#125;]) $first 根据资源文档的排序获取第一个文档数据。 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, first_url : &#123;$first : &quot;$url&quot;&#125;&#125;&#125;]) $last 根据资源文档的排序获取最后一个文档数据 db.mycol.aggregate([&#123;$group : &#123;_id : &quot;$by_user&quot;, last_url : &#123;$last : &quot;$url&quot;&#125;&#125;&#125;]) MapReduce12345678910111213db.runCommand(&#123; mapreduce:&lt;collection&gt;, //需要进行处理的集合名 map:&lt;mapfunction&gt;, //映射函数（分组） reduce:&lt;reducefunction&gt;, //统计函数 [,query:&lt;query filter object&gt;] //，在发往map函数之前，对文档进行过滤 [,sort:&lt;sorts the input objects using this key.Useful for optimization,like sorting by the emit key for fewer reduces&gt;] //在发往map函数之前，对文档进行排序 [,limit:&lt;number of objects to return from collection&gt;] //限制发往map函数的文档数量 [,out:&lt;see output options below&gt;] //新建集合，用于存放统计结果 [,keeptemp:&lt;true|false&gt;] //是否保存统计结果为临时集合 [,finalize:&lt;finalizefunction&gt;] //最终处理函数，对reduce返回结果（存入out之前）进行最终处理 [,scope:&lt;object where fields go into javascript global scope&gt;] //向map、reduce、finalize导入外部变量 [,verbose:true] //详细的统计信息，用于调试&#125;); map函数(分组) ，调用emit(key, values)，遍历collection中所有的记录，其中emit中的key是分组依据，values是分组后需要保留的数据；key对应最后结果集中的_id。 reduce函数(聚合) ，接收map函数的key,values作为参数，将key-values变成key-value，将values数组变成一个个单一的value。当key-values中的values数组过大时，会再被切分成多个小的key-values，再对这些小的key-values分别执行reduce，再将多个块的结构组合成一个新的数组，作为redus函数的第二个参数，继续reduce操作。 group,aggegate,MapReduce比较 group aggregate MapReduce 是否使用JavaScript引擎 是，定制reduce函数 是，不能编写自定义函数 是 返回结果集保存位置 内联，结果必须符合BSON文档限制 内联，服务器支持最大文档大小(16Mb) 内联，新集合，合并，替换，减少 处理数据大小 小于10000 操作在内存中完成，有内存大小的限制， 大型数据集，超过20000的独立分组 处理性能 低于aggregate 较高，管道可以重复使用 低于aggregate 灵活度 低于mapReduce 低于MapReduce 较高，能使用JavaScript","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"数据库/MongoDB","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://blog.ifan.host/tags/MongoDB/"}]},{"title":"MongoDB 数据导入导出","slug":"database/mongodb/mongodb-backup","date":"2022-06-07T01:10:00.000Z","updated":"2023-12-25T05:56:09.162Z","comments":true,"path":"2022/06/07/database/mongodb/mongodb-backup/","link":"","permalink":"https://blog.ifan.host/2022/06/07/database/mongodb/mongodb-backup/","excerpt":"","text":"备份和恢复数据备份在 Mongodb 中，使用 mongodump 命令来备份 MongoDB 数据。该命令可以导出所有数据到指定目录中。 mongodump 命令可以通过参数指定导出的数据量级转存的服务器。 mongodump 命令语法如下： 1mongodump -h dbhost -d dbname -o dbdirectory -h：MongDB 所在服务器地址，例如：127.0.0.1，当然也可以指定端口号：127.0.0.1:27017 -d：需要备份的数据库实例，例如：test -o：备份的数据存放位置，例如：c:\\data\\dump，当然该目录需要提前建立，在备份完成后，系统自动在 dump 目录下建立一个 test 目录，这个目录里面存放该数据库实例的备份数据。 mongodump 命令可选参数列表如下所示： 语法 描述 实例 mongodump –host HOST_NAME –port PORT_NUMBER 该命令将备份所有 MongoDB 数据 mongodump –host runoob.com –port 27017 mongodump –dbpath DB_PATH –out BACKUP_DIRECTORY mongodump –dbpath &#x2F;data&#x2F;db&#x2F; –out &#x2F;data&#x2F;backup&#x2F; mongodump –collection COLLECTION –db DB_NAME 该命令将备份指定数据库的集合。 mongodump –collection mycol –db test 【示例】备份全量数据 1$ mongodump -h 127.0.0.1 --port 27017 -o test2 【示例】备份指定数据库 1mongodump -h 127.0.0.1 --port 27017 -d admin -o test3 数据恢复mongodb 使用 mongorestore 命令来恢复备份的数据。 mongorestore 命令语法如下： 1&gt; mongorestore -h &lt;hostname&gt;&lt;:port&gt; -d dbname &lt;path&gt; --host &lt;:port&gt;, -h &lt;:port&gt;：MongoDB 所在服务器地址，默认为： localhost:27017 --db , -d ：需要恢复的数据库实例，例如：test，当然这个名称也可以和备份时候的不一样，比如 test2 --drop：恢复的时候，先删除当前数据，然后恢复备份的数据。就是说，恢复后，备份后添加修改的数据都会被删除，慎用哦！ &lt;path&gt;：mongorestore 最后的一个参数，设置备份数据所在位置，例如：c:\\data\\dump\\test。你不能同时指定 &lt;path&gt; 和 --dir 选项，--dir 也可以设置备份目录。 --dir：指定备份的目录。你不能同时指定 &lt;path&gt; 和 --dir 选项。 【示例】 1$ mongorestore -h 127.0.0.1 --port 27017 -d test --dir test --drop 导入导出mongoimport 和 mongoexport 并不能可靠地保存所有的富文本 BSON 数据类型，因为 JSON 仅能代表一种 BSON 支持的子集类型。因此，数据用这些工具导出导入或许会丢失一些精确程度。 导入操作在 MongoDB 中，使用 mongoimport 来导入数据。 默认情况下，mongoimport 会将数据导入到本地主机端口 27017 上的 MongoDB 实例中。要将数据导入在其他主机或端口上运行的 MongoDB 实例中，请通过包含 --host 和 --port 选项来指定主机名或端口。 使用 --drop 选项删除集合（如果已经存在）。 这样可以确保该集合仅包含您要导入的数据。 语法格式： 1mongoimport -h IP --port 端口 -u 用户名 -p 密码 -d 数据库 -c 表名 --type 类型 --headerline --upsert --drop 文件名 【示例】导入表数据 1$ mongoimport -h 127.0.0.1 --port 27017 -d test -c book --drop test/book.dat 【示例】从 json 文件中导入表数据 1$ mongoimport -h 127.0.0.1 --port 27017 -d test -c student --upsert test/student.json 【示例】从 csv 文件中导入表数据 1$ mongoimport -h 127.0.0.1 --port 27017 -d test -c product --type csv --headerline test/product.csv 【示例】导入部分表字段数据 1$ mongoimport -h 127.0.0.1 --port 27017 -d test -c product --type json --upsertFields name,price test/product.json 导出操作语法格式： 1mongoexport -h &lt;IP&gt; --port &lt;端口&gt; -u &lt;用户名&gt; -p &lt;密码&gt; -d &lt;数据库&gt; -c &lt;表名&gt; -f &lt;字段&gt; -q &lt;条件导出&gt; --csv -o &lt;文件名&gt; -f：导出指字段，以逗号分割，-f name,email,age 导出 name,email,age 这三个字段 -q：可以根查询条件导出，-q &#39;&#123; &quot;uid&quot; : &quot;100&quot; &#125;&#39; 导出 uid 为 100 的数据 --csv：表示导出的文件格式为 csv 的，这个比较有用，因为大部分的关系型数据库都是支持 csv，在这里有共同点 【示例】导出整张表 1$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product -o test/product.dat 【示例】导出表到 json 文件 1$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product --type json -o test/product.json 【示例】导出表中部分字段到 csv 文件 1$ mongoexport -h 127.0.0.1 --port 27017 -d test -c product --type csv -f name,price -o test/product.csv","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"数据库/MongoDB","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://blog.ifan.host/tags/MongoDB/"}]},{"title":"MongoDB 建模","slug":"database/mongodb/mongodb-modeling","date":"2022-06-07T01:00:00.000Z","updated":"2023-12-25T05:56:09.162Z","comments":true,"path":"2022/06/07/database/mongodb/mongodb-modeling/","link":"","permalink":"https://blog.ifan.host/2022/06/07/database/mongodb/mongodb-modeling/","excerpt":"","text":"MongoDB 数据类型 数据类型 描述 String 字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。 Integer 整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。 Boolean 布尔值。用于存储布尔值（真&#x2F;假）。 Double 双精度浮点值。用于存储浮点值。 Min&#x2F;Max keys 将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。 Array 用于将数组或列表或多个值存储为一个键。 Timestamp 时间戳。记录文档修改或添加的具体时间。 Object 用于内嵌文档。 Null 用于创建空值。 Symbol 符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 Date 日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 Object ID 对象 ID。用于创建文档的 ID。 Binary Data 二进制数据。用于存储二进制数据。 Code 代码类型。用于在文档中存储 JavaScript 代码。 Regular expression 正则表达式类型。用于存储正则表达式。 MongoDB 建模MongoDB 的数据模式是一种灵活模式，关系型数据库要求你在插入数据之前必须先定义好一个表的模式结构，而 MongoDB 的集合则并不限制 document 结构。这种灵活性让对象和数据库文档之间的映射变得很容易。即使数据记录之间有很大的变化，每个文档也可以很好的映射到各条不同的记录。 当然在实际使用中，同一个集合中的文档往往都有一个比较类似的结构。 数据模型设计中最具挑战性的是在应用程序需求，数据库引擎性能要求和数据读写模式之间做权衡考量。当设计数据模型的时候，一定要考虑应用程序对数据的使用模式（如查询，更新和处理）以及数据本身的天然结构。 MongoDB 数据建模入门（一）定义数据集当需要建立数据存储时，首先应该思考以下问题：需要存储哪些数据？这些字段之间如何关联？ 这是一个数据建模的过程。目标是将业务需求抽象为逻辑模型。 假设这样一个场景：我们需要建立数据库以跟踪物料及其数量，大小，标签和等级。 如果是存储在 RDBMS，可能以下的数据表： name quantity size status tags rating journal 25 14x21,cm A brown, lined 9 notebook 50 8.5x11,in A college-ruled,perforated 8 paper 100 8.5x11,in D watercolor 10 planner 75 22.85x30,cm D 2019 10 postcard 45 10x,cm D double-sided,white 2 （二）思考 JSON 结构从上例中可以看出，表似乎是存储数据的好地方，但该数据集中的字段需要多个值，如果在单个列中建模，则不容易搜索或显示（对于 例如–大小和标签）。 在 SQL 数据库中，您可以通过创建关系表来解决此问题。 在 MongoDB 中，数据存储为文档（document）。 这些文档以 JSON（JavaScript 对象表示法）格式存储在 MongoDB 中。 JSON 文档支持嵌入式字段，因此相关数据和数据列表可以与文档一起存储，而不是与外部表一起存储。 JSON 格式为键&#x2F;值对。 在 JSON 文档中，字段名和值用冒号分隔，字段名和值对用逗号分隔，并且字段集封装在“大括号”（&#123;&#125;）中。 如果要开始对上面的行之一进行建模，例如此行： name quantity size status tags rating notebook 50 8.5x11,in A college-ruled,perforated 8 您可以从 name 和 quantity 字段开始。 在 JSON 中，这些字段如下所示： 1&#123; &quot;name&quot;: &quot;notebook&quot;, &quot;qty&quot;: 50 &#125; （三）确定哪些字段作为嵌入式数据接下来，需要确定哪些字段可能需要多个值。可以考虑将这些字段作为嵌入式文档或嵌入式文档中的 列表&#x2F;数组 对象。 例如，在上面的示例中，size 可能包含三个字段： 1&#123; &quot;h&quot;: 11, &quot;w&quot;: 8.5, &quot;uom&quot;: &quot;in&quot; &#125; And some items have multiple ratings, so ratings might be represented as a list of documents containing the field scores: 1[&#123; &quot;score&quot;: 8 &#125;, &#123; &quot;score&quot;: 9 &#125;] And you might need to handle multiple tags per item. So you might store them in a list too. 1[&quot;college-ruled&quot;, &quot;perforated&quot;] Finally, a JSON document that stores an inventory item might look like this: 12345678&#123; &quot;name&quot;: &quot;notebook&quot;, &quot;qty&quot;: 50, &quot;rating&quot;: [&#123; &quot;score&quot;: 8 &#125;, &#123; &quot;score&quot;: 9 &#125;], &quot;size&quot;: &#123; &quot;height&quot;: 11, &quot;width&quot;: 8.5, &quot;unit&quot;: &quot;in&quot; &#125;, &quot;status&quot;: &quot;A&quot;, &quot;tags&quot;: [&quot;college-ruled&quot;, &quot;perforated&quot;]&#125; This looks very different from the tabular data structure you started with in Step 1. 数据模型简介数据建模中的关键挑战是平衡应用程序的需求、数据库引擎的性能以及数据检索模式。 在设计数据模型时，始终需要考虑数据的应用程序使用情况（即数据的查询，更新和处理）以及数据本身的固有结构。 灵活的 Schema在关系型数据库中，必须在插入数据之前确定并声明表的结构。而 MongoDB 的 collection 默认情况下不需要其文档具有相同的架构。也就是说： 同一个 collection 中的 document 不需要具有相同的 field 集，并且 field 的数据类型可以在集合中的不同文档之间有所不同。 要更改 collection 中的 document 结构，例如添加新 field，删除现有 field 或将 field 值更改为新类型，只需要将文档更新为新结构即可。 这种灵活性有助于将 document 映射到实体或对象。每个 document 都可以匹配所表示实体的数据字段，即使该文档与集合中的其他文档有很大的不同。但是，实际上，集合中的文档具有相似的结构，并且您可以在更新和插入操作期间对 collection 强制执行 document 校验规则。 Document 结构嵌入式数据模型嵌入式 document 通过将相关数据存储在单个 document 结构中来捕获数据之间的关系。 MongoDB document 可以将 document 结构嵌入到另一个 document 中的字段或数组中。这些非规范化的数据模型允许应用程序在单个数据库操作中检索和操纵相关数据。 对于 MongoDB 中的很多场景，非规范化数据模型都是最佳的。 嵌入式 document 有大小限制：必须小于 16 MB。 如果是较大的二进制数据，可以考虑 GridFS。 引用式数据模型引用通过包含从一个 document 到另一个 document 的链接或引用来存储数据之间的关系。 应用程序可以解析这些引用以访问相关数据。 广义上讲，这些是规范化的数据模型。 通常，在以下场景使用引用式的数据模型： 嵌入时会导致数据重复，但无法提供足够的读取性能优势，无法胜过重复的含义。 代表更复杂的多对多关系。 为大规模分层数据集建模。 为了 join collection，MongoDB 支持聚合 stage： $lookup（MongoDB 3.2 开始支持） $graphLookup（MongoDB 3.4 开始支持） MongoDB 还提供了引用来支持跨集合 join 数据： 引用数据模型示例，参考：Model One-to-Many Relationships with Document References. 更多树形模型，参考：Model Tree Structures. 原子写操作单 document 的原子性在 MongoDB 中，针对单个 document 的写操作是原子性的，即使该 document 中嵌入了多个子 document。 具有嵌入数据的非规范化数据模型将所有相关数据合并在一个 document 中，而不是在多个 document 和 collection 中进行规范化。 该数据模型有助于原子操作。 当单个写入操作（例如 db.collection.updateMany()）修改多个 document 时，每个 document 的独立修改是原子的，但整个操作不是原子的。 多 document 事务对于需要对多个 document（在单个或多个集合中）进行读写原子性的情况，MongoDB 支持多 document 事务。 在版本 4.0 中，MongoDB 在副本集上支持多 document 事务。 在版本 4.2 中，MongoDB 引入了分布式事务，它增加了对分片群集上多 document 事务的支持，并合并了对副本集上多 document 事务的现有支持。 在大多数情况下，多 document 事务会比单 document 的写入产生更高的性能消耗，并且多 document 事务的可用性不能替代高效的结构设计。 在许多情况下，非规范化数据模型（嵌入式 document 和数组）仍是最佳选择。 也就是说，合理的数据建模，将最大程度地减少对多 document 事务的需求。 数据使用和性能在设计数据模型时，请考虑应用程序将如何使用您的数据库。 例如，如果您的应用程序仅使用最近插入的 document，请考虑使用上限集合。 或者，如果您的应用程序主要是对 collection 的读取操作，则添加索引以提高性能。 Schema 校验指定校验规则如果创建新 collection 时要指定校验规则，需要在使用 db.createCollection() 时指定 validator 选项。 如果要将 document 校验添加到现有 collection 中，需要使用带有 validator 选项的 collMod 命令。 MongoDB 还提供以下相关选项： validationLevel 选项（用于确定 MongoDB 在更新过程中，对现有 document 应用校验规则的严格程度） validationAction 选项（用于确定 MongoDB 发现违反校验规则的 document 时，是选择报错并拒绝，还是接受数据但在日志中告警）。 JSON Schema从 3.6 版本开始，MongoDB 开始支持 JSON Schema 校验。 可以通过在 validator 表达式中使用 $jsonSchema 操作来指定 JSON Schema 校验。 【示例】 123456789101112131415161718192021222324252627282930313233343536373839404142db.createCollection(&#x27;students&#x27;, &#123; validator: &#123; $jsonSchema: &#123; bsonType: &#x27;object&#x27;, required: [&#x27;name&#x27;, &#x27;year&#x27;, &#x27;major&#x27;, &#x27;address&#x27;], properties: &#123; name: &#123; bsonType: &#x27;string&#x27;, description: &#x27;must be a string and is required&#x27;, &#125;, year: &#123; bsonType: &#x27;int&#x27;, minimum: 2017, maximum: 3017, description: &#x27;must be an integer in [ 2017, 3017 ] and is required&#x27;, &#125;, major: &#123; enum: [&#x27;Math&#x27;, &#x27;English&#x27;, &#x27;Computer Science&#x27;, &#x27;History&#x27;, null], description: &#x27;can only be one of the enum values and is required&#x27;, &#125;, gpa: &#123; bsonType: [&#x27;double&#x27;], description: &#x27;must be a double if the field exists&#x27;, &#125;, address: &#123; bsonType: &#x27;object&#x27;, required: [&#x27;city&#x27;], properties: &#123; street: &#123; bsonType: &#x27;string&#x27;, description: &#x27;must be a string if the field exists&#x27;, &#125;, city: &#123; bsonType: &#x27;string&#x27;, description: &#x27;must be a string and is required&#x27;, &#125;, &#125;, &#125;, &#125;, &#125;, &#125;,&#125;) 其它查询表达式除了使用 $jsonSchema 查询运算符的 JSON Schema 校验外，MongoDB 还支持其它查询运算符的校验，但以下情况除外： $near, $nearSphere, $text, $where, and 带有 $function 表达式的 $expr 【示例】查询表达式中指定校验规则 123456789db.createCollection(&#x27;contacts&#x27;, &#123; validator: &#123; $or: [ &#123; phone: &#123; $type: &#x27;string&#x27; &#125; &#125;, &#123; email: &#123; $regex: /@mongodb\\.com$/ &#125; &#125;, &#123; status: &#123; $in: [&#x27;Unknown&#x27;, &#x27;Incomplete&#x27;] &#125; &#125;, ], &#125;,&#125;) 行为校验发生在更新和插入期间。添加校验规则到 collection 时，不会对现有的 document 进行校验，除非发生修改操作。 现有的 documentvalidationLevel 选项确定 MongoDB 进行规则校验时执行的操作： 如果 validationLevel 是 strict（严格级别。这是 MongoDB 默认级别），则 MongoDB 将校验规则应用于所有插入和更新。 如果 validationLevel 是 moderate（中等级别），则 MongoDB 只对已满足校验条件的现有文档的插入和更新操作进行校验；对不符合校验标准的现有文档的更新操作不进行校验。 【示例】 下面是一个正常的插入操作： 12345678910db.contacts.insert([ &#123; _id: 1, name: &#x27;Anne&#x27;, phone: &#x27;+1 555 123 456&#x27;, city: &#x27;London&#x27;, status: &#x27;Complete&#x27;, &#125;, &#123; _id: 2, name: &#x27;Ivan&#x27;, city: &#x27;Vancouver&#x27; &#125;,]) 在 collection 上配置一个校验规则： 1234567891011121314151617181920db.runCommand(&#123; collMod: &#x27;contacts&#x27;, validator: &#123; $jsonSchema: &#123; bsonType: &#x27;object&#x27;, required: [&#x27;phone&#x27;, &#x27;name&#x27;], properties: &#123; phone: &#123; bsonType: &#x27;string&#x27;, description: &#x27;must be a string and is required&#x27;, &#125;, name: &#123; bsonType: &#x27;string&#x27;, description: &#x27;must be a string and is required&#x27;, &#125;, &#125;, &#125;, &#125;, validationLevel: &#x27;moderate&#x27;,&#125;) 则 contacts collection 现在添加了含中等级别（moderate） validationLevel 的 validator： 如果尝试更新 _id为 1 的文档，则 MongoDB 将应用校验规则，因为现有文档符合条件。 相反，MongoDB 不会将校验 _id 为 2 的文档，因为它不符合校验规则。 如果要完全禁用校验，可以将 validationLevel 置为 off。 接受或拒绝无效的 document 如果 validationAction 是 Error（默认），则 MongoDB 拒绝任何违反校验规则的插入或更新。 如果 validationAction 是 Warn，MongoDB 会记录所有的违规，但允许进行插入或更新。 【示例】 创建集合时，配置 validationAction 为 warn。 12345678910111213141516171819202122232425db.createCollection(&#x27;contacts2&#x27;, &#123; validator: &#123; $jsonSchema: &#123; bsonType: &#x27;object&#x27;, required: [&#x27;phone&#x27;], properties: &#123; phone: &#123; bsonType: &#x27;string&#x27;, description: &#x27;must be a string and is required&#x27;, &#125;, email: &#123; bsonType: &#x27;string&#x27;, pattern: &#x27;@mongodb.com$&#x27;, description: &#x27;must be a string and match the regular expression pattern&#x27;, &#125;, status: &#123; enum: [&#x27;Unknown&#x27;, &#x27;Incomplete&#x27;], description: &#x27;can only be one of the enum values&#x27;, &#125;, &#125;, &#125;, &#125;, validationAction: &#x27;warn&#x27;,&#125;) 尝试插入一条违规记录 12&gt; db.contacts2.insert( &#123; name: &quot;Amanda&quot;, status: &quot;Updated&quot; &#125; )WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;) MongoDB 允许这条操作执行，但是服务器会记录下告警信息。 1&#123;&quot;t&quot;:&#123;&quot;$date&quot;:&quot;2020-09-11T16:35:57.754+08:00&quot;&#125;,&quot;s&quot;:&quot;W&quot;, &quot;c&quot;:&quot;STORAGE&quot;, &quot;id&quot;:20294, &quot;ctx&quot;:&quot;conn14&quot;,&quot;msg&quot;:&quot;Document would fail validation&quot;,&quot;attr&quot;:&#123;&quot;namespace&quot;:&quot;test.contacts2&quot;,&quot;document&quot;:&#123;&quot;_id&quot;:&#123;&quot;$oid&quot;:&quot;5f5b36ed8ea53d62a0b51c4e&quot;&#125;,&quot;name&quot;:&quot;Amanda&quot;,&quot;status&quot;:&quot;Updated&quot;&#125;&#125;&#125; 限制不能在 admin、local、config 这几个特殊的数据库中指定校验规则。 不能在 system.* collection 中指定校验。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"数据库/MongoDB","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://blog.ifan.host/tags/MongoDB/"}]},{"title":"Scala Date 工具方法","slug":"language/scala/dateutils","date":"2022-06-05T09:10:00.000Z","updated":"2023-12-25T05:56:09.177Z","comments":true,"path":"2022/06/05/language/scala/dateutils/","link":"","permalink":"https://blog.ifan.host/2022/06/05/language/scala/dateutils/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187import java.util.Date import org.joda.time.&#123;DateTime, Instant&#125;import org.joda.time.format.&#123;DateTimeFormat, DateTimeFormatter&#125;object DateUtil &#123; val DATE_FORMAT: DateTimeFormatter = DateTimeFormat.forPattern(&quot;yyyy-MM-dd&quot;) val TIME_FORMAT: DateTimeFormatter = DateTimeFormat.forPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;) val DATE_KEY_FORMAT: DateTimeFormatter = DateTimeFormat.forPattern(&quot;yyyyMMdd&quot;) val TIME_MINUTE_FORMAT: DateTimeFormatter = DateTimeFormat.forPattern(&quot;yyyyMMddHHmm&quot;) /** * 获取昨天的, 格式: yyyy-MM-dd * * @return 昨天的日期 */ def getYesterdayDate: String = &#123; DateTime.now() .minusDays(1).toString(DATE_FORMAT) &#125; /** * 获取当天日期, 格式: yyyy-MM-dd * * @return 当天日期 */ def getTodayDate: String = &#123; DateTime.now().toString(DATE_FORMAT) &#125; /** * 获取距当天 N 天时的日期 格式: yyyy-MM-dd * * @param n : 当天为 0，前一天为 1, 后一天为 -1 * @return yyyy-MM-dd */ def getDate(n: Int): String = &#123; DateTime.now().minusDays(n).toString(DATE_FORMAT) &#125; /** * 获取日期时间 格式: yyyy-MM-dd * * @param yesterday : 需要计算的日期 * @param n : 当天为 0，前一天为 1, 后一天为 -1 * @return yyyy-MM-dd */ def getOneDate(yesterday: String, n: Int): String = &#123; DateTime.parse(yesterday).minusDays(n).toString(DATE_FORMAT) &#125; def getNowTime: String = &#123; DateTime.now().toString(TIME_FORMAT) &#125; /** * 格式化日期, 格式: yyyy-MM-dd * * @param date Date对象 * @return 格式化后的日期 */ def formatDate(date: Date): String = &#123; new DateTime(date).toString(DATE_FORMAT) &#125; /** * 格式化时间, 格式: yyyy-MM-dd HH:mm:ss * * @param date Date对象 * @return 格式化后的时间 */ def formatTime(date: Date): String = &#123; new DateTime(date).toString(TIME_FORMAT) &#125; /** * 格式化日期key, 格式: yyyyMMdd * * @param date : Date对象, Sat Sep 07 03:02:01 CST 2019 * @return yyyyMMdd */ def formatDateKey(date: Date): String = &#123; new DateTime(date).toString(DATE_KEY_FORMAT) &#125; /** * 格式化日期key, 格式: yyyyMMdd * * @param date : time String, yyyy-MM-dd HH:mm:ss * @return yyyyMMdd */ def formatDateKey(date: String): Date = &#123; DATE_KEY_FORMAT.parseDateTime(date).toDate &#125; /** * 格式化日期key, 格式: yyyyMMdd * * @param date : time String, yyyy-MM-dd HH:mm:ss * @return yyyyMMdd */ def dateToTimestamp(date: String): Long = &#123; TIME_FORMAT.parseDateTime(date).getMillis &#125; /** * 格式化日期为0点, 格式: yyyy-MM-dd 00:00:00 * * @param date : time String, yyyy-MM-dd HH:mm:ss * @return yyyy-MM-dd 00:00:00 */ def formatTimeZone(date: String): String = &#123; TIME_FORMAT.parseDateTime(date).toString(&quot;yyyy-MM-dd 00:00:00&quot;) &#125; /** * 时间戳转日期, 格式: yyyy-MM-dd * * @param timestamp : 时间戳, Long * @return yyyy-MM-dd */ def timestampToDate(timestamp: Long): String = &#123; new DateTime(timestamp).toString(DATE_FORMAT) &#125; /** * 时间戳转time, 格式: yyyy-MM-dd HH:mm:ss * * @param timestamp : 时间戳, Long, 13 位 * @return yyyy-MM-dd HH:mm:ss */ def timestampToTime(timestamp: Long): String = &#123; new DateTime(timestamp).toString(TIME_FORMAT) &#125; /** * 时间戳转指定格式time, 格式: 自定义 * * @param timestamp: 时间戳, Long, 13 位 * @param pattern: 格式化格式,如: yyyy-MM-dd * @return 自定义 */ def timestampToTime(timestamp: Long, pattern: String): String = &#123; new DateTime(timestamp).toString(pattern) &#125; /** * 通过时间戳获取小时, 格式: HH * * @param timestamp: 时间戳, Long, 13 位 * @return 小时, Int */ def getHour(timestamp: Long): Int = &#123; new DateTime(timestamp).hourOfDay().getAsString.toInt &#125; /** * 通过时间戳获取分钟, 格式: mm * * @param timestamp: 时间戳, Long, 13 位 * @return 分钟, Int */ def getMinute(timestamp: Long): Int = &#123; new DateTime(timestamp).millisOfSecond().getAsString.toInt &#125; /** * 通过时间戳获取秒, 格式: ss * * @param timestamp: 时间戳, Long, 13 位 * @return 秒, Int */ def getSecond(timestamp: Long): Int = &#123; new DateTime(timestamp).secondOfMinute().getAsString.toInt &#125; /** * 将时间转换为日期格式,格式化到月 * * @param date yyyy-MM-dd HH:mm:ss * @return yyyy-MM */ def formatDateToMonth(date: String): String = &#123; TIME_FORMAT.parseDateTime(date).toString(&quot;yyyy-MM&quot;) &#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://blog.ifan.host/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://blog.ifan.host/tags/Scala/"}]},{"title":"Java ThreadPoolExecutor 队列策略","slug":"language/java/base/threadPoolExecutor-rejected","date":"2022-06-05T08:15:00.000Z","updated":"2023-12-25T05:56:09.173Z","comments":true,"path":"2022/06/05/language/java/base/threadPoolExecutor-rejected/","link":"","permalink":"https://blog.ifan.host/2022/06/05/language/java/base/threadPoolExecutor-rejected/","excerpt":"","text":"错误复现1234567891011@Testpublic void testThreadPool() &#123; BlockingQueue&lt;Runnable&gt; queue = new LinkedBlockingQueue&lt;&gt;(10); ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 1, TimeUnit.MINUTES, queue); for (int i = 0; i &lt; 100; i++) &#123; final int j = i; executor.submit(() -&gt; &#123; System.out.println(&quot;task-&quot; + j); &#125;); &#125;&#125; 1234567891011java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.FutureTask@e50a6f6 rejected from java.util.concurrent.ThreadPoolExecutor@14ec4505[Running, pool size = 10, active threads = 10, queued tasks = 3, completed tasks = 27] at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:112) at tech.valuesimplex.workweixin.TestSdk.testThreadPool(TestSdk.java:37) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) 参数介绍12345678public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler); &#125; 序号参数含义备注1corePoolSize&nbsp;&nbsp;核心线程数量，线程池初始化时设定corePoolSize 大小和 maximumPoolSize 大小一致的话 线程池中的线程将不会空闲、 keepAliveTime 和 timeUnit 就不会再起作用2maximumPoolSize线程池最大线程数（非核心线程）&nbsp;核心线程 和 &nbsp;非核心线程 共同使用线程池、但是核心线程是不会被回收的、回收条件是线程池中的线程数量大于核心线程数3keepAliveTime &nbsp;如果当前线程池中线程数大于 corePoolSize。多余的线程、在等待 keepAliveTime 时间后如果还没有新的线程任务指派给它、它就会被回收4unit &nbsp;&nbsp;等待时间 keepAliveTime 的单位5workQueue &nbsp;等待队列默认 SynchronousQueue 一个没有存储空间的阻塞队列 ，将任务同步交付给工作线程；可以使用无界队 LinkedBlockingQueue；有界队列 ArrayBlockingQueue；以及优先级队列 PriorityBlockingQueue&nbsp;6RejectedExecutionHandler饱和策略（分 4 种）AbortPolicy（默认）：直接抛弃CallerRunsPolicy：用调用者的线程执行任务（始终此种方式，可以避免线程被线程池拒绝的情况）DiscardOldestPolicy：抛弃队列中最久的任务DiscardPolicy：抛弃当前任务 原因分析当线程池里的线程都繁忙的时候，新任务会被提交给阻塞队列保存，当提交给阻塞队列的任务，超出了该队列的最大容量时。线程池就会拒绝接收新任务，随即抛出异常 RejectedExecutionHandler 的四种饱和策略序号策略名称含义备注1AbortPolicy终止策略是默认的饱和策略，当队列满时，会抛出一个 RejectExecutionException 异常，客户可以捕获这个异常，根据需求编写自己的处理代码不是解决问题的根本2DiscardPolicy策略会悄悄抛弃该任务。建议最好不用，水太深3DiscardOldestPolicy策略将会抛弃下一个将要执行的任务，如果此策略配合优先队列 PriorityBlockingQueue，该策略将会抛弃优先级最高的任务4CallerRunsPolicy调用者运行策略，该策略不会抛出异常，不会抛弃任务，而是将任务回退给调用者线程执行（调用 execute 方法的线程），由于任务需要执行一段时间，所以在此期间不能提交任务，从而使工作线程有时间执行正在执行的任务。非常适合我们的业务场景 解决1ThreadPoolExecutor poolExecutor = new ThreadPoolExecutor(5, 10, 1, TimeUnit.MINUTES, queue, new CallerRunsPolicy()); 其他几种线程池序号线程池名称规则问题点1FixedThreadPool允许的请求队列长度为 Integer.MAX_VALUE会堆积大量的请求，从而导致 OOM2SingleThreadPool&nbsp;3CachedThreadPool&nbsp;允许的创建线程数量为 Integer.MAX_VALUE会创建大量的线程，从而导致 OOM4ScheduledThreadPool&nbsp;","categories":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/categories/Java/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://blog.ifan.host/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/tags/Java/"},{"name":"基础知识","slug":"基础知识","permalink":"https://blog.ifan.host/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"Scala json4s 使用","slug":"language/scala/json4s","date":"2022-06-05T08:10:00.000Z","updated":"2023-12-25T05:56:09.178Z","comments":true,"path":"2022/06/05/language/scala/json4s/","link":"","permalink":"https://blog.ifan.host/2022/06/05/language/scala/json4s/","excerpt":"","text":"引入12345&lt;dependency&gt; &lt;groupId&gt;org.json4s&lt;/groupId&gt; &lt;artifactId&gt;json4s-jackson_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;&#123;latestVersion&#125;&lt;/version&gt;&lt;/dependency&gt; 常用的查询方法12345678910111213141516171819202122232425262728293031323334// 引入依赖import org.json4s._import org.json4s.native.JsonMethods._// 转换val json = JsonMethods.parse(&quot;&quot;&quot; &#123; &quot;name&quot;: &quot;ifan&quot;, &quot;age&quot;: 1, &quot;friend&quot;: [ &#123; &quot;name&quot;: &quot;ifan1&quot;, &quot;age&quot;: 1 &#125;, &#123; &quot;name&quot;: &quot;ifan2&quot;, &quot;age&quot;: 2 &#125; ] &#125; &quot;&quot;&quot;)for &#123; JObject(friend) &lt;- json JField(&quot;age&quot;, JInt(age)) &lt;- friend&#125; ( println(age, friend))for &#123; JObject(friend) &lt;- json JField(&quot;name&quot;, JString(name)) &lt;- friend JField(&quot;age&quot;, JInt(age)) &lt;- friend if age == 1&#125; yield (name, age) xpath 模式获取123456789101112131415161718192021222324252627282930import org.json4s.jackson.JsonMethods._import org.json4s.JsonDSL._// 还是上面的Jsonprintln(json \\ &quot;name&quot;)println(json \\ &quot;sex&quot;)println(compact(render(json \\\\ &quot;name&quot;)))// 过滤println(compact(render((json removeField &#123; _ == JField(&quot;name&quot;, JString(&quot;ifan&quot;)) &#125;) \\\\ &quot;name&quot;)))// 查找并返回字段println(json findField &#123; case JField(&quot;name&quot;, _) =&gt; true case _ =&gt; false&#125;)// 过滤字段println(json filterField &#123; case JField(&quot;name&quot;, _) =&gt; true case _ =&gt; false&#125;)// 修改Jsonprintln(json transformField &#123; case JField(&quot;name&quot;, JString(s)) =&gt; (&quot;NAME&quot;, JString(s.toUpperCase))&#125;)// 查看数据println(json.values)// 获取数组元素println((json \\ &quot;friend&quot; )(0))// 获取Int类型的数据println(json \\\\ classOf[JInt]) Utils 方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234package tech.valuesimplex.data.analyse.utils.commonimport org.joda.time.DateTimeimport org.json4s.&#123;Formats, ShortTypeHints&#125;import org.json4s.jackson.Serializationimport org.json4s._import org.json4s.jackson.JsonMethods._import org.json4s.jackson.Serializationimport org.json4s.jackson.Serialization.&#123;read, write&#125;import java.sql.Dateimport java.text.SimpleDateFormatimport scala.collection.mutableimport scala.util.&#123;Success, Try&#125;object JsonParseUtils &#123; implicit val formats: Formats = Serialization.formats(ShortTypeHints(List())) /** * 检验json */ def validateJson(in: String): Boolean = &#123; try &#123; parse(in) true &#125; catch &#123; case _: Exception =&gt; false &#125; &#125; /** * 将对象转换为 jsonString */ def parseObjToJsonString(value: AnyRef): String = &#123; implicit val formats: Formats = DefaultFormats write(value) &#125; /** * 将 jsonString 转换为map */ def parseJsonStringToObj(in: String): Object = &#123; // compact(render(in)) parse(in, useBigDecimalForDouble = true).extract[Object] &#125; /** * 将 jsonString 转换为map * */ def readJsonStringToObj(in: String): Object = &#123; read[Object](in) &#125; /** * 将 jValue 转换为 jsonString */ def parseJValueToJsonString(obj: JValue): String = &#123; compact(render(obj)) &#125; /** * 将 jValue 转换为 jsonString */ def prettyJValueToJsonString(obj: JValue): String = &#123; pretty(render(obj)) &#125; /** * 将 jsonString 转换为 JValue */ def parseJsonStringToJValue(in: String): JValue = &#123; parse(in) &#125; /** * 将 jsonString 转换为 map */ def parseJsonStringToMap(json: String): mutable.Map[String, Any] = &#123; parse(json, useBigDecimalForDouble = true).extract[mutable.Map[String, Any]] &#125; /** * 添加或更新 jsonString 中的字段 */ def upsertKeyValueToJsonString(json: String, key: String, value: Any): String = &#123; val map = parseJsonStringToMap(json).+((key, value)) parseObjToJsonString(map) &#125; /** * 更新 jsonString 的字段 */ def alterKeyValueOfJsonString(json: String, key: String, value: Any): String = &#123; val map = parseJsonStringToMap(json) map(key) = value parseObjToJsonString(map) &#125; /** * 移除jsonString的字段 */ def removeKeyOfJsonString(json: String, key: String): String = &#123; val map = parseJsonStringToMap(json).-=(key) parseObjToJsonString(map) &#125; /** * 合并两个jValue */ def mergeJValue(json1: JValue, json2: JValue): String = &#123; (json1 merge json2).toString &#125; /** * 合并两个json */ def mergeJson(json1: String, json2: String): String = &#123; val map = parseJsonStringToMap(json1).++(parseJsonStringToMap(json2)) parseObjToJsonString(map) &#125; /** * 判断 jsonString 中是否包含某个 key */ def containsKey(in: String, key: String): Boolean = &#123; parse(in).findField(_._1.equals(key)).nonEmpty &#125; /** * 解析 jsonString 中的 String 类型字段 */ def parseValueString(in: String, key: String): String = &#123; (parse(in) \\ key).extractOrElse[String](&quot;&quot;).trim &#125; /** * 解析 jsonString 中的 Int 类型字段 */ def parseValueInt(in: String, key: String): Int = &#123; val numStr = (parse(in) \\ key).extractOrElse[String](&quot;0&quot;) if (numStr.nonEmpty &amp;&amp; isNumeric(numStr)) numStr.toInt else 0 &#125; /** * 解析 jsonString 中的 Long 类型字段 */ def parseValueLong(in: String, key: String): Long = &#123; val numStr = (parse(in) \\ key).extractOrElse[String](&quot;0&quot;) if (numStr.nonEmpty &amp;&amp; isNumeric(numStr)) numStr.toLong else 0L &#125; /** * 解析 jsonString 中的 Double 类型字段 */ def parseValueDouble(in: String, key: String): Double = &#123; val numStr = (parse(in) \\ key).extractOrElse[String](&quot;0&quot;) if (numStr.nonEmpty) numStr.toDouble else 0.0 &#125; /** * 解析 jsonString 中的 Long 类型字段为 time */ def parseValueTime(in: String, key: String): String = &#123; val ts = (parse(in) \\ key).extractOrElse[String](&quot;0&quot;) if (isNumeric(ts)) &#123; val timestamp = ts.toLong ts.length match &#123; case 10 =&gt; timestampToTime(timestamp * 1000) case 13 =&gt; timestampToTime(timestamp) case _ =&gt; getNowTime &#125; &#125; else &#123; ts &#125; &#125; /** * 解析 jsonString 中的 Long 类型字段为 date */ def parseValueDate(in: String, key: String): String = &#123; val ts = (parse(in) \\ key).extractOrElse[String](&quot;0&quot;) if (isNumeric(ts)) &#123; val timestamp = ts.toLong ts.length match &#123; case 10 =&gt; timestampToDate(timestamp * 1000) case 13 =&gt; timestampToDate(timestamp) case _ =&gt; getTodayDate &#125; &#125; else &#123; ts &#125; &#125; /** * 获取 JSONObject 对应 Key 的值, 过滤特殊字符 */ def parseRoleName(json: String, key: String): String = &#123; val roleName = parseValueString(json, key) if (roleName.isEmpty) &quot;&quot; else roleName.replace(&quot;\\\\\\n|\\\\\\r|\\&quot;|\\&#x27;|\\\\\\\\&quot;, &quot;&quot;) &#125; /** * 判断字符串是否是数字 * * @param str * @return */ def isNumeric(str: String): Boolean = &#123; scala.util.Try(str.toLong) match &#123; case Success(_) =&gt; true; case _ =&gt; false &#125; &#125; def timestampToDate(ts: Long): String = &#123; new Date(ts).formatted(&quot;yyyy-MM-dd&quot;) &#125; def timestampToTime(ts: Long): String = &#123; new DateTime(ts).formatted(&quot;yyyy-MM-dd HH:mm:ss&quot;) &#125; def getTodayDate(): String = &#123; new Date(System.currentTimeMillis()).formatted(&quot;yyyy-MM-dd&quot;) &#125; def getNowTime(): String = &#123; DateTime.now().formatted(&quot;yyyy-MM-dd HH:mm:ss&quot;) &#125;&#125;","categories":[{"name":"Scala","slug":"Scala","permalink":"https://blog.ifan.host/categories/Scala/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://blog.ifan.host/tags/Scala/"}]},{"title":"GoLang gin validator 字段校验","slug":"language/golang/libs/validator","date":"2022-06-04T06:25:00.000Z","updated":"2023-12-25T05:56:09.173Z","comments":true,"path":"2022/06/04/language/golang/libs/validator/","link":"","permalink":"https://blog.ifan.host/2022/06/04/language/golang/libs/validator/","excerpt":"","text":"1234// 下载go get github.com/go-playground/validator/v10// 使用import &quot;github.com/go-playground/validator/v10&quot; 简单使用12345678910111213141516171819202122232425type User struct &#123; Name string `binding:&quot;required&quot; validate:&quot;isName,min=6,max=10&quot;` Age uint8 `binding:&quot;required&quot; validate:&quot;gte=1,lte=10&quot;` Sex string `binding:&quot;required&quot; validate:&quot;oneof=female male&quot;` Email string `binding:&quot;required&quot; validate:&quot;email&quot;`&#125;// 自定义验证规则断言func isName(fl validator.FieldLevel) bool &#123; if name, ok := fl.Field().Interface().(string); ok &#123; return len(strings.Split(name, &quot; &quot;)) &gt;= 2 &#125; return true&#125;func main() &#123; validate := validator.New() user := User&#123;Name: &quot;ifan lw&quot;, Age: 11&#125; validate.RegisterValidation(&quot;isName&quot;, isName) err := validate.Struct(user) if err != nil &#123; fmt.Println(&quot;校验错误&quot;, err) &#125;&#125; 字段校验规则12345678910111213141516171819202122232425262728293031323334353637383940len：length 等于，长度相等max：小于等于min：大于等于eq：等于，字符串相等ne：不等于gt：大于gte：大于等于lt：小于lte：小于等于，例如lte=10；oneof：值中的一个，例如oneof=1 2支持时间范围的比较lte 时间 RegTime time.Time `validate:&quot;lte&quot;` 小于等于当前时间跨字段约束eqfield=ConfirmPasswordeqcsfield=InnerStructField.Field字符串规则contains=：包含参数子串containsany：包含参数中任意的 UNICODE 字符containsrune：包含参数表示的 rune 字符excludes：不包含参数子串excludesall：不包含参数中任意的 UNICODE 字符excludesrune：不包含参数表示的 rune 字符startswith：以参数子串为前缀endswith：以参数子串为后缀使用unqiue来指定唯一性约束，对不同类型的处理如下：对于数组和切片，unique约束没有重复的元素；对于map，unique约束没有重复的值；对于元素类型为结构体的切片，unique约束结构体对象的某个字段不重复，通过unqiue=name特殊规则-：跳过该字段，不检验；|：使用多个约束，只需要满足其中一个，例如rgb|rgba；required：字段必须设置，不能为默认值；omitempty：如果字段未设置，则忽略它。","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"Sqlite 学习","slug":"database/sqlite/action","date":"2022-06-04T01:00:00.000Z","updated":"2023-12-25T05:56:09.169Z","comments":true,"path":"2022/06/04/database/sqlite/action/","link":"","permalink":"https://blog.ifan.host/2022/06/04/database/sqlite/action/","excerpt":"","text":"简介优点 SQLite 是自给自足的，这意味着不需要任何外部的依赖。 SQLite 是无服务器的、零配置的，这意味着不需要安装或管理。 SQLite 事务是完全兼容 ACID 的，允许从多个进程或线程安全访问。 SQLite 是非常小的，是轻量级的，完全配置时小于 400KiB，省略可选功能配置时小于 250KiB。 SQLite 支持 SQL92（SQL2）标准的大多数查询语言的功能。 一个完整的 SQLite 数据库是存储在一个单一的跨平台的磁盘文件。 SQLite 使用 ANSI-C 编写的，并提供了简单和易于使用的 API。 SQLite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。 局限 特性 描述 RIGHT OUTER JOIN 只实现了 LEFT OUTER JOIN。 FULL OUTER JOIN 只实现了 LEFT OUTER JOIN。 ALTER TABLE 支持 RENAME TABLE 和 ALTER TABLE 的 ADD COLUMN variants 命令，不支持 DROP COLUMN、ALTER COLUMN、ADD CONSTRAINT。 Trigger 支持 支持 FOR EACH ROW 触发器，但不支持 FOR EACH STATEMENT 触发器。 VIEWs 在 SQLite 中，视图是只读的。您不可以在视图上执行 DELETE、INSERT 或 UPDATE 语句。 GRANT 和 REVOKE 可以应用的唯一的访问权限是底层操作系统的正常文件访问权限。 安装Sqlite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。 一般，Linux 和 Mac 上会预安装 sqlite。如果没有安装，可以在官方下载地址下载合适安装版本，自行安装。 语法 这里不会详细列举所有 SQL 语法，仅列举 SQLite 除标准 SQL 以外的，一些自身特殊的 SQL 语法。 扩展阅读：标准 SQL 基本语法 大小写敏感SQLite 是不区分大小写的，但也有一些命令是大小写敏感的，比如 GLOB 和 glob 在 SQLite 的语句中有不同的含义。 注释12345-- 单行注释/* 多行注释1 多行注释2 */ 创建数据库如下，创建一个名为 test 的数据库： 1234$ sqlite3 test.dbSQLite version 3.7.17 2013-05-20 00:56:22Enter &quot;.help&quot; for instructionsEnter SQL statements terminated with a &quot;;&quot; 创建表 123456create table contacts ( id integer primary key, -- 设置主键 name text not null collate nocase, -- 不能为空 排序时 忽略大小写 phone text not null default &#x27;unknown&#x27;, -- 设置默认值 uniqe (name, phone) --); 查看数据库1234sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db 退出数据库1sqlite&gt; .quit 附加数据库假设这样一种情况，当在同一时间有多个数据库可用，您想使用其中的任何一个。 SQLite 的 ATTACH DATABASE 语句是用来选择一个特定的数据库，使用该命令后，所有的 SQLite 语句将在附加的数据库下执行。 123456sqlite&gt; ATTACH DATABASE &#x27;test.db&#x27; AS &#x27;test&#x27;;sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db2 test /root/test.db 🔔 注意：数据库名 main 和 temp 被保留用于主数据库和存储临时表及其他临时数据对象的数据库。这两个数据库名称可用于每个数据库连接，且不应该被用于附加，否则将得到一个警告消息。 分离数据库SQLite 的 DETACH DATABASE 语句是用来把命名数据库从一个数据库连接分离和游离出来，连接是之前使用 ATTACH 语句附加的。 12345678910sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db2 test /root/test.dbsqlite&gt; DETACH DATABASE &#x27;test&#x27;;sqlite&gt; .databasesseq name file--- --------------- ----------------------------------------------------------0 main /root/test.db 备份数据库如下，备份 test 数据库到 /home/test.sql 1sqlite3 test.db .dump &gt; /home/test.sql 恢复数据库如下，根据 /home/test.sql 恢复 test 数据库 1sqlite3 test.db &lt; test.sql 数据类型SQLite 使用一个更普遍的动态类型系统。在 SQLite 中，值的数据类型与值本身是相关的，而不是与它的容器相关。 SQLite 存储类每个存储在 SQLite 数据库中的值都具有以下存储类之一： 存储类 描述 NULL 值是一个 NULL 值。 INTEGER 值是一个带符号的整数，根据值的大小存储在 1、2、3、4、6 或 8 字节中。 REAL 值是一个浮点值，存储为 8 字节的 IEEE 浮点数字。 TEXT 值是一个文本字符串，使用数据库编码（UTF-8、UTF-16BE 或 UTF-16LE）存储。 BLOB 值是一个 blob 数据，完全根据它的输入存储。 SQLite 的存储类稍微比数据类型更普遍。INTEGER 存储类，例如，包含 6 种不同的不同长度的整数数据类型。 SQLite 亲和(Affinity)类型SQLite 支持列的亲和类型概念。任何列仍然可以存储任何类型的数据，当数据插入时，该字段的数据将会优先采用亲缘类型作为该值的存储方式。SQLite 目前的版本支持以下五种亲缘类型： 亲和类型 描述 TEXT 数值型数据在被插入之前，需要先被转换为文本格式，之后再插入到目标字段中。 NUMERIC 当文本数据被插入到亲缘性为 NUMERIC 的字段中时，如果转换操作不会导致数据信息丢失以及完全可逆，那么 SQLite 就会将该文本数据转换为 INTEGER 或 REAL 类型的数据，如果转换失败，SQLite 仍会以 TEXT 方式存储该数据。对于 NULL 或 BLOB 类型的新数据，SQLite 将不做任何转换，直接以 NULL 或 BLOB 的方式存储该数据。需要额外说明的是，对于浮点格式的常量文本，如”30000.0”，如果该值可以转换为 INTEGER 同时又不会丢失数值信息，那么 SQLite 就会将其转换为 INTEGER 的存储方式。 INTEGER 对于亲缘类型为 INTEGER 的字段，其规则等同于 NUMERIC，唯一差别是在执行 CAST 表达式时。 REAL 其规则基本等同于 NUMERIC，唯一的差别是不会将”30000.0”这样的文本数据转换为 INTEGER 存储方式。 NONE 不做任何的转换，直接以该数据所属的数据类型进行存储。 SQLite 亲和类型(Affinity)及类型名称下表列出了当创建 SQLite3 表时可使用的各种数据类型名称，同时也显示了相应的亲和类型： 数据类型 亲和类型 INT, INTEGER, TINYINT, SMALLINT, MEDIUMINT, BIGINT, UNSIGNED BIG INT, INT2, INT8 INTEGER CHARACTER(20), VARCHAR(255), VARYING CHARACTER(255), NCHAR(55), NATIVE CHARACTER(70), NVARCHAR(100), TEXT, CLOB TEXT BLOB, no datatype specified NONE REAL, DOUBLE, DOUBLE PRECISION, FLOAT REAL NUMERIC, DECIMAL(10,5), BOOLEAN, DATE, DATETIME NUMERIC Boolean 数据类型SQLite 没有单独的 Boolean 存储类。相反，布尔值被存储为整数 0（false）和 1（true）。 Date 与 Time 数据类型SQLite 没有一个单独的用于存储日期和&#x2F;或时间的存储类，但 SQLite 能够把日期和时间存储为 TEXT、REAL 或 INTEGER 值。 存储类 日期格式 TEXT 格式为 “YYYY-MM-DD HH:MM:SS.SSS” 的日期。 REAL 从公元前 4714 年 11 月 24 日格林尼治时间的正午开始算起的天数。 INTEGER 从 1970-01-01 00:00:00 UTC 算起的秒数。 您可以以任何上述格式来存储日期和时间，并且可以使用内置的日期和时间函数来自由转换不同格式。 SQLite 命令控制命令 命令 描述 .backup ?DB? FILE 备份 DB 数据库（默认是 “main”）到 FILE 文件。 .bail ON|OFF 发生错误后停止。默认为 OFF。 .databases 列出数据库的名称及其所依附的文件。 .dump ?TABLE? 以 SQL 文本格式转储数据库。如果指定了 TABLE 表，则只转储匹配 LIKE 模式的 TABLE 表。 .echo ON|OFF 开启或关闭 echo 命令。 .exit 退出 SQLite 提示符。 .explain ON|OFF 开启或关闭适合于 EXPLAIN 的输出模式。如果没有带参数，则为 EXPLAIN on，及开启 EXPLAIN。 .header(s) ON|OFF 开启或关闭头部显示。 .help 显示消息。 .import FILE TABLE 导入来自 FILE 文件的数据到 TABLE 表中。 .indices ?TABLE? 显示所有索引的名称。如果指定了 TABLE 表，则只显示匹配 LIKE 模式的 TABLE 表的索引。 .load FILE ?ENTRY? 加载一个扩展库。 .log FILE|off 开启或关闭日志。FILE 文件可以是 stderr（标准错误）&#x2F;stdout（标准输出）。 .mode MODE 设置输出模式，MODE 可以是下列之一：csv 逗号分隔的值column 左对齐的列html HTML 的 代码insert TABLE 表的 SQL 插入（insert）语句line 每行一个值list 由 .separator 字符串分隔的值tabs 由 Tab 分隔的值tcl TCL 列表元素 .nullvalue STRING 在 NULL 值的地方输出 STRING 字符串。 .output FILENAME 发送输出到 FILENAME 文件。 .output stdout 发送输出到屏幕。 .print STRING… 逐字地输出 STRING 字符串。 .prompt MAIN CONTINUE 替换标准提示符。 .quit 退出 SQLite 提示符。 .read FILENAME 执行 FILENAME 文件中的 SQL。 .schema ?TABLE? 显示 CREATE 语句。如果指定了 TABLE 表，则只显示匹配 LIKE 模式的 TABLE 表。 .separator STRING 改变输出模式和 .import 所使用的分隔符。 .show 显示各种设置的当前值。 .stats ON|OFF 开启或关闭统计。 .tables ?PATTERN? 列出匹配 LIKE 模式的表的名称。 .timeout MS 尝试打开锁定的表 MS 毫秒。 .width NUM NUM 为 “column” 模式设置列宽度。 .timer ON|OFF 开启或关闭 CPU 定时器。 查询命令1. 获取最后插入的自动增量的值1select last_insert_rowid(); 2. 查看表信息12345678910111213.tables --查看当前数据库所有表 .tables table_name --查看当前数据库指定表.schema --查看当前数据库所有表的建表(CREATE)语句 .schema table_name --查看指定数据表的建表语句 select * from sqlite_master from; --查看所有表结构及索引信息 select * from sqlite_master where type=&#x27;table&#x27;; --查看所有表结构信息 select name from sqlite_master where type=&#x27;table&#x27;; --对于表来说，name字段指表名，查询所有表select * from sqlite_master where type=&#x27;table&#x27; and name=&#x27;table_name&#x27;; --查看指定表结构信息 select * from sqlite_master where type=&#x27;index&#x27;; --查看所有表索引信息，查询所有索引 select name from sqlite_master where type=&#x27;table&#x27;; --对于索引来说，name字段指索引名select * from sqlite_master where type=&#x27;index&#x27; and name=&#x27;table_name&#x27;; --查看指定表索引信息pragma table_info (&#x27;table_name&#x27;) --查看指定表所有字段信息，类似于msyql：desc table_nameselect typeof(&#x27;column&#x27;) from table_name; --查看指定表字段【column】类型，括号内可不输引号 3. 清空表数据123456789delete from [tablename]//1. 将表名为tablename的自增量置0update sqlite_sequence set seq = 0 where name = &#x27;tablename&#x27;//2. 将表名为tablename的记录删除delete from sqlite_sequence where name = &#x27;tablename&#x27;//3. 将sqlite_sequence表清空数据delete from sqlite_sequence 数据导入导出导入1234567891011121314151617181920212223# 导出sql文件.output sql_file_name.dump.output stdout# 导出CSV文件.output file.csv.separatorselect * from table_name;.output stdout# 导出csv文件2.output file.csv.mode csvselect * from table_name;.output stdout# 方法1 和 2 的区别时 2会自动换行 字段值并将其加上双引号，而列模式不加# 直接导出数据sqlite3 test.db .dump &gt; test.sql# 创建数据库的两种方法sqlite3 test.db &lt; test.sql# 加上.exit 是因为不想进入 sqlite shell 命令界面sqlite3 -init test.sql test.db .exit 导出12345678910# sql 文件.read file.sql# 有分隔符的文件# 1. 查看当前指定的分隔符.show # separator -&gt; &quot;|&quot;# 2. 指定不同的分隔符.separator# 3. 导入文件.import [file] [table] 函数转字符串、时间1234SELECT date(&#x27;now&#x27;); --&gt;结果：2018-05-05SELECT time(&#x27;now&#x27;); --&gt;结果：06:55:38SELECT datetime(&#x27;now&#x27;); --&gt;结果：2018-05-05 06:55:38SELECT strftime(&#x27;%Y-%m-%d %H:%M:%S&#x27;, &#x27;now&#x27;); --&gt;结果：2018-05-05 06:55:38 转时间戳12select strftime(&#x27;%s&#x27;,&#x27;now&#x27;); --&gt;结果：1525502284select strftime(&#x27;%s&#x27;,&#x27;2018-05-05&#x27;); --&gt;结果：1525478400 时间戳转时间、字符串1select datetime(1525502284, &#x27;unixepoch&#x27;, &#x27;localtime&#x27;); --&gt;结果：2018-05-05 14:38:04 扩展：SQLite 的五个时间函数： date(timestring, modifier, modifier, …)：以 YYYY-MM-DD 格式返回日期 time(timestring, modifier, modifier, …)：以 HH:MM:SS 格式返回时间 datetime(timestring, modifier, modifier, …)：以 YYYY-MM-DD HH:MM:SS 格式返回日期时间 julianday(format, timestring, modifier, modifier, ..)：返回从格林尼治时间的公元前 4714 年 11 月 24 日正午算起的天数 strftime(format, timestring, modifier, modifier, ..)：根据第一个参数指定的格式字符串返回格式化的日期 讲道理其他四个函数都可以用 strftime() 函数来表示：1234date(…) –&gt; strftime(&#x27;%Y-%m-%d&#x27;,…)time(…) –&gt; strftime(&#x27;%H:%M:%S&#x27;,…)datetime(…) –&gt; strftime(&#x27;%Y-%m-%d %H:%M:%S&#x27;,…)julianday(…) –&gt; strftime(&#x27;%J&#x27;,…) 时间格式化日期时间字符串（timestring） 序号 日期时间字符串 实例 1 YYYY-MM-DD 2018-05-05 2 YYYY-MM-DD HH:MM 2018-05-05 12:10 3 YYYY-MM-DD HH:MM:SS.SSS 2018-05-05 15:39:20.100 4 MM-DD-YYYY HH:MM 05-05-2018 12:10 5 HH:MM 同理 6 YYYY-MM-DDTHH:MM 同理 7 HH:MM:SS 同理 8 YYYYMMDD HHMMSS 同理 9 now 2018-05-05 15:39:20 10 DDDDDDDDDD 1525478400（时间戳） strftime() 函数，格式化串（format）： 符号 描述 %d 一月中的第几天 01-31 %f 小数形式的秒，SS.SSSS %H 小时 00-24 %j 一年中的第几天 01-366 %J Julian Day Numbers %m 月份 01-12 %M 分钟 00-59 %s 从 1970-01-01 日开始计算的秒数 %S 秒 00-59 %w 星期，0-6，0 是星期天 %W 一年中的第几周 00-53 %Y 年份 0000-9999 %% % 百分号 修饰符（modifier）： 序号 符号 作用 １ [+-]NNN years 增加 &#x2F; 减去指定数值的年 ２ [+-]NNN months 增加 &#x2F; 减去指定数值的月 ３ [+-]NNN days 增加 &#x2F; 减去指定数值的天 ４ [+-]NNN hours 增加 &#x2F; 减去指定数值的小时 ５ [+-]NNN minutes 增加 &#x2F; 减去指定数值的分钟 ６ [+-]NNN.NNNN seconds 增加 &#x2F; 减去指定数值的秒 7 start of year 当前日期的开始年 8 start of month 当前日期的开始月 9 start of day 当前日期的开始日 11 weekday N 表示返回下一个星期是 N 的日期和时间 12 unixepoch 用于将日期解释为 UNIX 时间 (即：自 1970-01-01 以来的秒数，也就是时间戳) 13 localtime 表示返回本地时间 14 utc 表示返回 UTC（世界统一时间）时间 重点：修饰符运用实例1234567891011121314151617SELECT datetime(&#x27;now&#x27;); --&gt;结果：2018-05-05 08:10:26（和本机时间可能不一致，时区问题请看使用&#x27;localtime&#x27;修饰符后的变化）SELECT datetime(&#x27;now&#x27;,&#x27;1 years&#x27;);--&gt;结果：2019-05-05 08:10:26SELECT datetime(&#x27;now&#x27;,&#x27;1 months&#x27;);--&gt;结果：2018-06-05 08:10:26SELECT datetime(&#x27;now&#x27;,&#x27;1 days&#x27;);--&gt;结果：2018-05-06 08:10:26SELECT datetime(&#x27;now&#x27;,&#x27;1 hours&#x27;);--&gt;结果：2018-05-05 09:10:26SELECT datetime(&#x27;now&#x27;,&#x27;1 minutes&#x27;);--&gt;结果：2018-05-05 08:11:26SELECT datetime(&#x27;now&#x27;,&#x27;1 seconds&#x27;);--&gt;结果：2018-05-05 08:10:27SELECT datetime(&#x27;now&#x27;,&#x27;start of year&#x27;);--&gt;结果：2018-01-01 00:00:00SELECT datetime(&#x27;now&#x27;,&#x27;start of month&#x27;);--&gt;结果：2018-05-01 00:00:00SELECT datetime(&#x27;now&#x27;,&#x27;start of day&#x27;);--&gt;结果：2018-05-05 00:00:00SELECT datetime(&#x27;now&#x27;,&#x27;weekday 0&#x27;);--&gt;结果：2018-05-06 08:10:26（解释：2018-05-05是周六，“weekday 0”表示返回下周的周日(系统默认以周日为一周的开始0)，明天06号是周日，所以，返回了2018-05-06）SELECT datetime(&#x27;1525478400&#x27;,&#x27;unixepoch&#x27;);--&gt;结果：2018-05-05 00:00:00（unixepoch一般用于解释时间戳）SELECT datetime(&#x27;now&#x27;,&#x27;localtime&#x27;);--&gt;结果：2018-05-05 16:10:26（你会发现这个时间比没使用&#x27;localtime&#x27;参数的时间多了8个小时，因为中国是东八时区）SELECT datetime(&#x27;now&#x27;,&#x27;utc&#x27;);--&gt;结果：2018-05-05 00:10:26（时区问题）","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Sqlite","slug":"数据库/Sqlite","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Sqlite/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Sqlite","slug":"Sqlite","permalink":"https://blog.ifan.host/tags/Sqlite/"}]},{"title":"GoLang 基础组件 template","slug":"language/golang/base/template","date":"2022-06-04T01:00:00.000Z","updated":"2023-12-25T05:56:09.171Z","comments":true,"path":"2022/06/04/language/golang/base/template/","link":"","permalink":"https://blog.ifan.host/2022/06/04/language/golang/base/template/","excerpt":"","text":"简单使用1234567891011121314151617181920212223242526272829303132type WebPage struct &#123; Name string Href string Icon string Desc string Next []WebPage Category []WebPage&#125;var webpageTpl = `[webpage]&#123;&#123;range $index, $n := .webpage&#125;&#125; &#123;&#123;$index&#125;&#125;.&#123;&#123;$.Name&#125;&#125; &#123;&#123;.Desc&#125;&#125;&#123;&#123;end&#125;&#125;`data := []WebPage&#123; &#123; Name: &quot;w1&quot;, Next: []WebPage&#123; &#123;Name: &quot;w12&quot;&#125; &#125; &#125;, &#123; Name: &quot;w2&quot;, Next: []WebPage&#123; &#123;Name: &quot;w22&quot;&#125; &#125; &#125;&#125;tmpl, err := template.New(&quot;main&quot;).Parse(webpageTpl)var buf bytes.Buffertmpl.Execute(&amp;buf, data)fmt.Println(buf.String()) 函数12345678910111213141516171819202122232425262728293031323334var webpageTpl = `[webpage]&#123;&#123;range $index, $n := .webpage&#125;&#125; &#123;&#123; if isStart $index &#125;&#125; &#123;&#123;$index&#125;&#125;.&#123;&#123;$.Name&#125;&#125; &#123;&#123;.Desc&#125;&#125; &#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;`data := []WebPage&#123; &#123; Name: &quot;w1&quot;, Next: []WebPage&#123; &#123;Name: &quot;w12&quot;&#125; &#125; &#125;, &#123; Name: &quot;w2&quot;, Next: []WebPage&#123; &#123;Name: &quot;w22&quot;&#125; &#125; &#125;&#125;tmpl, err := template.New(&quot;main&quot;).Funcs( template.FuncMap&#123; &quot;isStart&quot;: func (i int) bool &#123; return i == 0 &#125; &#125;).Parse(webpageTpl)var buf bytes.Buffertmpl.Execute(&amp;buf, data)fmt.Println(buf.String()) 12345&#123;&#123;FuncName&#125;&#125; // FuncName()&#123;&#123;FuncName &quot;Arg1&quot; &quot;Arg2&quot;&#125;&#125; // FuncName(&quot;Arg1&quot;, &quot;Arg2&quot;)&#123;&#123;.Arg | FuncName&#125;&#125; // FuncName(.Arg)&#123;&#123;call FuncName Arg1&#125;&#125; // FuncName(Arg1) 基础语法1234567&#123;&#123;/* 注释 */&#125;&#125;// 输出当前对象值&#123;&#123;.&#125;&#125;&#123;&#123;.Name&#125;&#125;// 定义变量&#123;&#123;$x := &quot;OK&quot;&#125;&#125;&#123;&#123;$x := pipeline&#125;&#125; 条件判断123&#123;&#123;if pipeline&#125;&#125; T1 &#123;&#123;end&#125;&#125;&#123;&#123;if pipeline&#125;&#125; T1 &#123;&#123;else&#125;&#125; T0 &#123;&#123;end&#125;&#125;&#123;&#123;if pipeline&#125;&#125; T1 &#123;&#123;else if pipeline&#125;&#125; T0 &#123;&#123;end&#125;&#125; 循环遍历123&#123;&#123;range $k, $v := .Var&#125;&#125; &#123;&#123;$k&#125;&#125; =&gt; &#123;&#123;$v&#125;&#125; &#123;&#123;end&#125;&#125;&#123;&#123;range .Var&#125;&#125; &#123;&#123;.&#125;&#125; &#123;&#123;end&#125;&#125;&#123;&#123;range pipeline&#125;&#125; T1 &#123;&#123;else&#125;&#125; T0 &#123;&#123;end&#125;&#125; 输出字符串12345&#123;&#123;&quot;put&quot; | printf &quot;%s%s&quot; &quot;out&quot; | printf &quot;%q&quot;&#125;&#125; // printf(&quot;%q&quot;, printf(&quot;%s%s&quot;, &quot;out&quot;, &quot;put&quot;))printprintfprintln 分别等价于fmt包中的Sprint、Sprintf、Sprintln 条件判断12345678910eq arg1 arg2 // arg1 == arg2ne arg1 arg2 // arg1 != arg2lt arg1 arg2 // arg1 &lt; arg2le arg1 arg2 // arg1 &lt;= arg2gt arg1 arg2 // arg1 &gt; arg2ge arg1 arg2 // arg1 &gt;= arg2// 如果传入多个参数的话 比较的规则是// arg1==arg2 || arg1==arg3 || arg1==arg4 ... 其他内置的一些函数12345678910111213141516171819202122232425len 返回参数的length。index 对可索引对象进行索引取值。第一个参数是索引对象，后面的参数是索引位。 &quot;index x 1 2 3&quot;代表的是x[1][2][3]。 可索引对象包括map、slice、array。call 显式调用函数。第一个参数必须是函数类型，且不是template中的函数，而是外部函数。 例如一个struct中的某个字段是func类型的。 &quot;call .X.Y 1 2&quot;表示调用dot.X.Y(1, 2)，Y必须是func类型，函数参数是1和2。 函数必须只能有一个或2个返回值，如果有第二个返回值，则必须为error类型。and 返回第一个为空的参数或最后一个参数。可以有任意多个参数。 and x y等价于if x then y else xnot 布尔取反。只能一个参数。or 返回第一个不为空的参数或最后一个参数。可以有任意多个参数。 &quot;or x y&quot;等价于&quot;if x then x else y&quot;。 不转义需要将不转义的变量进行类型转换 1234type CSStype HTMLtype JStype URL 模板继承12&#123;&#123;template &quot;name&quot;&#125;&#125;&#123;&#123;template &quot;name&quot; pipeline&#125;&#125; 1234567891011func main() &#123; t1 := template.New(&quot;test1&quot;) tmpl, _ := t1.Parse(`&#123;&#123;- define &quot;T1&quot;&#125;&#125;ONE &#123;&#123;println .&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;- define &quot;T2&quot;&#125;&#125;TWO &#123;&#123;println .&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;- define &quot;T3&quot;&#125;&#125;&#123;&#123;template &quot;T1&quot;&#125;&#125;&#123;&#123;template &quot;T2&quot; &quot;haha&quot;&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;- template &quot;T3&quot; -&#125;&#125;`) _ = tmpl.Execute(os.Stdout, &quot;hello world&quot;)&#125;// ONE &lt;nil&gt;// TWO haha 12// 先查找 template 为 T1 的模板，没有则定义一个&#123;&#123;block &quot;T1&quot; .&#125;&#125; one &#123;&#123;end&#125;&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"Mysql 配置文件","slug":"database/mysql/conf","date":"2022-06-03T10:00:00.000Z","updated":"2023-12-25T05:56:09.163Z","comments":true,"path":"2022/06/03/database/mysql/conf/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/mysql/conf/","excerpt":"","text":"服务器配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697[mysqld]datadir=/data/datafilesocket=/var/lib/mysql/mysql.socklog-error=/data/log/mysqld.logpid-file=/var/run/mysqld/mysqld.pidcharacter_set_server=utf8mb4bind-address = 0.0.0.0default_time_zone = &#x27;+8:00&#x27;default_storage_engine=InnoDB#二进制配置server-id = 1log-bin = /data/log/mysql-bin.loglog-bin-index =/data/log/binlog.indexlog_bin_trust_function_creators=1expire_logs_days=7#InnoDB存储数据字典、内部数据结构的缓冲池，16MB已经足够大了。innodb_additional_mem_pool_size = 16M#InnoDB用于缓存数据、索引、锁、插入缓冲、数据字典等#如果是专用的DB服务器，且以InnoDB引擎为主的场景，通常可设置物理内存的60%#如果是非专用DB服务器，可以先尝试设置成内存的1/4innodb_buffer_pool_size = 4G#InnoDB的log buffer，通常设置为 64MBinnodb_log_buffer_size = 64M#InnoDB redo log大小，通常设置256MBinnodb_log_file_size = 256M#InnoDB redo log文件组，通常设置为 2innodb_log_files_in_group = 2# 表名忽略大小写lower_case_table_name = 1#共享表空间:某一个数据库的所有的表数据，索引文件全部放在一个文件中，默认这个共享表空间的文件路径在data目录下。默认的文件名为:ibdata1 初始化为10M。#独占表空间:每一个表都将会生成以独立的文件方式来进行存储，每一个表都有一个.frm表描述文件，还有一个.ibd文件。其中这个文件包括了单独一个表的数据内容以及索引内容，默认情况下它的存储位置也是在表的位置之中。#设置参数为1启用InnoDB的独立表空间模式，便于管理innodb_file_per_table = 1#InnoDB共享表空间初始化大小，默认是 10MB，改成 1GB，并且自动扩展innodb_data_file_path = ibdata1:1G:autoextend#设置临时表空间最大4Ginnodb_temp_data_file_path=ibtmp1:500M:autoextend:max:4096M#启用InnoDB的status file，便于管理员查看以及监控innodb_status_file = 1#当设置为0，该模式速度最快，但不太安全，mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。#当设置为1，该模式是最安全的，但也是最慢的一种方式。在mysqld 服务崩溃或者服务器主机crash的情况下，binary log 只有可能丢失最多一个语句或者一个事务。#当设置为2，该模式速度较快，也比0安全，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。innodb_flush_log_at_trx_commit = 1#设置事务隔离级别为 READ-COMMITED，提高事务效率，通常都满足事务一致性要求#transaction_isolation = READ-COMMITTED#max_connections：针对所有的账号所有的客户端并行连接到MYSQL服务的最大并行连接数。简单说是指MYSQL服务能够同时接受的最大并行连接数。#max_user_connections : 针对某一个账号的所有客户端并行连接到MYSQL服务的最大并行连接数。简单说是指同一个账号能够同时连接到mysql服务的最大连接数。设置为0表示不限制。#max_connect_errors：针对某一个IP主机连接中断与mysql服务连接的次数，如果超过这个值，这个IP主机将会阻止从这个IP主机发送出去的连接请求。遇到这种情况，需执行flush hosts。#执行flush host或者 mysqladmin flush-hosts，其目的是为了清空host cache里的信息。可适当加大，防止频繁连接错误后，前端host被mysql拒绝掉#在 show global 里有个系统状态Max_used_connections,它是指从这次mysql服务启动到现在，同一时刻并行连接数的最大值。它不是指当前的连接情况，而是一个比较值。如果在过去某一个时刻，MYSQL服务同时有1000个请求连接过来，而之后再也没有出现这么大的并发请求时，则Max_used_connections=1000.请注意与show variables 里的max_user_connections的区别。#Max_used_connections / max_connections * 100% ≈ 85%# show variables like &#x27;%max_connections%&#x27;;# show status like &#x27;%max_used_connections%&#x27;;max_connections=10000max_connect_errors=10000max_user_connections=400#设置临时表最大值，这是每次连接都会分配，不宜设置过大 max_heap_table_size 和 tmp_table_size 要设置一样大max_heap_table_size = 100Mtmp_table_size = 100M#每个连接都会分配的一些排序、连接等缓冲，一般设置为 2MBsort_buffer_size = 2Mjoin_buffer_size = 2Mread_buffer_size = 2Mread_rnd_buffer_size = 2M#建议关闭query cache，有些时候对性能反而是一种损害query_cache_size = 0#如果是以InnoDB引擎为主的DB，专用于MyISAM引擎的 key_buffer_size 可以设置较小，8MB 已足够#如果是以MyISAM引擎为主，可设置较大，但不能超过4Gkey_buffer_size = 8M#设置连接超时阀值，如果前端程序采用短连接，建议缩短这2个值，如果前端程序采用长连接，可直接注释掉这两个选项，是用默认配置(8小时)# show variables like &quot;wait_timeout&quot;;# show variables like &quot;interactive_timeout&quot;;wait_timeout = 60 # 非交互的连接interactive_timeout = 6000 # 交互的连接#InnoDB使用后台线程处理数据页上读写I/0请求的数量,允许值的范围是1-64#假设CPU是2颗4核的，且数据库读操作比写操作多，可设置#innodb_read_io_threads=5#innodb_write_io_threads=3#通过show engine innodb status的FILE I/O选项可查看到线程分配#设置慢查询阀值，单位为秒long_query_time = 120slow_query_log=1 #开启mysql慢sql的日志log_output=table,File #日志输出会写表，也会写日志文件，为了便于程序去统计，所以最好写表slow_query_log_file=/data/log/mysql-slow.log##针对log_queries_not_using_indexes开启后，记录慢sql的频次、每分钟记录的条数#log_throttle_queries_not_using_indexes = 5##作为从库时生效,从库复制中如何有慢sql也将被记录#log_slow_slave_statements = 1##检查未使用到索引的sql#log_queries_not_using_indexes = 1#快速预热缓冲池innodb_buffer_pool_dump_at_shutdown=1innodb_buffer_pool_load_at_startup=1#打印deadlock日志innodb_print_all_deadlocks=1 配置介绍123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321[client]# 服务端口号，默认 3306port = 3306# socket 文件socket = /var/lib/mysql/mysql.sock[mysqld]# GENERAL# -------------------------------------------------------------------------------# socket 文件socket = /var/lib/mysql/mysql.sock# PID 文件pid_file = /var/lib/mysql/mysql.pid# 启动 mysql 服务进程的用户user = mysql# 服务端口号，默认 3306port = 3306# 默认时区default_time_zone = &#x27;+8:00&#x27;# Mysql 服务 ID，单点服务时没必要设置server-id = 1# 事务隔离级别，默认为可重复读（REPEATABLE-READ）。（此级别下可能参数很多间隙锁，影响性能，但是修改又影响主从复制及灾难恢复，建议还是修改代码逻辑吧）# 隔离级别可选项目：READ-UNCOMMITTED READ-COMMITTED REPEATABLE-READ SERIALIZABLEtransaction_isolation = REPEATABLE-READ# 目录配置# -------------------------------------------------------------------------------# mysql 安装根目录basedir = /usr/local/mysql-5.7.21# mysql 数据文件所在目录datadir = /var/lib/mysql# 临时目录 比如 load data infile 会用到，一般都是使用/tmptmpdir = /tmp# 数据库引擎配置# -------------------------------------------------------------------------------# mysql 5.1 之后，默认引擎是 InnoDBdefault_storage_engine = InnoDB# 内存临时表默认引擎，默认 InnoDBdefault_tmp_storage_engine = InnoDB# mysql 5.7 新增特性，磁盘临时表默认引擎，默认 InnoDBinternal_tmp_disk_storage_engine = InnoDB# 字符集配置# -------------------------------------------------------------------------------# 数据库默认字符集，主流字符集支持一些特殊表情符号（特殊表情符占用 4 个字节）character_set_server = utf8mb4# 数据库字符集对应一些排序等规则，注意要和 character_set_server 对应collation-server = utf8mb4_0900_ai_ci# 设置 client 连接 mysql 时的字符集，防止乱码# init_connect=&#x27;SET NAMES utf8&#x27;# 是否对 sql 语句大小写敏感，默认值为 0，1 表示不敏感lower_case_table_names = 1# 数据库连接配置# -------------------------------------------------------------------------------# 最大连接数，可设最大值 16384，一般考虑根据同时在线人数设置一个比较综合的数字，鉴于该数值增大并不太消耗系统资源，建议直接设 10000# 如果在访问时经常出现 Too Many Connections 的错误提示，则需要增大该参数值max_connections = 10000# 默认值 100，最大错误连接数，如果有超出该参数值个数的中断错误连接，则该主机将被禁止连接。如需对该主机进行解禁，执行：FLUSH HOST# 考虑高并发场景下的容错，建议加大。max_connect_errors = 10000# MySQL 打开的文件描述符限制，默认最小 1024;# 当 open_files_limit 没有被配置的时候，比较 max_connections\\*5 和 ulimit -n 的值，哪个大用哪个，# 当 open_file_limit 被配置的时候，比较 open_files_limit 和 max_connections\\*5 的值，哪个大用哪个。# 注意：仍然可能出现报错信息 Can&#x27;t create a new thread；此时观察系统 cat /proc/mysql 进程号/limits，观察进程 ulimit 限制情况# 过小的话，考虑修改系统配置表，/etc/security/limits.conf 和 /etc/security/limits.d/90-nproc.confopen_files_limit = 65535# 超时配置# -------------------------------------------------------------------------------# MySQL 默认的 wait_timeout 值为 8 个小时，interactive_timeout 参数需要同时配置才能生效# MySQL 连接闲置超过一定时间后(单位：秒，此处为 1800 秒)将会被强行关闭interactive_timeout = 1800wait_timeout = 1800# 在 MySQL 暂时停止响应新请求之前的短时间内多少个请求可以被存在堆栈中# 官方建议 back_log = 50 + (max_connections / 5)，封顶数为 900back_log = 900# 数据库数据交换配置# -------------------------------------------------------------------------------# 该参数限制服务器端，接受的数据包大小，如果有 BLOB 子段，建议增大此值，避免写入或者更新出错。有 BLOB 子段，建议改为 1024Mmax_allowed_packet = 128M# 内存、cache 与 buffer 设置# 内存临时表的最大值，默认 16M，此处设置成 64Mtmp_table_size = 64M# 用户创建的内存表的大小，默认 16M，往往和 tmp_table_size 一起设置，限制用户临时表大小。# 超限的话，MySQL 就会自动地把它转化为基于磁盘的 MyISAM 表，存储在指定的 tmpdir 目录下，增大 IO 压力，建议内存大，增大该数值。max_heap_table_size = 64M# 表示这个 mysql 版本是否支持查询缓存。ps：SHOW STATUS LIKE &#x27;qcache%&#x27;，与缓存相关的状态变量。# have_query_cache# 这个系统变量控制着查询缓存功能的开启和关闭，0 表示关闭，1 表示打开，2 表示只要 select 中明确指定 SQL_CACHE 才缓存。# 看业务场景决定是否使用缓存，不使用，下面就不用配置了。# Mysql8 不支持query_cache_type = 0# 默认值 1M，优点是查询缓存可以极大的提高服务器速度，如果你有大量的相同的查询并且很少修改表。# 缺点：在你表经常变化的情况下或者如果你的查询原文每次都不同，查询缓存也许引起性能下降而不是性能提升。# Mysql8 不支持query_cache_size = 64M# 只有小于此设定值的结果才会被缓冲，保护查询缓冲，防止一个极大的结果集将其他所有的查询结果都覆盖。query_cache_limit = 2M# 每个被缓存的结果集要占用的最小内存，默认值 4kb，一般不怎么调整。# 如果 Qcache_free_blocks 值过大，可能是 query_cache_min_res_unit 值过大，应该调小些# query_cache_min_res_unit 的估计值：(query_cache_size - Qcache_free_memory) / Qcache_queries_in_cachequery_cache_min_res_unit = 4kb# 在一个事务中 binlog 为了记录 SQL 状态所持有的 cache 大小# 如果你经常使用大的、多声明的事务，你可以增加此值来获取更大的性能。# 所有从事务来的状态都将被缓冲在 binlog 缓冲中然后在提交后一次性写入到 binlog 中# 如果事务比此值大，会使用磁盘上的临时文件来替代。# 此缓冲在每个连接的事务第一次更新状态时被创建binlog_cache_size = 1M# 日志配置# -------------------------------------------------------------------------------# 日志文件相关设置，一般只开启三种日志，错误日志，慢查询日志，二进制日志。普通查询日志不开启。# 普通查询日志，默认值 off，不开启general_log = 0# 普通查询日志存放地址general_log_file = /usr/local/mysql-5.7.21/log/mysql-general.log# 全局动态变量，默认 3，范围：1 ～ 3# 表示错误日志记录的信息，1：只记录 error 信息；2：记录 error 和 warnings 信息；3：记录 error、warnings 和普通的 notes 信息。log_error_verbosity = 2# 错误日志文件地址log_error = /usr/local/mysql-5.7.21/log/mysql-error.log# 开启慢查询slow_query_log = 1# 开启慢查询时间，此处为 1 秒，达到此值才记录数据long_query_time = 3# 检索行数达到此数值，才记录慢查询日志中min_examined_row_limit = 100# mysql 5.6.5 新增，用来表示每分钟允许记录到 slow log 的且未使用索引的 SQL 语句次数，默认值为 0，不限制。log_throttle_queries_not_using_indexes = 0# 慢查询日志文件地址slow_query_log_file = /var/log/mysql/mysql-slow.log# 开启记录没有使用索引查询语句log-queries-not-using-indexes = 1# 开启二进制日志log_bin = /usr/local/mysql-5.7.21/log/mysql-bin.log# mysql 清除过期日志的时间，默认值 0，不自动清理，而是使用滚动循环的方式。expire_logs_days = 0# 如果二进制日志写入的内容超出给定值，日志就会发生滚动。你不能将该变量设置为大于 1GB 或小于 4096 字节。 默认值是 1GB。max_binlog_size = 1000M# binlog 的格式也有三种：STATEMENT，ROW，MIXED。mysql 5.7.7 后，默认值从 MIXED 改为 ROW# 关于 binlog 日志格式问题，请查阅网络资料binlog_format = row# 默认值 N=1，使 binlog 在每 N 次 binlog 写入后与硬盘同步，ps：1 最慢# sync_binlog = 1# MyISAM 引擎配置# -------------------------------------------------------------------------------# 指定索引缓冲区的大小，为 MYISAM 数据表开启供线程共享的索引缓存，对 INNODB 引擎无效。相当影响 MyISAM 的性能。# 不要将其设置大于你可用内存的 30%，因为一部分内存同样被 OS 用来缓冲行数据# 甚至在你并不使用 MyISAM 表的情况下，你也需要仍旧设置起 8-64M 内存由于它同样会被内部临时磁盘表使用。# 默认值 8M，建议值：对于内存在 4GB 左右的服务器该参数可设置为 256M 或 384M。注意：该参数值设置的过大反而会是服务器整体效率降低！key_buffer_size = 64M# 为每个扫描 MyISAM 的线程分配参数设置的内存大小缓冲区。# 默认值 128kb，建议值：16G 内存建议 1M，4G：128kb 或者 256kb 吧# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 128kb*连接数；极端情况 128kb*maxconnectiosns，会超级大，所以要考虑日常平均连接数。# 一般不需要太关心该数值，稍微增大就可以了，read_buffer_size = 262144# 支持任何存储引擎# MySQL 的随机读缓冲区大小，适当增大，可以提高性能。# 默认值 256kb；建议值：得参考连接数，16G 内存，有人推荐 8M# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 128kb*连接数；极端情况 128kb*maxconnectiosns，会超级大，所以要考虑日常平均连接数。read_rnd_buffer_size = 1M# order by 或 group by 时用到# 支持所有引擎，innodb 和 myisam 有自己的 innodb_sort_buffer_size 和 myisam_sort_buffer_size 设置# 默认值 256kb；建议值：得参考连接数，16G 内存，有人推荐 8M。# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 1M*连接数；极端情况 1M*maxconnectiosns，会超级大。所以要考虑日常平均连接数。sort_buffer_size = 1M# 此缓冲被使用来优化全联合(full JOINs 不带索引的联合)# 类似的联合在极大多数情况下有非常糟糕的性能表现，但是将此值设大能够减轻性能影响。# 通过 “Select_full_join” 状态变量查看全联合的数量# 注意，该缓冲区是每个连接独占的，所以总缓冲区大小为 1M*连接数；极端情况 1M*maxconnectiosns，会超级大。所以要考虑日常平均连接数。# 默认值 256kb;建议值：16G 内存，设置 8M。join_buffer_size = 1M# 缓存 linux 文件描述符信息，加快数据文件打开速度# 它影响 myisam 表的打开关闭，但是不影响 innodb 表的打开关闭。# 默认值 2000，建议值：根据状态变量 Opened_tables 去设定table_open_cache = 2000# 缓存表定义的相关信息，加快读取表信息速度# 默认值 1400，最大值 2000，建议值：基本不改。table_definition_cache = 1400# 该参数是 myssql 5.6 后引入的，目的是提高并发。# 默认值 1，建议值：cpu 核数，并且&lt;=16table_open_cache_instances = 2# 当客户端断开之后，服务器处理此客户的线程将会缓存起来以响应下一个客户而不是销毁。可重用，减小了系统开销。# 默认值为 9，建议值：两种取值方式，方式一，根据物理内存，1G —&gt; 8；2G —&gt; 16； 3G —&gt; 32； &gt;3G —&gt; 64；# 方式二，根据 show status like &#x27;threads%&#x27;，查看 Threads_connected 值。thread_cache_size = 16# 默认值 256k，建议值：16/32G 内存，512kb，其他一般不改变，如果报错：Thread stack overrun，就增大看看，# 注意，每个线程分配内存空间，所以总内存空间。。。你懂得。thread_stack = 512k# InnoDB 引擎配置# -------------------------------------------------------------------------------# 说明：该参数可以提升扩展性和刷脏页性能。# 默认值 1，建议值：4-8；并且必须小于 innodb_buffer_pool_instancesinnodb_page_cleaners = 4# 说明：一般 8k 和 16k 中选择，8k 的话，cpu 消耗小些，selcet 效率高一点，一般不用改# 默认值：16k；建议值：不改，innodb_page_size = 16384# 说明：InnoDB 使用一个缓冲池来保存索引和原始数据，不像 MyISAM。这里你设置越大，你在存取表里面数据时所需要的磁盘 I/O 越少。# 在一个独立使用的数据库服务器上，你可以设置这个变量到服务器物理内存大小的 60%-80%# 注意别设置的过大，会导致 system 的 swap 空间被占用，导致操作系统变慢，从而减低 sql 查询的效率# 默认值：128M，建议值：物理内存的 60%-80%innodb_buffer_pool_size = 512M# 说明：只有当设置 innodb_buffer_pool_size 值大于 1G 时才有意义，小于 1G，instances 默认为 1，大于 1G，instances 默认为 8# 但是网络上有评价，最佳性能，每个实例至少 1G 大小。# 默认值：1 或 8，建议值：innodb_buffer_pool_size/innodb_buffer_pool_instances &gt;= 1Ginnodb_buffer_pool_instances = 1# 说明：mysql 5.7 新特性，defines the chunk size for online InnoDB buffer pool resizing operations。# 实际缓冲区大小必须为 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances*倍数，取略大于 innodb_buffer_pool_size# 默认值 128M，建议值：默认值就好，乱改反而容易出问题，它会影响实际 buffer pool 大小。innodb_buffer_pool_chunk_size = 128M# 在启动时把热数据加载到内存。默认值为 on，不修改innodb_buffer_pool_load_at_startup = 1# 在关闭时把热数据 dump 到本地磁盘。默认值为 on，不修改innodb_buffer_pool_dump_at_shutdown = 1# 说明：影响 Innodb 缓冲区的刷新算法，建议从小到大配置，直到 zero free pages；innodb_lru_scan_depth \\* innodb_buffer_pool_instances defines the amount of work performed by the page cleaner thread each second。# 默认值 1024，建议值： 未知innodb_lru_scan_depth = 1024# 说明：事务等待获取资源等待的最长时间，单位为秒，看具体业务情况，一般默认值就好# 默认值：50，建议值：看业务。innodb_lock_wait_timeout = 60# 说明：设置了 Mysql 后台任务（例如页刷新和 merge dadta from buffer pool）每秒 io 操作的上限。# 默认值：200，建议值：方法一，单盘 sata 设 100，sas10，raid10 设 200，ssd 设 2000，fushion-io 设 50000；方法二，通过测试工具获得磁盘 io 性能后，设置 IOPS 数值/2。innodb_io_capacity = 2000# 说明：该参数是所有缓冲区线程 io 操作的总上限。# 默认值：innodb_io_capacity 的两倍。建议值：例如用 iometer 测试后的 iops 数值就好innodb_io_capacity_max = 4000# 说明：控制着 innodb 数据文件及 redo log 的打开、刷写模式，三种模式：fdatasync(默认)，O_DSYNC，O_DIRECT# fdatasync：数据文件，buffer pool-&gt;os buffer-&gt;磁盘；日志文件，buffer pool-&gt;os buffer-&gt;磁盘；# O_DSYNC： 数据文件，buffer pool-&gt;os buffer-&gt;磁盘；日志文件，buffer pool-&gt;磁盘；# O_DIRECT： 数据文件，buffer pool-&gt;磁盘； 日志文件，buffer pool-&gt;os buffer-&gt;磁盘；# 默认值为空，建议值：使用 SAN 或者 raid，建议用 O_DIRECT，不懂测试的话，默认生产上使用 O_DIRECTinnodb_flush_method = O_DIRECT# 说明：mysql5.7 之后默认开启，意思是，每张表一个独立表空间。# 默认值 1，开启innodb_file_per_table = 1# 说明：The path where InnoDB creates undo tablespaces。通常等于 undo log 文件的存放目录。# 默认值 ./;自行设置innodb_undo_directory = /usr/local/mysql-5.7.21/log# 说明：The number of undo tablespaces used by InnoDB 等于 undo log 文件数量。5.7.21 后开始弃用# 默认值为 0，建议默认值就好，不用调整了。innodb_undo_tablespaces = 0# 说明：定义 undo 使用的回滚段数量。5.7.19 后弃用# 默认值 128，建议不动，以后弃用了。innodb_undo_logs = 128# 说明：5.7.5 后开始使用，在线收缩 undo log 使用的空间。# 默认值：关闭，建议值：开启innodb_undo_log_truncate = 1# 说明：结合 innodb_undo_log_truncate，实现 undo 空间收缩功能# 默认值：1G，建议值，不改。innodb_max_undo_log_size = 1G# 说明：重作日志文件的存放目录innodb_log_group_home_dir = /usr/local/mysql-5.7.21/log# 说明：日志文件的大小# 默认值：48M，建议值：根据你系统的磁盘空间和日志增长情况调整大小innodb_log_file_size = 128M# 说明：日志组中的文件数量，mysql 以循环方式写入日志# 默认值 2，建议值：根据你系统的磁盘空间和日志增长情况调整大小innodb_log_files_in_group = 3# 此参数确定些日志文件所用的内存大小，以 M 为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。MySQL 开发人员建议设置为 1－8M 之间innodb_log_buffer_size = 16M# 说明：可以控制 log 从系统 buffer 刷入磁盘文件的刷新频率，增大可减轻系统负荷# 默认值是 1；建议值不改。系统性能一般够用。innodb_flush_log_at_timeout = 1# 说明：参数可设为 0，1，2；# 参数 0：表示每秒将 log buffer 内容刷新到系统 buffer 中，再调用系统 flush 操作写入磁盘文件。# 参数 1：表示每次事物提交，将 log buffer 内容刷新到系统 buffer 中，再调用系统 flush 操作写入磁盘文件。# 参数 2：表示每次事物提交，将 log buffer 内容刷新到系统 buffer 中，隔 1 秒后再调用系统 flush 操作写入磁盘文件。innodb_flush_log_at_trx_commit = 1# 说明：限制 Innodb 能打开的表的数据，如果库里的表特别多的情况，请增加这个。# 值默认是 2000，建议值：参考数据库表总数再进行调整，一般够用不用调整。innodb_open_files = 8192# innodb 处理 io 读写的后台并发线程数量，根据 cpu 核来确认，取值范围：1-64# 默认值：4，建议值：与逻辑 cpu 数量的一半保持一致。innodb_read_io_threads = 4innodb_write_io_threads = 4# 默认设置为 0，表示不限制并发数，这里推荐设置为 0，更好去发挥 CPU 多核处理能力，提高并发量innodb_thread_concurrency = 0# 默认值为 4，建议不变。InnoDB 中的清除操作是一类定期回收无用数据的操作。mysql 5.5 之后，支持多线程清除操作。innodb_purge_threads = 4# 说明：mysql 缓冲区分为 new blocks 和 old blocks；此参数表示 old blocks 占比；# 默认值：37，建议值，一般不动innodb_old_blocks_pct = 37# 说明：新数据被载入缓冲池，进入 old pages 链区，当 1 秒后再次访问，则提升进入 new pages 链区。# 默认值：1000innodb_old_blocks_time=1000# 说明：开启异步 io，可以提高并发性，默认开启。# 默认值为 1，建议不动innodb_use_native_aio = 1# 说明：默认为空，使用 data 目录，一般不改。innodb_data_home_dir=/usr/local/mysql-5.7.21/data# 说明：Defines the name，size，and attributes of InnoDB system tablespace data files。# 默认值，不指定，默认为 ibdata1：12M：autoextendinnodb_data_file_path = ibdata1：12M：autoextend# 说明：设置了 InnoDB 存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，除非你的数据对象及其多，否则一般默认不改。# innodb_additional_mem_pool_size = 16M# 说明：The crash recovery mode。只有紧急情况需要恢复数据的时候，才改为大于 1-6 之间数值，含义查下官网。# 默认值为 0；#innodb_force_recovery = 0# 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。# 缺点：所有远程主机连接授权都要使用IP地址方式，因为只认得ip地址了。# skip_name_resolve = 0# 默认值为off,timestamp列会自动更新为当前时间，设置为on|1，timestamp列的值就要显式更新explicit_defaults_for_timestamp = 1[mysqldump]# quick 选项强制 mysqldump 从服务器查询取得记录直接输出而不是取得所有记录后将它们缓存到内存中quickmax_allowed_packet = 16M[mysql]# mysql 命令行工具不使用自动补全功能，建议还是改为# no-auto-rehashauto-rehash# socket 文件socket = /var/lib/mysql/mysql.sock 参考文档 Mysql配置文件&#x2F;etc&#x2F;my.cnf解析","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Redis Linux 编译安装","slug":"database/redis/install","date":"2022-06-03T10:00:00.000Z","updated":"2023-12-25T05:56:09.167Z","comments":true,"path":"2022/06/03/database/redis/install/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/install/","excerpt":"","text":"编译安装1. 下载123wget https://download.redis.io/releases/redis-6.2.7.tar.gztar -zvxf redis-6.2.7.tar.gzyum install gcc 2. 编译安装1make &amp;&amp; make install PREFIX=/opt/redis 3. 启动1mkdir /opt/redis/conf &amp;&amp; cp redis.conf /opt/redis/conf &amp;&amp; cd /opt/redis &amp;&amp; ./bin/redis-server ./conf/redis.conf 4. 主从模式主节点 12345678bind 0.0.0.0 # 允许所有主机联机port 6379 # 工作端口protected-mode no # 关闭保护模式daemonize yes # 开启守护进程logfile /opt/software/redis/logs/6379.log # 指定日志文件目录dir /opt/software/redis/data/6379 # 指定本地数据存放位置appendonly yes # 开启AOF持久化功能requirepass ifan@2022 # 设置 redis 连接密码 从节点 123456# 在上面配置的基础上添加port 6380 # 工作端口logfile /opt/software/redis/logs/6380.log # 指定日志文件目录dir /opt/software/redis/data/6380 # 指定本地数据存放位置masterauth ifan@2022 # 设置slave 服务连接 master 的密码replicaof 127.0.0.1 6379 # 指定要同步的Master节点IP和端口 5. 哨兵模式主从节点不用关掉，哨兵模式只是一个监控和故障转移的服务 123456789101112pidfile &quot;/opt/redis/logs/sentinel/sentinel.pid&quot;protected-mode noport 26379daemonize yeslogfile &quot;/opt/redis/logs/sentinel/sentinel.log&quot;dir &quot;/opt/redis/data/sentinel/26379&quot;sentinel monitor mymaster 127.0.0.1 6380 3sentinel auth-pass mymaster ifan@2022sentinel down-after-milliseconds mymaster 30000sentinel parallel-syncs mymaster 1sentinel failover-timeout mymaster 180000sentinel deny-scripts-reconfig yes 6. 集群模式123456protected-mode no # 关闭保护模式port 7000 # 每个实例都要修改，redis监听端口，daemonize yes # 后台模式cluster-enabled yes # 开启群集功能cluster-config-file nodes-7000.conf # 群集名称文件设置，每个实例都要修改cluster-node-timeout 15000 # 集群超时时间设置 12# 修改配置文件监听的端口后，启动每个实例redis-server redis.conf 12# 创建节点，cluster-replicas是从节点的个数，每个主节点，配置一个从节点redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 --cluster-replicas 1 后记总体来说，主从复制模式是只是防止单节点挂掉之后，整个缓存系统无法使用的情况（虽然从节点只能读，但是也比之前的单节点方案好了很多），而且如果系统只有读的需求的话，可以分散主节点的压力；哨兵模式是对之前主从模式的增强，可以自动化的切换故障节点。集群模式是主从模式的另一种横向扩充，实现了容量的增大，顺便集成了哨兵模式。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 哨兵模式","slug":"database/redis/sentinel","date":"2022-06-03T09:05:00.000Z","updated":"2023-12-25T05:56:09.169Z","comments":true,"path":"2022/06/03/database/redis/sentinel/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/sentinel/","excerpt":"","text":"哨兵简介Redis 哨兵（Sentinel）是 Redis 的高可用性（Hight Availability）解决方案：由一个或多个 Sentinel 实例组成的 Sentinel 系统可以监视任意多个主服务器，以及这些主服务器的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。 Sentinel 本质上是一个运行在特殊状模式下的 Redis 服务器。 Sentinel 的主要功能如下： 监控（Monitoring） - Sentinel 不断检查主从服务器是否正常在工作。 通知（Notification） - Sentinel 可以通过一个 api 来通知系统管理员或者另外的应用程序，被监控的 Redis 实例有一些问题。 自动故障转移（Automatic Failover） - 如果一个主服务器下线，Sentinel 会开始自动故障转移：把一个从节点提升为主节点，并重新配置其他的从节点使用新的主节点，使用 Redis 服务的应用程序在连接的时候也被通知新的地址。 配置提供者（Configuration provider） - Sentinel 给客户端的服务发现提供来源：对于一个给定的服务，客户端连接到 Sentinels 来寻找当前主节点的地址。当故障转移发生的时候，Sentinel 将报告新的地址。 监控检测服务器状态 Sentinel 向 Redis 服务器发送 PING 命令，检查其状态。 默认情况下，每个 Sentinel 节点会以 每秒一次 的频率对 Redis 节点和 其它 的 Sentinel 节点发送 PING 命令，并通过节点的 回复 来判断节点是否在线。 主观下线：主观下线 适用于所有 主节点 和 从节点。如果在 down-after-milliseconds 毫秒内，Sentinel 没有收到 目标节点 的有效回复，则会判定 该节点 为 主观下线。 客观下线：客观下线 只适用于 主节点。当 Sentinel 将一个主服务器判断为主管下线后，为了确认这个主服务器是否真的下线，会向同样监视这一主服务器的其他 Sentinel 询问，看它们是否也认为主服务器已经下线。当足够数量的 Sentinel 认为主服务器已下线，就判定其为客观下线，并对其执行故障转移操作。 Sentinel 节点通过 sentinel is-master-down-by-addr 命令，向其它 Sentinel 节点询问对该节点的 状态判断。 获取服务器信息 Sentinel 向主服务器发送 INFO 命令，获取主服务器及它的从服务器信息。 获取主服务器信息 - Sentinel 默认会以每十秒一次的频率，通过命令连接向被监视的主服务器发送 INFO 命令，并通过分析 INFO 命令的回复来获取主服务器的当前信息。 主服务自身信息：包括 run_id 域记录的服务器运行 ID，以及 role 域记录的服务器角色 主服务的从服务器信息：包括 IP 地址和端口号 获取从服务器信息 - 当 Sentinel 发现主服务器有新的从服务器出现时，Sentinel 除了会为这个新的从服务器创建相应的实例结构之外，Sentinel 还会创建连接到从服务器的命令连接和订阅连接。 命令 PING: 返回 PONG 。 SENTINEL masters: 列出所有被监视的主服务器，以及这些主服务器的当前状态； SENTINEL slaves &lt;master name&gt;: 列出给定主服务器的所有从服务器，以及这些从服务器的当前状态； SENTINEL get-master-addr-by-name &lt;master name&gt;: 返回给定名字的主服务器的 IP 地址和端口号。 如果这个主服务器正在执行故障转移操作， 或者针对这个主服务器的故障转移操作已经完成， 那么这个命令返回新的主服务器的 IP 地址和端口号； SENTINEL reset &lt;pattern&gt;: 重置所有名字和给定模式 pattern 相匹配的主服务器。 pattern 参数是一个 Glob 风格的模式。 重置操作清楚主服务器目前的所有状态， 包括正在执行中的故障转移， 并移除目前已经发现和关联的， 主服务器的所有从服务器和 Sentinel ； SENTINEL failover &lt;master name&gt;: 当主服务器失效时， 在不询问其他 Sentinel 意见的情况下， 强制开始一次自动故障迁移。 （不过发起故障转移的 Sentinel 会向其他 Sentinel 发送一个新的配置，其他 Sentinel 会根据这个配置进行相应的更新） SENTINEL MONITOR &lt;name&gt; &lt;ip&gt; &lt;port&gt; &lt;quorum&gt;: 这个命令告诉sentinel去监听一个新的master SENTINEL REMOVE &lt;name&gt;: 命令sentinel放弃对某个master的监听 SENTINEL SET &lt;name&gt; &lt;option&gt; &lt;value&gt;: 这个命令很像Redis的CONFIG SET命令，用来改变指定master的配置。支持多个。例如以下实例：SENTINEL SET objects-cache-master down-after-milliseconds 1000只要是配置文件中存在的配置项，都可以用SENTINEL SET命令来设置。这个还可以用来设置master的属性，比如说quorum(票数)，而不需要先删除master，再重新添加master。例如：SENTINEL SET objects-cache-master quorum 5 通知对于每个与 Sentinel 连接的服务器，Sentinel 既会向服务器的 __sentinel__:hello 频道发送消息，也会订阅服务器的 __sentinel__:hello 频道的消息。 向服务器发送消息在默认情况下，Sentinel 会以每两秒一次的频率，通过命令向所有被监视的主服务器和从服务器发送以下格式的命令。 1PUBLISH __sentinel__:hello &quot;&lt;s_ip&gt;,&lt;s_port&gt;,&lt;s_runid&gt;,&lt;s_epoch&gt;,&lt;m_name&gt;,&lt;m_ip&gt;,&lt;m_port&gt;,&lt;m_epoch&gt;&quot; 这条命令向服务器的 __sentinel__:hello 频道发送一条消息。 接收服务器的消息当 Sentinel 与一个主服务器或从服务器建立起订阅连接后，Sentinel 就会通过订阅连接，向服务器发送以下命令：SUBSCRIBE __sentinel__:hello。 Sentinel 对 __sentinel__:hello 频道的订阅会一直持续到 Sentinel 与服务器断开连接为止。 选举 Leader Redis Sentinel 系统选举 Leader 的算法是 Raft 的实现。 当一个主服务器被判断为客观下线时，监视这个下线主服务器的各个 Sentinel 会进行协商，选举出一个领头的 Sentinel，并由领头 Sentinel 对下线主服务器执行故障转移操作。 所有在线 Sentinel 都有资格被选为 Leader。 每个 Sentinel 节点都需要 定期执行 以下任务： 每个 Sentinel 以 每秒钟 一次的频率，向它所知的 主服务器、从服务器 以及其他 Sentinel 实例 发送一个 PING 命令。 如果一个 实例（instance）距离 最后一次 有效回复 PING 命令的时间超过 down-after-milliseconds 所指定的值，那么这个实例会被 Sentinel 标记为 主观下线。 如果一个 主服务器 被标记为 主观下线，那么正在 监视 这个 主服务器 的所有 Sentinel 节点，要以 每秒一次 的频率确认 主服务器 的确进入了 主观下线 状态。 如果一个 主服务器 被标记为 主观下线，并且有 足够数量 的 Sentinel（至少要达到 配置文件 指定的数量）在指定的 时间范围 内同意这一判断，那么这个 主服务器 被标记为 客观下线。 在一般情况下， 每个 Sentinel 会以每 10 秒一次的频率，向它已知的所有 主服务器 和 从服务器 发送 INFO 命令。当一个 主服务器 被 Sentinel 标记为 客观下线 时，Sentinel 向 下线主服务器 的所有 从服务器 发送 INFO 命令的频率，会从 10 秒一次改为 每秒一次。 Sentinel 和其他 Sentinel 协商 主节点 的状态，如果 主节点 处于 SDOWN 状态，则投票自动选出新的 主节点。将剩余的 从节点 指向 新的主节点 进行 数据复制。 当没有足够数量的 Sentinel 同意 主服务器 下线时， 主服务器 的 客观下线状态 就会被移除。当 主服务器 重新向 Sentinel 的 PING 命令返回 有效回复 时，主服务器 的 主观下线状态 就会被移除。 注意：一个有效的 PING 回复可以是：+PONG、-LOADING 或者 -MASTERDOWN。如果 服务器 返回除以上三种回复之外的其他回复，又或者在 指定时间 内没有回复 PING 命令， 那么 Sentinel 认为服务器返回的回复 无效（non-valid）。 六、故障转移在选举产生出 Sentinel Leader 后，Sentinel Leader 将对已下线的主服务器执行故障转移操作。操作含以下三个步骤： （一）选出新的主服务器 故障转移第一步，是 Sentinel Leader 在已下线主服务属下的所有从服务器中，挑选一个状态良好、数据完整的从服务器。然后，向这个从服务器发送 SLAVEOF no one 命令，将其转换为主服务器。 Sentinel Leader 如何选出新的主服务器： 删除列表中所有处于下线或断线状态的从服务器。 删除列表中所有最近五秒没有回复过 Sentinel Leader 的 INFO 命令的从服务器。 删除所有与已下线主服务器连接断开超过 down-after-milliseconds * 10 毫秒的从服务器（down-after-milliseconds 指定了判断主服务器下线所需的时间）。 之后， Sentinel Leader 先选出优先级最高的从服务器；如果优先级一样高，再选择复制偏移量最大的从服务器；如果结果还不唯一，则选出运行 ID 最小的从服务器。 （二）修改从服务器的复制目标 选出新的主服务器后，Sentinel Leader 会向所有从服务器发送 SLAVEOF 命令，让它们去复制新的主服务器。 （三）将旧的主服务器变为从服务器 Sentinel Leader 将旧的主服务器标记为从服务器。当旧的主服务器重新上线，Sentinel 会向它发送 SLAVEOF 命令，让其成为从服务器。 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# Example sentinel.conf# 关闭安全模式protected-mode no# 哨兵模式使用的端口port 26379# 后台启动daemonize yes# 后台启动后存放的PIDpidfile /var/run/redis-sentinel.pid# 日志文件logfile &quot;&quot;# 指定 Sentinel 使用的端口和IP，不需要同时使用。# sentinel announce-ip &lt;ip&gt;# sentinel announce-port &lt;port&gt;# 工作目录dir /tmp# 哨兵 sentinel 监控的 redis 主节点 ip port# master-name 可以自己命名的主节点名字 只能由字母 A-Z 数字0-9 这三个字符&quot;.-_&quot; 组成# quorum 配置多少个 sentinel 哨兵统一认为 master主节点失联, 那么这时候客观上认为主节点失联了sentinel monitor mymaster 127.0.0.1 6379 2# 当Redia实例中开启了 requirepass foobared 授权密码 这样所有的连接Redis 实例的客户端都要提供密码# 设置哨兵 sentinel 连接主从密码, 注意必须为主从设置一样的验证密码# sentinel auth-pass mymaster MySUPER--secret-0123passw0rd# 同上，高版本使用了用户名密码的方式# sentinel auth-user &lt;master-name&gt; &lt;username&gt;# 指定多少毫秒之后, 主节点没有应答哨兵 sentinel 此时 哨兵主管上认为节点下线 默认 30ssentinel down-after-milliseconds mymaster 30000# ACL LOG## ACL日志认证失败的最大存储长度，方还在内存里面acllog-max-len 128# 使用外部的ACL日志认证文件# aclfile /etc/redis/sentinel-users.acl# Sentinel 链接使用的密码# requirepass &lt;password&gt;# sentinel sentinel-user &lt;username&gt;# sentinel sentinel-pass &lt;password&gt;# 这个配置项指定了发生 failover 主备切换时最多有多少个slave同时对新的master 进行同步# 这个数字越小 完成 failover 所需时间就越长# 但是如果这个数字越大, 就意味着越多的 slave 因为 replication而不可用# 可以通过这个值设置为1 来保证每次只有一个 slave 处于不能处理命令的请求状态sentinel parallel-syncs mymaster 1# 故障转移超时时间 failover-timeout 可以用在以下这些方面# 1 同一个 sentinel 对同一个master两次 failover 之间的间隔时间# 2 一个 slave 从一个错误的 master 那里同步数据开始计算时间, 直到 slave 被纠正为正确的 master 那里同步数据时# 3 当想要取消一个正在进行的 failover 所需要的时间# 4 当进行 failover 时, 配置所有 slaves 指定新的 master 所需要的最大时间. 不过, 即使过了这个超时, slave 依然会被正确的配置为指向 master 但是就不按 paralle1- syncs 所配置规来了sentinel failover-timeout mymaster 180000# SCRIPTS EXECUTION# 配置当某一件事情发生时, 所需要执行的脚本, 可以通过脚本通知来通知管理员, 例如当系统不能正常发邮件通知相关人员# 对于脚本的运行结果有以下规则# 若脚本执行后返回1, 那么该脚本稍后将会被再次执行, 重复次数默认为 10# 若脚本执行后返回2, 或者比2更高的一个返回值, 脚本将不会重复执行# 如果脚本在执行过程中由于收到系统中断信号被终止了, 则同返回值1时的行为相同# 一个脚本的最大执行时间为60s, 如果超过了这个时间, 脚本将会被一个SIGKILL信号终止, 之后重新执行 # sentinel notification-script mymaster /var/redis/notify.sh# CLIENTS RECONFIGURATION SCRIPT## sentinel 重新配置# sentinel client-reconfig-script &lt;master-name&gt; &lt;script-path&gt;# sentinel client-reconfig-script mymaster /var/redis/reconfig.sh# SECURITY## 避免脚本重置sentinel deny-scripts-reconfig yes# REDIS COMMANDS RENAMING# sentinel 命令重命名# SENTINEL rename-command mymaster CONFIG GUESSME# SENTINEL rename-command mymaster CONFIG CONFIG# HOSTNAMES SUPPORTSENTINEL resolve-hostnames noSENTINEL announce-hostnames no","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 模块","slug":"database/redis/module","date":"2022-06-03T08:50:00.000Z","updated":"2023-12-25T05:56:09.168Z","comments":true,"path":"2022/06/03/database/redis/module/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/module/","excerpt":"","text":"官方提供的模块RedisBloom支持布隆过滤模块 GitHub 使用1docker run -p 6379:6379 --name redis-redisbloom redislabs/rebloom:latest 1redis-server --loadmodule /path/to/redisbloom.so 命令12345678910111213# 添加元素BF.ADD key valuebf.madd key value value...# 判断元素是否存在BF.EXISTS key valuebf.mexists key value value...# 创建一个过滤器 # one-more-filter: 键# 0.0001: 期望错误率，期望错误率越低，需要的空间就越大。默认为0.01# 1000000: 初始容量，当实际元素的数量超过这个初始化容量时，误判率上升。默认为100bf.reserve one-more-filter 0.0001 1000000 CuckooFilter布谷鸟过滤器：Cuckoo hash算法的哈希函数是成对的（具体的实现可以根据需求设计），每一个元素都有两个哈希函数用来分别映射到两个位置，其中一个是记录的位置，另一个是备用位置，这个备用位置是处理碰撞时用的。 GitHub RedisjsonRedisJSON是一个Redis模块，它实现了JSON数据交换标准ECMA-404,作为原生数据类型。它允许从Redis中存储、更新和获取JSON值. GitHub 使用1docker run -p 6379:6379 --name redis-redisjson redislabs/rejson:latest 1loadmodule /usr/lib/redis/module/rejson.so 命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394命令和子命令的名称是大写的，例如JSON.SET 和INDENT强制参数用尖括号括起来，例如&lt;path&gt;可选参数用方括号括起来，例如[index]其他可选参数由三个句点字符表示，即...管道字符|表示异或##标量命令#设置json值JSON.SET &lt;key&gt; &lt;path&gt; &lt;json&gt; [NX | XX]说明:NX: 如果不存在就添加XX: 如果存在就更新#查询key的值JSON.GET &lt;key&gt; [INDENT indentation-string] [NEWLINE line-break-string] [SPACE space-string] [path ...]说明:可以接受多个path,默认是rootINDENT: 设置嵌套级别NEWLINE: 每行末尾打印的字符串SPACE: 设置key和value之间的字符串JSON.GET myjsonkey INDENT &quot;\\t&quot; NEWLINE &quot;\\n&quot; SPACE &quot; &quot; .JSON.SET doc $ &#x27;&#123;&quot;a&quot;:2, &quot;b&quot;: 3, &quot;nested&quot;: &#123;&quot;a&quot;: 4, &quot;b&quot;: null&#125;&#125;&#x27;JSON.GET doc $..bJSON.GET doc ..a $..b#查询指定路径下的多个key,不存在的key或path返回nullJSON.MGET &lt;key&gt; [key ...] &lt;path&gt;JSON.SET doc1 $ &#x27;&#123;&quot;a&quot;:1, &quot;b&quot;: 2, &quot;nested&quot;: &#123;&quot;a&quot;: 3&#125;, &quot;c&quot;: null&#125;&#x27;JSON.SET doc2 $ &#x27;&#123;&quot;a&quot;:4, &quot;b&quot;: 5, &quot;nested&quot;: &#123;&quot;a&quot;: 6&#125;, &quot;c&quot;: null&#125;&#x27;JSON.MGET doc1 doc2 $..a#删除值JSON.DEL &lt;key&gt; [path]说明: 不存在的key或path会被忽略返回integer#增加数字的值JSON.NUMINCRBY &lt;key&gt; &lt;path&gt; &lt;number&gt;#数字乘法,过时了JSON.NUMMULTBY &lt;key&gt; &lt;path&gt; &lt;number&gt;#追加字符串JSON.STRAPPEND &lt;key&gt; [path] &lt;json-string&gt;#字符串的长度JSON.STRLEN &lt;key&gt; [path]##数组命令#追加数组元素JSON.ARRAPPEND &lt;key&gt; &lt;path&gt; &lt;json&gt; [json ...]#搜索指定元素在数组中第一次出现的位置,如果存在返回索引,不存在返回-1JSON.ARRINDEX &lt;key&gt; &lt;path&gt; &lt;json-scalar&gt; [start [stop]]说明:[start [stop]] 从start开始(包含)到stop(不包含)的范围#在数组指定位置插入元素JSON.ARRINSERT &lt;key&gt; &lt;path&gt; &lt;index&gt; &lt;json&gt; [json ...]说明:index: 0是数组第一个元素,负数表示从末端开始计算#数组的长度JSON.ARRLEN &lt;key&gt; [path]说明:如果key或path不存在,返回null#删除返回数组中指定位置的元素JSON.ARRPOP &lt;key&gt; [path [index]]说明:index: 默认是-1,最后一个元素#去掉元素,使其仅包含指定的包含范围的元素JSON.ARRTRIM &lt;key&gt; &lt;path&gt; &lt;start&gt; &lt;stop&gt;##对象命令#返回对象中的keyJSON.OBJKEYS &lt;key&gt; [path]#返回对象key的数量JSON.OBJLEN &lt;key&gt; [path]##模块命令#返回json value的数据类型JSON.TYPE &lt;key&gt; [path]#返回key的字节数JSON.DEBUG MEMORY &lt;key&gt; [path] redlock分布式锁","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 数据备份 RDB","slug":"database/redis/persistence","date":"2022-06-03T08:45:00.000Z","updated":"2023-12-25T05:56:09.168Z","comments":true,"path":"2022/06/03/database/redis/persistence/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/persistence/","excerpt":"","text":"RDBRDB 的优点 RDB 文件非常紧凑，适合作为冷备。比如你可以在每个小时报保存一下过去 24 小时内的数据，同时每天保存过去 30 天的数据，这样即使出了问题你也可以根据需求恢复到不同版本的数据集。 快照在保存 RDB 文件时父进程唯一需要做的就是 fork 出一个子进程，接下来的工作全部由子进程来做，父进程不需要再做其他 IO 操作，所以快照持久化方式可以最大化 Redis 的性能。 恢复大数据集时，RDB 比 AOF 更快。 RDB 的缺点 如果系统发生故障，将会丢失最后一次创建快照之后的数据。如果你希望在 Redis 意外停止工作（例如电源中断）的情况下丢失的数据最少的话，那么 快照不适合你。虽然你可以配置不同的 save 时间点(例如每隔 5 分钟并且对数据集有 100 个写的操作)，是 Redis 要完整的保存整个数据集是一个比较繁重的工作，你通常会每隔 5 分钟或者更久做一次完整的保存，万一在 Redis 意外宕机，你可能会丢失几分钟的数据。 如果数据量很大，保存快照的时间会很长。快照需要经常 fork 子进程来保存数据集到硬盘上。当数据集比较大的时候，fork 的过程是非常耗时的，可能会导致 Redis 在一些毫秒级内不能响应客户端的请求。如果数据集巨大并且 CPU 性能不是很好的情况下，这种情况会持续 1 秒。AOF 也需要 fork，但是你可以调节重写日志文件的频率来提高数据集的耐久度。 RDB 的载入RDB 文件的载入工作是在服务器启动时自动执行的，Redis 并没有专门用于载入 RDB 文件的命令。 服务器载入 RDB 文件期间，会一直处于阻塞状态，直到载入完成为止。 🔔 注意：因为 AOF 通常更新频率比 RDB 高，所以丢失数据相对更少。基于这个原因，Redis 有以下默认行为： 只有在关闭 AOF 功能的情况下，才会使用 RDB 还原数据，否则优先使用 AOF 文件来还原数据。 Redis 的配置文件 redis.conf 中与 RDB 有关的选项： save - Redis 会根据 save 选项，让服务器每隔一段时间自动执行一次 BGSAVE 命令。 stop-writes-on-bgsave-error - 当 BGSAVE 命令出现错误时停止写 RDB 文件 rdbcompression - RDB 文件开启压缩功能。 rdbchecksum - 对 RDB 文件进行校验。 dbfilename - RDB 文件名。 dir - RDB 文件和 AOF 文件的存储路径。 AOFAOF 的优点 如果系统发生故障，AOF 丢失数据比 RDB 少。你可以使用不同的 fsync 策略：无 fsync；每秒 fsync；每次写的时候 fsync。使用默认的每秒 fsync 策略，Redis 的性能依然很好(fsync 是由后台线程进行处理的,主线程会尽力处理客户端请求)，一旦出现故障，你最多丢失 1 秒的数据。 AOF 文件可修复 - AOF 文件是一个只进行追加的日志文件，所以不需要写入 seek，即使由于某些原因(磁盘空间已满，写的过程中宕机等等)未执行完整的写入命令，你也也可使用 redis-check-aof 工具修复这些问题。 AOF 文件可压缩。Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写：重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件可读 - AOF 文件有序地保存了对数据库执行的所有写入操作，这些写入操作以 Redis 命令的格式保存。因此 AOF 文件的内容非常容易被人读懂，对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单。举个例子，如果你不小心执行了 FLUSHALL 命令，但只要 AOF 文件未被重写，那么只要停止服务器，移除 AOF 文件末尾的 FLUSHALL 命令，并重启 Redis ，就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF 的缺点 AOF 文件体积一般比 RDB 大 - 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 恢复大数据集时，AOF 比 RDB 慢。 - 根据所使用的 fsync 策略，AOF 的速度可能会慢于快照。在一般情况下，每秒 fsync 的性能依然非常高，而关闭 fsync 可以让 AOF 的速度和快照一样快，即使在高负荷之下也是如此。不过在处理巨大的写入载入时，快照可以提供更有保证的最大延迟时间（latency）。 appendfsync 不同选项决定了不同的持久化行为： always - 将缓冲区所有内容写入并同步到 AOF 文件。 everysec - 将缓冲区所有内容写入到 AOF 文件，如果上次同步 AOF 文件的时间距离现在超过一秒钟，那么再次对 AOF 文件进行同步，这个同步操作是有一个线程专门负责执行的。 no - 将缓冲区所有内容写入到 AOF 文件，但并不对 AOF 文件进行同步，何时同步由操作系统决定。 AOF 的载入因为 AOF 文件中包含了重建数据库所需的所有写命令，所以服务器只要载入并执行一遍 AOF 文件中保存的写命令，就可以还原服务器关闭前的数据库状态。 AOF 载入过程如下： 服务器启动载入程序。 创建一个伪客户端。因为 Redis 命令只能在客户端上下文中执行，所以需要创建一个伪客户端来载入、执行 AOF 文件中记录的命令。 从 AOF 文件中分析并读取一条写命令。 使用伪客户端执行写命令。 循环执行步骤 3、4，直到所有写命令都被处理完毕为止。 载入完毕。 AOF 的重写随着 Redis 不断运行，AOF 的体积也会不断增长，这将导致两个问题： AOF 耗尽磁盘可用空间。 Redis 重启后需要执行 AOF 文件记录的所有写命令来还原数据集，如果 AOF 过大，则还原操作执行的时间就会非常长。 为了解决 AOF 体积膨胀问题，Redis 提供了 AOF 重写功能，来对 AOF 文件进行压缩。AOF 重写可以产生一个新的 AOF 文件，这个新的 AOF 文件和原来的 AOF 文件所保存的数据库状态一致，但体积更小。 AOF 重写并非读取和分析现有 AOF 文件的内容，而是直接从数据库中读取当前的数据库状态。即依次读取数据库中的每个键值对，然后用一条命令去记录该键值对，以此代替之前可能存在冗余的命令。 AOF 后台重写作为一种辅助性功能，显然 Redis 并不想在 AOF 重写时阻塞 Redis 服务接收其他命令。因此，Redis 决定通过 BGREWRITEAOF 命令创建一个子进程，然后由子进程负责对 AOF 文件进行重写，这与 BGSAVE 原理类似。 在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区。当 AOF 重写子进程开始工作后，Redis 每执行完一个写命令，会同时将这个命令发送给 AOF 缓冲区和 AOF 重写缓冲区。 由于彼此不是在同一个进程中工作，AOF 重写不影响 AOF 写入和同步。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新旧两个 AOF 文件所保存的数据库状态一致。 最后，服务器用新的 AOF 文件替换就的 AOF 文件，以此来完成 AOF 重写操作。 可以通过设置 auto-aof-rewrite-percentage 和 auto-aof-rewrite-min-size，使得 Redis 在满足条件时，自动执行 BGREWRITEAOF。 假设配置如下： 12auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 表明，当 AOF 大于 64MB，且 AOF 体积比上一次重写后的体积大了至少 100% 时，执行 BGREWRITEAOF。 AOF 的配置AOF 的默认配置： 12345appendonly noappendfsync everysecno-appendfsync-on-rewrite noauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb AOF 持久化通过在 redis.conf 中的 appendonly yes 配置选项来开启。 appendonly - 开启 AOF 功能。 appendfilename - AOF 文件名。 appendfsync - 用于设置同步频率，它有以下可选项： always - 每个 Redis 写命令都要同步写入硬盘。这样做会严重降低 Redis 的速度。 everysec - 每秒执行一次同步，显示地将多个写命令同步到硬盘。为了兼顾数据安全和写入性能，推荐使用 appendfsync everysec 选项。Redis 每秒同步一次 AOF 文件时的性能和不使用任何持久化特性时的性能相差无几。 no - 让操作系统来决定应该何时进行同步。 no-appendfsync-on-rewrite - AOF 重写时不支持追加命令。 auto-aof-rewrite-percentage - AOF 重写百分比。 auto-aof-rewrite-min-size - AOF 重写文件的最小大小。 dir - RDB 文件和 AOF 文件的存储路径。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 主从复制","slug":"database/redis/replication","date":"2022-06-03T08:45:00.000Z","updated":"2023-12-25T05:56:09.168Z","comments":true,"path":"2022/06/03/database/redis/replication/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/replication/","excerpt":"","text":"一、复制简介Redis 通过 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。 一个主服务器可以有多个从服务器。不仅主服务器可以有从服务器，从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个主从链。 一个从服务器只能有一个主服务器，并且不支持主主复制。 可以通过复制功能来让主服务器免于执行持久化操作： 只要关闭主服务器的持久化功能， 然后由从服务器去执行持久化操作即可。 在使用 Redis 复制功能时的设置中，强烈建议在 master 和在 slave 中启用持久化。当不启用时，例如由于非常慢的磁盘性能而导致的延迟问题，应该配置实例来避免重置后自动重启。 从 Redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。 只读模式由 redis.conf 文件中的 slave-read-only 选项控制， 也可以通过 CONFIG SET parameter value 命令来开启或关闭这个模式。 只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。 二、旧版复制 Redis 2.8 版本以前实现方式：SYNC 命令 Redis 的复制功能分为同步（sync）和命令传播（command propagate）两个操作： 同步（sync） - 用于将从服务器的数据库状态更新至主服务器当前的数据库状态。 命令传播（command propagate） - 当主服务器的数据库状态被修改，导致主从数据库状态不一致时，让主从服务器的数据库重新回到一致状态。 同步SYNC 命令的执行步骤： 从服务器向主服务器发送 SYNC 命令。 收到 SYNC 命令的主服务器执行 BGSAVE 命令，在后台生成一个 RDB 文件，并使用一个缓冲区记录从现在开始执行的所有写命令。 主服务器执行 BGSAVE 完毕后，主服务器会将生成的 RDB 文件发送给从服务器。从服务器接收并载入 RDB 文件，更新自己的数据库状态。 主服务器将记录在缓冲区中的所有写命令发送给从服务器，从服务器执行这些写命令，更新自己的数据库状态。 命令传播同步操作完成后，主从数据库的数据库状态将达到一致。每当主服务器执行客户端发送的写命令时，主从数据库状态不再一致。需要将写命令发送给从服务器执行，使得二者的数据库状态重新达到一致。 旧版复制的缺陷从服务器对主服务器的复制存在两种情况： 初次复制 - 从服务器以前没有复制过将要复制的主服务器。 断线后重复制 - 处于命令传播阶段的主从服务器因为网络原因而中断了复制，当从服务器通过自动重连重新连上了主服务器后，继续复制主服务器。 对于初次复制，旧版复制功能可用很好完成任务；但是对于断线后重复制，由于每次任然需要生成 RDB 并传输，效率很低。 :bell: 注意：SYNC 命令是一个非常耗费资源的操作。 主服务器执行 BGSAVE 命令生成 RDB 文件，这个操作会耗费主服务器大量的 CPU、内存和磁盘 I&#x2F;O 资源。 主服务器传输 RDB 文件给从服务器，这个操作会耗费主从服务器大量的网络资源，并对主服务器响应时延产生影响。 从服务器载入 RDB 文件期间，会阻塞其他命令请求。 三、新版复制 Redis 2.8 版本以后的新实现方式：使用 PSYNC 命令替代 SYNC 命令。 PSYNC 命令具有完整重同步和部分重同步两种模式： 完整重同步（full resychronization） - 用于初次复制。执行步骤与 SYNC 命令基本一致。 部分重同步（partial resychronization） - 用于断线后重复制。如果条件允许，主服务器可以将主从服务器连接断开期间执行的写命令发送给从服务器，从服务器只需接收并执行这些写命令，即可将主从服务器的数据库状态保持一致。 部分重同步部分重同步功能实现由三个部分构成： 主从服务器的复制偏移量（replication offset） 主服务器的复制积压缓冲区（replication backlog） 服务器的运行 ID 复制偏移量主服务器和从服务器会分别维护一个复制偏移量。 如果主从服务器的复制偏移量相同，则说明二者的数据库状态一致； 反之，则说明二者的数据库状态不一致。 复制积压缓冲区复制积压缓冲区是主服务器维护的一个固定长度的先进先出（FIFO）队列，默认大小为 1MB。 复制积压缓冲区会保存一部分最近传播的写命令，并且复制积压缓冲区会为队列中的每个字节记录相应的复制偏移量。 当从服务器断线重连主服务时，从服务器会通过 PSYNC 命令将自己的复制偏移量 offset 发送给主服务器，主服务器会根据这个复制偏移量来决定对从服务器执行何种同步操作。 如果 offset 之后的数据仍然在复制积压缓冲区，则主服务器对从服务器执行部分重同步操作。 反之，则主服务器对从服务器执行完整重同步操作。 🔔 注意：合理调整复制积压缓冲区的大小 Redis 复制积压缓冲区默认大小为 1MB。 复制积压缓冲区的最小大小可以根据公式 second * write_size_per_second 估算。 服务器的运行 ID 每个 Redis 服务器，都有运行 ID，用于唯一识别身份。 运行 ID 在服务器启动时自动生成，由 40 个随机的十六进制字符组成。 从服务器对主服务器进行初次复制时，主服务器会将自己的运行 ID 传送给从服务器，从服务器会将这个运行 ID 保存下来。 当从服务器断线重连一个主服务器时，从服务器会发送之前保存的运行 ID： 如果保存的运行 ID 和当前主服务器的运行 ID 一致，则说明从服务器断线之前连接的就是这个主服务器，主服务器可以继续尝试执行部分重同步操作； 反之，若运行 ID 不一致，则说明从服务器断线之前连接的不是这个主服务器，主服务器将对从服务器执行完整重同步操作。 PSYNC 命令了解了部分重同步的实现，PSYNC 的实现就很容易理解了，它的基本工作原理大致如下： 当从服务接收到 SLAVEOF 命令时，先判断从服务器以前是否执行过复制操作。 如果没有复制过任何主服务器，向要复制的主服务器发送 PSYNC ? -1 命令，主动请求进行完整重同步。 反之，向要复制的主服务器发送 PSYNC &lt;runid&gt; &lt;offset&gt; 命令。 runid 是上一次复制的主服务器的运行 ID。 offset 是复制偏移量。 接收到 PSYNC &lt;runid&gt; &lt;offset&gt; 命令的主服务会进行分析： 假如主从服务器的 master run id 相同，并且指定的偏移量（offset）在内存缓冲区中还有效，复制就会从上次中断的点开始继续。 如果其中一个条件不满足，就会进行完全重新同步（在 2.8 版本之前就是直接进行完全重新同步）。 四、心跳检测在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务器发送命令： 1REPLCONF ACK &lt;replication_offset&gt; 其中，replication_offset 是从服务器当前的复制偏移量。 发送 REPLCONF ACK 命令对于主从服务器有三个作用： 检测主从服务器的网络连接状态。 辅助实现 min-slaves 选项。 检测命令丢失。 检测主从连接状态可以通过发送和接收 REPLCONF ACK 命令来检查主从服务器之间的网络连接是否正常：如果主服务器超过一秒没有收到从服务器发来的 REPLCONF ACK 命令，那么主服务器就知道主从服务器之间的连接出现问题了。 可以通过向主服务器发送 INFO replication 命令，在列出的从服务器列表的 lag 一栏中，可以看到从服务器向主服务器发送 REPLCONF ACK 命令已经过去多少秒。 辅助实现 min-slaves 选项Redis 的 min-slaves-to-write 和 min-slaves-max-lag 两个选项可以防止主服务器在不安全的情况下执行写命令。 【示例】min-slaves 配置项 12min-slaves-to-write 3min-slaves-max-lag 10 以上配置表示：从服务器小于 3 个，或三个从服务器的延迟（lag）都大于等于 10 秒时，主服务器将拒绝执行写命令。 检测命令丢失如果因为网络故障，主服务传播给从服务器的写命令丢失，那么从服务器定时向主服务器发送 REPLCONF ACK 命令时，主服务器将发觉从服务器的复制偏移量少于自己的。然后，主服务器就会根据从服务器提交的复制偏移量，在复制积压缓冲区中找到从服务器缺少的数据，并将这些数据重新发送给从服务器。 五、复制的流程通过向从服务器发送如下 SLAVEOF 命令，可以让一个从服务器去复制一个主服务器。 1SLAVEOF &lt;master_ip&gt; &lt;master_port&gt; 步骤 1. 设置主从服务器配置一个从服务器非常简单， 只要在配置文件中增加以下的这一行就可以了： 1slaveof 127.0.0.1 6379 当然， 你需要将代码中的 127.0.0.1 和 6379 替换成你的主服务器的 IP 和端口号。 另外一种方法是调用 SLAVEOF host port 命令， 输入主服务器的 IP 和端口， 然后同步就会开始： 12127.0.0.1:6379&gt; SLAVEOF 127.0.0.1 10086OK 步骤 2. 主从服务器建立 TCP 连接。步骤 3. 发送 PING 检查通信状态。步骤 4. 身份验证。如果主服务器没有设置 requirepass ，从服务器没有设置 masterauth，则不进行身份验证；反之，则需要进行身份验证。如果身份验证失败，则放弃执行复制工作。 如果主服务器通过 requirepass 选项设置了密码， 那么为了让从服务器的同步操作可以顺利进行， 我们也必须为从服务器进行相应的身份验证设置。 对于一个正在运行的服务器， 可以使用客户端输入以下命令： 1config set masterauth &lt;password&gt; 要永久地设置这个密码， 那么可以将它加入到配置文件中： 1masterauth &lt;password&gt; 另外还有几个选项， 它们和主服务器执行部分重同步时所使用的复制流缓冲区有关， 详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。 步骤 5. 发送端口信息。从服务器执行 REPLCONF listening-port &lt;port-number&gt; ，向主服务器发送从服务器的监听端口号。 步骤 6. 同步。前文已介绍，此处不赘述。 步骤 7. 命令传播。在命令传播阶段，从服务器默认会以每秒一次的频率，向主服务发送命令： 1REPLCONF ACK &lt;replication_coffset&gt; 命令的作用： 检测主从服务器的网络连接状态。 辅助实现 min-slave 选项。 检测命令丢失。 六、复制的配置项从 Redis 2.8 开始， 为了保证数据的安全性， 可以通过配置， 让主服务器只在有至少 N 个当前已连接从服务器的情况下， 才执行写命令。 不过， 因为 Redis 使用异步复制， 所以主服务器发送的写数据并不一定会被从服务器接收到， 因此， 数据丢失的可能性仍然是存在的。 以下是这个特性的运作原理： 从服务器以每秒一次的频率 PING 主服务器一次， 并报告复制流的处理情况。 主服务器会记录各个从服务器最后一次向它发送 PING 的时间。 用户可以通过配置， 指定网络延迟的最大值 min-slaves-max-lag ， 以及执行写操作所需的至少从服务器数量 min-slaves-to-write 。 如果至少有 min-slaves-to-write 个从服务器， 并且这些服务器的延迟值都少于 min-slaves-max-lag秒， 那么主服务器就会执行客户端请求的写操作。 你可以将这个特性看作 CAP 理论中的 C 的条件放宽版本： 尽管不能保证写操作的持久性， 但起码丢失数据的窗口会被严格限制在指定的秒数中。 另一方面， 如果条件达不到 min-slaves-to-write 和 min-slaves-max-lag 所指定的条件， 那么写操作就不会被执行， 主服务器会向请求执行写操作的客户端返回一个错误。 以下是这个特性的两个选项和它们所需的参数： min-slaves-to-write &lt;number of slaves&gt; min-slaves-max-lag &lt;number of seconds&gt; 详细的信息可以参考 Redis 源码中附带的 redis.conf 示例文件。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 面试的一些问题","slug":"database/redis/interview","date":"2022-06-03T08:35:00.000Z","updated":"2023-12-25T05:56:09.167Z","comments":true,"path":"2022/06/03/database/redis/interview/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/interview/","excerpt":"","text":"内存策略1. 内存淘汰有哪些，分别适用于哪些场景Redis 过期策略是：定期删除+惰性删除。 消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它。 主动方法（active way），定期从设置了失效时间的主键中选择一部分失效的主键删除。 2. 有哪些删除失效key的方法 noeviction - 当内存使用达到阈值的时候，所有引起申请内存的命令会报错。这是 Redis 默认的策略。 allkeys-lru - 在主键空间中，优先移除最近未使用的 key。 allkeys-random - 在主键空间中，随机移除某个 key。 volatile-lru - 在设置了过期时间的键空间中，优先移除最近未使用的 key。 volatile-random - 在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl - 在设置了过期时间的键空间中，具有更早过期时间的 key 优先移除。 3. 如何选择淘汰策略 如果**数据呈现幂等分布（存在热点数据，部分数据访问频率高，部分数据访问频率低），则使用 allkeys-lru**。 如果**数据呈现平等分布（数据访问频率大致相同），则使用 allkeys-random**。 如果希望**使用不同的 TTL 值向 Redis 提示哪些 key 更适合被淘汰，请使用 volatile-ttl**。 volatile-lru 和 volatile-random 适合既应用于缓存和又应用于持久化存储的场景，然而我们也可以通过使用两个 Redis 实例来达到相同的效果。 将 key 设置过期时间实际上会消耗更多的内存，因此建议使用 allkeys-lru 策略从而更有效率的使用内存。 4. 设计一个LRU算法 可以继承 LinkedHashMap，并覆写 removeEldestEntry 方法来实现一个最简单的 LRUCache 内存淘汰要点 最大缓存 - Redis 允许通过 maxmemory 参数来设置内存最大值。 失效时间 - 作为一种定期清理无效数据的重要机制，在 Redis 提供的诸多命令中，EXPIRE、EXPIREAT、PEXPIRE、PEXPIREAT 以及 SETEX 和 PSETEX 均可以用来设置一条键值对的失效时间。而一条键值对一旦被关联了失效时间就会在到期后自动删除（或者说变得无法访问更为准确）。 淘汰策略 - 随着不断的向 Redis 中保存数据，当内存剩余空间无法满足添加的数据时，Redis 内就会施行数据淘汰策略，清除一部分内容然后保证新的数据可以保存到内存中。内存淘汰机制是为了更好的使用内存，用一定得 miss 来换取内存的利用率，保证 Redis 缓存中保存的都是热点数据。 非精准的 LRU - 实际上 Redis 实现的 LRU 并不是可靠的 LRU，也就是名义上我们使用 LRU 算法淘汰键，但是实际上被淘汰的键并不一定是真正的最久没用的。 持久化不同持久化方式的特性和原理是什么？Redis 会周期性生成 RDB 文件。生成 RDB 流程：Redis fork 一个子进程，负责生成 RDB；生成 RDB 采用 Copy On Write 模式，此时，如果收到写请求，会在原副本上操作，不影响工作。RDB 只能恢复生成快照时刻的数据，之后的数据无法恢复。生成 RDB 的资源开销高昂。RDB 适合做冷备。 AOF 会将写命令不断追加到 AOF 文本日志末尾。 AOF 丢数据比 RDB 少，但文件会比 RDB 文件大很多。 一般，AOF 设置 appendfsync 同步频率为 everysec 即可。 建议同时使用 RDB 和 AOF。用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。 Redis 管道Redis 是一种基于 C&#x2F;S 模型以及请求&#x2F;响应协议的 TCP 服务。Redis 支持管道技术。管道技术允许请求以异步方式发送，即旧请求的应答还未返回的情况下，允许发送新请求。这种方式可以大大提高传输效率。使用管道发送命令时，Redis Server 会将部分请求放到缓存队列中（占用内存），执行完毕后一次性发送结果。如果需要发送大量的命令，会占用大量的内存，因此应该按照合理数量分批次的处理。","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 一些使用场景","slug":"database/redis/action","date":"2022-06-03T08:15:00.000Z","updated":"2023-12-25T05:56:09.166Z","comments":true,"path":"2022/06/03/database/redis/action/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/action/","excerpt":"","text":"数据缓存将热点数据放到内存中，设置内存的最大使用量以及过期淘汰策略来保证缓存的命中率。 缓存雪崩和缓存击穿 雪崩：如果大量的key过期时间设置的过于集中，到过期的那个时间点，Redis可能会出现短暂的卡顿现象。严重的话会出现缓存雪崩，我们一般需要在时间上加一个随机值，使得过期时间分散一些。 击穿：大量的请求都不存在缓存中，直接查询了数据库 大数据量去重boolfilter原理当一个元素加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，并置为1。查找的时候，只要这几个位点存在一个0，则一定不存在；如果都为1，则该元素可能存在。 应用场景 缓存穿透：每次查询都直接打到数据库，没有使用缓存 爬虫过滤：将采集下来的url放在集合中，每次采集前先查询一遍 bitmap优缺点优点 省空间，使用bit存储 操作快，读为O(1)，写为O(n) 方便扩容，方便计算 缺点 限制在512MB内，最大为2^32位 使用场景 用户在线状态 统计活跃用户 用户签到 轻量级分布式锁 互斥性: 使用 setnx 抢占锁。 避免永远不释放锁: 使用 expire 加一个过期时间，避免一直不释放锁，导致阻塞。 原子性: setnx 和 expire 必须合并为一个原子指令，避免 setnx 后，机器崩溃，没来得及设置 expire，从而导致锁永不释放。 计数器支持很频繁的读写操作 应用限流进行流量降级 消息队列 list stream zset 用时间戳做score，内容做key，消费者使用 zrangebyscore 消费N秒之前的数据，延时队列 排行榜、点赞、投票使用 ZSET 数据类型。 查找表可以将Session，用户信息等放进来 自动补全时间线ID生成、短网址ID生成 使用 incr 或 incrby 生成 短网址 为目标网站生成新的数字ID，可以使用 incr 或 incrby 生成 将十进制数字转换成36进制数字来创建短网址 将短网址作为key，目标网址作为value放入哈希表中","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 文档索引","slug":"database/redis/doc","date":"2022-06-03T08:10:00.000Z","updated":"2023-12-25T05:56:09.167Z","comments":true,"path":"2022/06/03/database/redis/doc/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/doc/","excerpt":"","text":"Redis 命令参考 Redis中文网 Redisson","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis Resp 协议","slug":"database/redis/resp","date":"2022-06-03T08:05:00.000Z","updated":"2023-12-25T05:56:09.168Z","comments":true,"path":"2022/06/03/database/redis/resp/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/resp/","excerpt":"","text":"RESP协议 实现简单.可以减低客户端出现bug的机率 解析速度快.由于RESP能知道返回数据的固定长度,所以不用像json那样扫描整个payload去解析, 所以它的性能是能跟解析二进制数据的性能相媲美的. 可读性好. 数据类型 simple string. 简单的字符串 error. 就是表示这是一个错误(异常)情况 integer 表示这是一个整数 bulk string. 表示是长字符串,但是必须小于512M. arrays. 表示这是一个数组,数组元素可以是上面的任意一种类型,也可以是一个数组 simple string 的第一个字节是个”+”(加号), 后面接着的是字符串的内容, 最后以CRLF(\\r\\n)结尾.例如: 1&quot;+OK\\r\\n&quot; error. error其实和string是类似的, 但是RESP为了能让不同客户端把这种error和正常的返回结果区分开来对待 (例如redis返回error的话,就抛出异常),特意多设计了这个数据类型. error类型的第一个字节是”-“(减号), 后面接着的是错误的信息, 最后以CRLF(\\r\\n)结尾,例如: 1&quot;-ERR unknown command &#x27;foobar&#x27;\\r\\n&quot; integer 类型的第一个字节是”:”(冒号), 后面接着的是整数,最后以CRLF(\\r\\n)结尾, 例如: 1&quot;:1000\\r\\n&quot; bulk string. 本质上也是字符串.跟普通字符串区分开来, 它的第一个字节是”$”(美元符号),紧接着是一个整数,表示字符串的字节数,字节数后面接一个CRLF. CRLF后面是字符串的内容, 最后以一个CRLF结尾. 例如: 12345&quot;$0\\r\\n&quot; --$后面的0表示这是一个空字符串&quot;$-1\\r\\n&quot; -- $后面的-1表示这是一个null字符串,Null Bulk String要求客户端返回空对象,而不能简单地返回个空字符串&quot;$6\\r\\nABCDEF\\r\\n&quot; -- ABCDEF是6个字节,所以$后面是6 arrays的第一个字节是”*”(星号), 紧接着后面是一个数字,表示这个数组的长度,数字后面是一个CRLF. 需要注意的是这个CRLF之后才是数组的真正内容, 而且数组内容可以是任意类型, 包括arrays和bulk string, 每个元素也要以CRLF结尾. 最后以CRLF(\\r\\n)结尾. 举例: 123456789101112131415&quot;*0\\r\\n&quot; --*后面的0表示表示空的数组&quot;*-1\\r\\n&quot; --*后面的-1表示表示是null数组&quot;*5\\r\\n -- *5表示这是一个拥有5个元素的数组+bar\\r\\n -- 第1个元素是简单的字符串-unknown command\\r\\n -- 第2个元素是个异常:3\\r\\n -- 第3个元素是个整数$3\\r\\n -- 第4个元素是长度为3个字节的长字符串foofoo\\r\\n -- 第4个元素的内容*3\\r\\n -- 第5个元素又是个数组:1\\r\\n -- 第5个元素数组的第1元素:2\\r\\n -- 第5个元素数组的第2元素:3\\r\\n -- 第5个元素数组的第3元素&quot; request-response模型一般来说,redis客户端和服务端交互都是通过以下两个步骤: 121. redis发送一个命令到服务端, 然后阻塞在socket.read()方法, 等待服务端的返回2. 服务端收到一个命令, 处理完成后将数据发送回去给客户端 这个就被称为request&#x2F;reponse模型. redis的大部分命令都是使用这种模型进行通讯, 除了两种情况: 121. pipeline模式. 在pipeline模式下, 客户端可能会把多个命令收集在一起, 然后一并发送给服务端, 最后等待服务端把所有命令的执行响应一并发送回来2. pub/sub, 发布订阅模式下, redis客户端只需要发送一次订阅命令 RESP协议的request&#x2F;response模型可以总结为以下两个步骤 121. 客户端发送命令, 一般组装成bulk string的数组2. 服务端处理命令, 根据不同的命令,可能返回不同的数据类型 例如命令”set test1 1” 一般被序列化成 12345678910*3\\r\\n$3\\r\\nset\\r\\n$5\\r\\ntest1\\r\\n$1\\r\\n1\\r\\n-- 为了方便理解, 每个CRLF我们给它换一下行*3\\r\\n -- 这个命令包含3个(bulk)字符串$3\\r\\n -- 第一个bulk string有3个字节set\\r\\n -- 第一个bulk string是set$5\\r\\n -- 第二个bulk string有5个字节test1\\r\\n -- 第二个bulk string是test1$1\\r\\n -- 第三个bulk string有1个字节1\\r\\n -- 第三个bulk string是1 它的返回是: 1+OK\\r\\n --一个简单的字符串 再例如命令”get test1”: 12345678 *2\\r\\n$3\\r\\nget\\r\\n$5\\r\\ntest1\\r\\n即:*2\\r\\n -- 这个命令是2个bulk字符串的数组$3\\r\\n -- 第一个bulk字符串有3个字节: getget\\r\\n$5\\r\\n -- 第二个bulk字符串有5个字节: test1test1\\r\\n 这个命令的返回是: 12$1\\r\\n -- 只有一个字节的bulk string1\\r\\n 再来看一个错误的命令”get “, 这里我们get的命令故意不传参数 123456789request:*1\\r\\n$3\\r\\nget\\r\\nresponse(跟我们在redis-cli里面获取的提示是一样的):-ERR wrong number of arguments for &#x27;get&#x27; command\\r\\n","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Redis 命令索引","slug":"database/redis/cmd","date":"2022-06-03T08:00:00.000Z","updated":"2023-12-25T05:56:09.167Z","comments":true,"path":"2022/06/03/database/redis/cmd/","link":"","permalink":"https://blog.ifan.host/2022/06/03/database/redis/cmd/","excerpt":"","text":"命令索引String 命令 描述 时间复杂度 APPEND key value 追加一个值到key上 O(1) BITCOUNT key [start end] 统计字符串指定起始位置的字节数 O(N) BITFIELD key [GET type offset] [SET type offset value] [INCRBY type offset increment] [OVERFLOW WRAP|SAT|FAIL] 位图的操作 O(1) BITOP operation destkey key [key ...] 对一个或多个保存二进制位的字符串key进行位元操作，将结果保存在destkey O(N) BITPOS key bit [start] [end] 返回字符串中第一个被设置为1或者0的bit位 O(N) DECR key 整数原子减1 O(1) DECRBY key decrement 原子减指定的整数 O(1) GET key 返回key的value O(1) GETBIT key offset 返回位的值存储在关键的字符串值的偏移量。 O(1) GETRANGE key start end 获取存储在key上的值的一个子字符串 O(N) GETSET key value 设置一个key的value，并获取设置前的值 O(1) INCR key 执行原子加1操作 O(1) INCRBY key increment 执行原子增加一个整数 O(1) INCRBYFLOAT key increment 执行原子增加一个浮点数 O(1) MGET key [key ...] 获得所有key的值 O(N) MSET key value [key value ...] 设置多个key value O(N) MSETNX key value [key value ...] 设置多个key value,仅当key存在时 O(N) PSETEX key milliseconds value Set the value and expiration in milliseconds of a key O(1) SET key value [EX seconds] [PX milliseconds] [NX|XX] 设置一个key的value值 O(1) SETBIT key offset value Sets or clears the bit at offset in the string value stored at key O(1) SETEX key seconds value 设置key-value并设置过期时间（单位：秒） O(1) SETNX key value 设置的一个关键的价值，只有当该键不存在 O(1) SETRANGE key offset value Overwrite part of a string at key starting at the specified offset O(M) STRLEN key 获取指定key值的长度 O(1) List 命令 描述 时间复杂度 BLPOP key [key ...] timeout 删除，并获得该列表中的第一元素，或阻塞，直到有一个可用 BRPOP key [key ...] timeout 删除，并获得该列表中的最后一个元素，或阻塞，直到有一个可用 BRPOPLPUSH source destination timeout 弹出一个列表的值，将它推到另一个列表，并返回它;或阻塞，直到有一个可用 LINDEX key index 获取一个元素，通过其索引列表 LINSERT key BEFORE|AFTER pivot value 在列表中的另一个元素之前或之后插入一个元素 LLEN key 获得队列(List)的长度 LPOP key 从队列的左边出队一个元素 LPUSH key value [value ...] 从队列的左边入队一个或多个元素 LPUSHX key value 当队列存在时，从队到左边入队一个元素 LRANGE key start stop 从列表中获取指定返回的元素 LREM key count value 从列表中删除元素 LSET key index value 设置队列里面一个元素的值 LTRIM key start stop 修剪到指定范围内的清单 RPOP key 从队列的右边出队一个元 RPOPLPUSH source destination 删除列表中的最后一个元素，将其追加到另一个列表 RPUSH key value [value ...] 从队列的右边入队一个元素 RPUSHX key value 从队列的右边入队一个元素，仅队列存在时有效 Hash 命令 描述 时间复杂度 HDEL key field [field ...] 删除一个或多个Hash的field HEXISTS key field 判断field是否存在于hash中 HGET key field 获取hash中field的值 HGETALL key 从hash中读取全部的域和值 HINCRBY key field increment 将hash中指定域的值增加给定的数字 HINCRBYFLOAT key field increment 将hash中指定域的值增加给定的浮点数 HKEYS key 获取hash的所有字段 HLEN key 获取hash里所有字段的数量 HMGET key field [field ...] 获取hash里面指定字段的值 HMSET key field value [field value ...] 设置hash字段值 HSET key field value 设置hash里面一个字段的值 HSETNX key field value 设置hash的一个字段，只有当这个字段不存在时有效 HSTRLEN key field 获取hash里面指定field的长度 HVALS key 获得hash的所有值 HSCAN key cursor [MATCH pattern] [COUNT count] 迭代hash里面的元素 Sets 命令 描述 时间复杂度 SADD key member [member ...] 添加一个或者多个元素到集合里 O(N) SCARD key 获取集合里面元素的数量 O(1) SDIFF key [key ...] 获取队列中不存在的元素 O(N) SDIFFSTORE destination key [key1 key2 ...] 取key1和key2的差集，放在key中，如果key中存在元素，则将其覆盖 O(N) SINTER key [key ...] 获得两个集合的交集 O(N*M) SINTERSTORE destination key [key1 key2 ...] 取key1和key2的交集，放在key中，如果key中存在元素，则将其覆盖 O(N*M) SISMEMBER key member 确定一个给定的值是一个集合的成员 O(1) SMEMBERS key 获取集合里面的所有元素 O(N) SMOVE source destination member 移动集合里面的一个元素到另一个集合 O(1) SPOP key [count] 删除并获取一个集合里面的元素 O(1) SRANDMEMBER key [count] 从集合里面随机获取一个元素 O(N) SREM key member [member ...] 从集合里删除一个或多个元素 O(N) SUNION key [key ...] 返回给定的多个集合的并集中的所有成员. O(N) SUNIONSTORE destination key [key ...] 合并set元素，并将结果存入新的set里面 O(N) SSCAN key cursor [MATCH pattern] [COUNT count] 迭代set里面的元素 O(N) ZSet 命令 描述 时间复杂度 ZADD key [NX|XX] [CH] [INCR] score member [score member ...] 添加到有序set的一个或多个成员，或更新的分数，如果它已经存在 ZCARD key 获取一个排序的集合中的成员数量 ZCOUNT key min max 返回分数范围内的成员数量 ZINCRBY key increment member 增量的一名成员在排序设置的评分 ZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX] 相交多个排序集，导致排序的设置存储在一个新的关键 ZLEXCOUNT key min max 返回成员之间的成员数量 ZPOPMAX key [count] Remove and return members with the highest scores in a sorted set ZPOPMIN key [count] Remove and return members with the lowest scores in a sorted set ZRANGE key start stop [WITHSCORES] 根据指定的index返回，返回sorted set的成员列表 ZRANGEBYLEX key min max [LIMIT offset count] 返回指定成员区间内的成员，按字典正序排列, 分数必须相同。 ZREVRANGEBYLEX key max min [LIMIT offset count] 返回指定成员区间内的成员，按字典倒序排列, 分数必须相同 ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] 返回有序集合中指定分数区间内的成员，分数由低到高排序。 ZRANK key member 确定在排序集合成员的索引 ZREM key member [member ...] 从排序的集合中删除一个或多个成员 ZREMRANGEBYLEX key min max 删除名称按字典由低到高排序成员之间所有成员。 ZREMRANGEBYRANK key start stop 在排序设置的所有成员在给定的索引中删除 ZREMRANGEBYSCORE key min max 删除一个排序的设置在给定的分数所有成员 ZREVRANGE key start stop [WITHSCORES] 在排序的设置返回的成员范围，通过索引，下令从分数高到低 ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset count] 返回有序集合中指定分数区间内的成员，分数由高到低排序。 ZREVRANK key member 确定指数在排序集的成员，下令从分数高到低 ZSCORE key member 获取成员在排序设置相关的比分 ZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX] 添加多个排序集和导致排序的设置存储在一个新的关键 ZSCAN key cursor [MATCH pattern] [COUNT count] 迭代sorted sets里面的元素 Geo 命令 描述 时间复杂度 geoadd key [ NX | XX] [CH] longitude latitude member [ longitude latitude member ...] 添加地理位置坐标 O(log(N)) geopos key member [member ...] 获取地理位置的坐标 O(N) geodist key member1 member2 [ M | KM | FT | MI] 计算两个位置之间的距离 O(log(N)) georadius key longitude latitude radius M | KM | FT | MI [WITHCOORD] [WITHDIST] [WITHHASH] [ COUNT count [ANY]] [ ASC | DESC] [STORE key] [STOREDIST key] 根据用户给定的经纬度坐标来获取指定范围内的地理位置集合 O(N+log(M)) georadiusbymember key member radius M | KM | FT | MI [WITHCOORD] [WITHDIST] [WITHHASH] [ COUNT count [ANY]] [ ASC | DESC] [STORE key] [STOREDIST key] 根据储存在位置集合里面的某个地点获取指定范围内的地理位置集合 O(N+log(M)) geohash key member [member ...] 返回一个或多个位置对象的 geohash 值 O(log(N)) HyperLogLog 命令 描述 时间复杂度 PFADD key element [element …] 添加元素到 HyperLogLog 中 O(1) PFCOUNT key [key …] 获取给定键的近似基数，不存在则返回0 O(N) PFMERGE destkey sourcekey [sourcekey …] 合并多个 HyperLogLog 为一个 O(N) 集群命令 命令 描述 时间复杂度 cluster info 打印集群的信息 cluster nodes 列出集群当前已知的所有节点（ node），以及这些节点的相关信息。 cluster meet &lt;ip&gt; &lt;port&gt; 将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。 cluster forget &lt;node_id&gt; 从集群中移除 node_id 指定的节点。 cluster replicate &lt;node_id&gt; 将当前节点设置为 node_id 指定的节点的从节点。 cluster saveconfig 将节点的配置文件保存到硬盘里面。 cluster addslots &lt;slot&gt; [slot ...] 将一个或多个槽（ slot）指派（ assign）给当前节点。 cluster delslots &lt;slot&gt; [slot ...] 移除一个或多个槽对当前节点的指派。 cluster flushslots 移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。 cluster setslot &lt;slot&gt; node &lt;node_id&gt; 将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽&gt;，然后再进行指派。 cluster setslot &lt;slot&gt; migrating &lt;node_id&gt; 将本节点的槽 slot 迁移到 node_id 指定的节点中。 cluster setslot &lt;slot&gt; importing &lt;node_id&gt; 从 node_id 指定的节点中导入槽 slot 到本节点。 cluster setslot &lt;slot&gt; stable 取消对槽 slot 的导入（ import）或者迁移（ migrate）。 cluster keyslot &lt;key&gt; 计算键 key 应该被放置在哪个槽上。 cluster countkeysinslot &lt;slot&gt; 返回槽 slot 目前包含的键值对数量。 cluster getkeysinslot &lt;slot&gt; &lt;count&gt; 返回 count 个 slot 槽中的键","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"}]},{"title":"Python 工具方法 发送邮件","slug":"language/python/tools/send-email","date":"2022-06-03T07:00:00.000Z","updated":"2023-12-25T05:56:09.177Z","comments":true,"path":"2022/06/03/language/python/tools/send-email/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/python/tools/send-email/","excerpt":"","text":"发送 exchangelib 邮件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from exchangelib import DELEGATE, Account, Credentials, Message, Mailbox, HTMLBody, FileAttachment, Configuration, NTLMfrom exchangelib.protocol import BaseProtocol, NoVerifyHTTPAdapter# 此句用来消除ssl证书错误，exchange使用自签证书需加上import loggingimport urllib3urllib3.disable_warnings()BaseProtocol.HTTP_ADAPTER_CLS = NoVerifyHTTPAdapterlogger = logging.getLogger(__name__)class Mail: def __init__(self, email, password, host, port, retry: int = 3, *args, **kwargs) -&gt; None: self.email = email self.password = password self.host = host self.port = port def send(self, to_user, title: str, content: str): self.send_exchange_mail(to_user, title, content) def send_exchange_mail(self, to, subject, content, _type=&quot;plain&quot;, filename=None, is_send=False): if isinstance(to, str): to = [to] for t in to: try: if self.password: cred = Credentials(self.email.split(&#x27;@&#x27;)[0], self.password) else: cred = None config = Configuration(server=self.host, credentials=cred, auth_type=NTLM) a = Account( primary_smtp_address=self.email, config=config, autodiscover=False, access_type=DELEGATE ) m = Message( account=a, folder=a.sent, subject=subject, body=HTMLBody(content), to_recipients=[Mailbox(email_address=t)] ) if filename: with open(filename, &#x27;rb&#x27;) as f: _content = f.read() file_attach = FileAttachment(name=filename[filename.rfind(&#x27;/&#x27;)+1:], content=_content) m.attach(file_attach) m.send() print(&quot;【%s】邮件发送成功&quot; % subject) return True except Exception as e: print(f&quot;【%s】邮件发送失败,请检查信息\\n&#123;e&#125;&quot; % subject) return Falseif __name__ == &#x27;__main__&#x27;: &quot;&quot;&quot; pip3 install exchangelib -i https://pypi.doubanio.com/simple/ &quot;&quot;&quot; mail = Mail(&quot;username@email.com&quot;, &quot;username&quot;, &quot;email.com&quot;, 25) mail.send_exchange_mail(&quot;touser@email&quot;, &quot;测试消息&quot;, &quot;测试消息通过abc&quot;)","categories":[{"name":"python","slug":"python","permalink":"https://blog.ifan.host/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.ifan.host/tags/python/"}]},{"title":"GoLang 结构体转map","slug":"language/golang/base/struct-map","date":"2022-06-03T06:55:00.000Z","updated":"2023-12-25T05:56:09.171Z","comments":true,"path":"2022/06/03/language/golang/base/struct-map/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/base/struct-map/","excerpt":"","text":"结构体转Map三方库(structs) 反射123456789101112131415161718192021222324// ToMap 结构体转为Map[string]interface&#123;&#125;func ToMap(in interface&#123;&#125;, tagName string) (map[string]interface&#123;&#125;, error)&#123; out := make(map[string]interface&#123;&#125;) v := reflect.ValueOf(in) if v.Kind() == reflect.Ptr &#123; v = v.Elem() &#125; if v.Kind() != reflect.Struct &#123; // 非结构体返回错误提示 return nil, fmt.Errorf(&quot;ToMap only accepts struct or struct pointer; got %T&quot;, v) &#125; t := v.Type() // 遍历结构体字段 // 指定tagName值为map中key;字段值为map中value for i := 0; i &lt; v.NumField(); i++ &#123; fi := t.Field(i) if tagValue := fi.Tag.Get(tagName); tagValue != &quot;&quot; &#123; out[tagValue] = v.Field(i).Interface() &#125; &#125; return out, nil&#125; 1234m2, _ := ToMap(&amp;u1, &quot;json&quot;)for k, v := range m2&#123; fmt.Printf(&quot;key:%v value:%v value type:%T\\n&quot;, k, v, v)&#125; 嵌套struct12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// ToMap2 将结构体转为单层mapfunc ToMap2(in interface&#123;&#125;, tag string) (map[string]interface&#123;&#125;, error) &#123; // 当前函数只接收struct类型 v := reflect.ValueOf(in) if v.Kind() == reflect.Ptr &#123; // 结构体指针 v = v.Elem() &#125; if v.Kind() != reflect.Struct &#123; return nil, fmt.Errorf(&quot;ToMap only accepts struct or struct pointer; got %T&quot;, v) &#125; out := make(map[string]interface&#123;&#125;) queue := make([]interface&#123;&#125;, 0, 1) queue = append(queue, in) for len(queue) &gt; 0 &#123; v := reflect.ValueOf(queue[0]) if v.Kind() == reflect.Ptr &#123; // 结构体指针 v = v.Elem() &#125; queue = queue[1:] t := v.Type() for i := 0; i &lt; v.NumField(); i++ &#123; vi := v.Field(i) if vi.Kind() == reflect.Ptr &#123; // 内嵌指针 vi = vi.Elem() if vi.Kind() == reflect.Struct &#123; // 结构体 queue = append(queue, vi.Interface()) &#125; else &#123; ti := t.Field(i) if tagValue := ti.Tag.Get(tag); tagValue != &quot;&quot; &#123; // 存入map out[tagValue] = vi.Interface() &#125; &#125; break &#125; if vi.Kind() == reflect.Struct &#123; // 内嵌结构体 queue = append(queue, vi.Interface()) break &#125; // 一般字段 ti := t.Field(i) if tagValue := ti.Tag.Get(tag); tagValue != &quot;&quot; &#123; // 存入map out[tagValue] = vi.Interface() &#125; &#125; &#125; return out, nil&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang 时间相关的方法","slug":"language/golang/base/time","date":"2022-06-03T06:50:00.000Z","updated":"2023-12-25T05:56:09.171Z","comments":true,"path":"2022/06/03/language/golang/base/time/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/base/time/","excerpt":"","text":"间隔指定时间运行1234567891011121314package mainimport ( &quot;fmt&quot; &quot;time&quot;)func main() &#123; // 每隔一秒执行一次 tick := time.Tick(time.Second) for range tick &#123; fmt.Print(&quot;.&quot;) &#125;&#125; 时间格式化12345678910111213141516171819202122232425262728293031package mainimport ( &quot;fmt&quot; &quot;time&quot;)func main() &#123; currentTime := time.Now() fmt.Println(&quot;当前时间 : &quot;, currentTime) fmt.Println(&quot;当前时间字符串: &quot;, currentTime.String()) fmt.Println(&quot;MM-DD-YYYY : &quot;, currentTime.Format(&quot;01-02-2006&quot;)) fmt.Println(&quot;YYYY-MM-DD : &quot;, currentTime.Format(&quot;2006-01-02&quot;)) fmt.Println(&quot;YYYY.MM.DD : &quot;, currentTime.Format(&quot;2006.01.02 15:04:05&quot;)) fmt.Println(&quot;YYYY#MM#DD &#123;Special Character&#125; : &quot;, currentTime.Format(&quot;2006#01#02&quot;)) fmt.Println(&quot;YYYY-MM-DD hh:mm:ss : &quot;, currentTime.Format(&quot;2006-01-02 15:04:05&quot;)) fmt.Println(&quot;Time with MicroSeconds: &quot;, currentTime.Format(&quot;2006-01-02 15:04:05.000000&quot;)) fmt.Println(&quot;Time with NanoSeconds: &quot;, currentTime.Format(&quot;2006-01-02 15:04:05.000000000&quot;)) fmt.Println(&quot;ShortNum Month : &quot;, currentTime.Format(&quot;2006-1-02&quot;)) fmt.Println(&quot;LongMonth : &quot;, currentTime.Format(&quot;2006-January-02&quot;)) fmt.Println(&quot;ShortMonth : &quot;, currentTime.Format(&quot;2006-Jan-02&quot;)) fmt.Println(&quot;ShortYear : &quot;, currentTime.Format(&quot;06-Jan-02&quot;)) fmt.Println(&quot;LongWeekDay : &quot;, currentTime.Format(&quot;2006-01-02 15:04:05 Monday&quot;)) fmt.Println(&quot;ShortWeek Day : &quot;, currentTime.Format(&quot;2006-01-02 Mon&quot;)) fmt.Println(&quot;ShortDay : &quot;, currentTime.Format(&quot;Mon 2006-01-2&quot;)) fmt.Println(&quot;Short Hour Minute Second: &quot;, currentTime.Format(&quot;2006-01-02 3:4:5&quot;)) fmt.Println(&quot;Short Hour Minute Second: &quot;, currentTime.Format(&quot;2006-01-02 3:4:5 PM&quot;)) fmt.Println(&quot;Short Hour Minute Second: &quot;, currentTime.Format(&quot;2006-01-02 3:4:5 pm&quot;))&#125; 时间加减123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354func main() &#123; // Add 时间相加 now := time.Now() // ParseDuration parses a duration string. // A duration string is a possibly signed sequence of decimal numbers, // each with optional fraction and a unit suffix, // such as &quot;300ms&quot;, &quot;-1.5h&quot; or &quot;2h45m&quot;. // Valid time units are &quot;ns&quot;, &quot;us&quot; (or &quot;µs&quot;), &quot;ms&quot;, &quot;s&quot;, &quot;m&quot;, &quot;h&quot;. // 10分钟前 m, _ := time.ParseDuration(&quot;-1m&quot;) m1 := now.Add(m) fmt.Println(m1) // 8个小时前 h, _ := time.ParseDuration(&quot;-1h&quot;) h1 := now.Add(8 * h) fmt.Println(h1) // 一天前 d, _ := time.ParseDuration(&quot;-24h&quot;) d1 := now.Add(d) fmt.Println(d1) printSplit(50) // 10分钟后 mm, _ := time.ParseDuration(&quot;1m&quot;) mm1 := now.Add(mm) fmt.Println(mm1) // 8小时后 hh, _ := time.ParseDuration(&quot;1h&quot;) hh1 := now.Add(hh) fmt.Println(hh1) // 一天后 dd, _ := time.ParseDuration(&quot;24h&quot;) dd1 := now.Add(dd) fmt.Println(dd1) printSplit(50) // Sub 计算两个时间差 subM := now.Sub(m1) fmt.Println(subM.Minutes(), &quot;分钟&quot;) sumH := now.Sub(h1) fmt.Println(sumH.Hours(), &quot;小时&quot;) sumD := now.Sub(d1) fmt.Printf(&quot;%v 天\\n&quot;, sumD.Hours()/24)&#125; 1234567891011121314151617181920t := time.Now()addOneHour := t.Add(time.Hour)addTwoHour := t.Add(2 * time.Hour)fmt.Println(&quot;增加1小时：&quot;, addOneHour)fmt.Println(&quot;增加2小时：&quot;, addTwoHour)subTwoHour := t.Add(-2 * time.Hour)fmt.Println(&quot;减去2小时：&quot;, subTwoHour)addDate := t.AddDate(1, 0, 0)fmt.Println(&quot;增加1年：&quot;, addDate) // 2021-10-24 22:10:53.328973 +0800 CSTsubDate := t.AddDate(-1, 0, 0)fmt.Println(&quot;减去1年：&quot;, subDate) // 2019-10-24 22:10:53.328973 +0800 CSTbefore := t.Before(t.Add(time.Hour))fmt.Println(&quot;before：&quot;, before)after := t.After(t.Add(time.Hour))fmt.Println(&quot;after：&quot;, after) 计算耗时1234t := time.Now()time.Sleep(2e9) // 休眠2秒delta := time.Now().Sub(t)fmt.Println(&quot;时间差：&quot;, delta) // 2.0534341s 时间转换毫秒转Time 12nanoSecondToTime := time.Unix(0, 1603546715761482000)fmt.Println(&quot;毫秒值转Time：&quot;, nanoSecondToTime) // 2020-10-24 21:38:35.761482 +0800 CST Time转秒 12secondToTime := time.Unix(1603546715, 0)fmt.Println(&quot;秒值转Time：&quot;, secondToTime) // 2020-10-24 21:38:35 +0800 CST","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang 基础数据类型","slug":"language/golang/base/data-structure/base-data","date":"2022-06-03T06:45:00.000Z","updated":"2023-12-25T05:56:09.170Z","comments":true,"path":"2022/06/03/language/golang/base/data-structure/base-data/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/base/data-structure/base-data/","excerpt":"","text":"基础数据类型 基本类型：boolean，numeric，string 复合类型：array，struct，指针，function，interface，slice，map，channel类型。 Int 数据类型 取值范围 uint8 0 ~ 255 uint16 0 ~ 65535 uint32 0 ~ 4294967295 uint64 0 ~ 18446744073709551615 int8 -128 ~ 127 int16 -32768 ~ 32767 int32 -2147483648 ~ 2147483647 int64 -9223372036854775808 ~ 9223372036854775807 Map12make(map[string]int)make(map[string]int, 100) 类型转换Int12345678910111213141516171819202122// int转换成int32i32 = int32(i)// int转换成int64i64 = int64(i)// int转为floatb = float64(10)// int转为string// 通过fmt.Sprintf方法转换（%d代表Integer，i可以是int，int32，int64类型）str1 := fmt.Sprintf(&quot;%d&quot;, i)// 通过strconv.Itoa方法转换（i是int类型）str2 := strconv.Itoa(i)// 通过strconv.FormatInt方法转换（i可以是int，int32，int64类型）str3 := strconv.FormatInt(int64(i), 10)fmt.Sprintfstrconv.Itoastrconv.FormatInt int32，int64转换成int12345// int32转换成inti = int(int32)// int64转换成inti = int(int64) string1234567891011// string转换成int64strInt64, _ := strconv.ParseInt(str, 10, 64)// string转换成int32strInt32, _ := strconv.ParseInt(str, 10, 32)// 这里strInt32实际上还是int64类型的，只是截取了32位，所以最终还是要强转一下变成int32类型，如果不强转成int32是会编译报错的var realInt32 int32 = 0realInt32 := int32(strInt32)// string转换成intstrInt, err := strconv.Atoi(str)","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang 基础知识 导入本地包","slug":"language/golang/base/local-lib","date":"2022-06-03T06:40:00.000Z","updated":"2023-12-25T05:56:09.170Z","comments":true,"path":"2022/06/03/language/golang/base/local-lib/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/base/local-lib/","excerpt":"","text":"同一个项目下12345demo1|--- go.mod|--- main.go|--- demo2|------ main.go 12345678910package mainimport ( &quot;fmt&quot; &quot;demo2/demo2&quot; // demo2)func main() &#123; demo2.New() fmt.Println(&quot;main&quot;)&#125; 不同项目下1234567demo1|--- go.mod|--- main.godemo2|--- go.mod|--- main.go 12345678import ( &quot;fmt&quot; &quot;demo2&quot;)func main() &#123; demo2.New() fmt.Println(&quot;main&quot;)&#125; 123456module moduledemogo 1.17require &quot;demo2&quot; v0.0.0replace &quot;demo2&quot; =&gt; &quot;../demo2&quot;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang 数据结构 set","slug":"language/golang/base/data-structure/set","date":"2022-06-03T06:40:00.000Z","updated":"2023-12-25T05:56:09.170Z","comments":true,"path":"2022/06/03/language/golang/base/data-structure/set/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/base/data-structure/set/","excerpt":"","text":"Set简单实现12345678set := make(map[string]bool) // New empty setset[&quot;Foo&quot;] = true // Addfor k := range set &#123; // Loop fmt.Println(k)&#125;delete(set, &quot;Foo&quot;) // Deletesize := len(set) // Sizeexists := set[&quot;Foo&quot;] // Membership 优化一下1234567891011type void struct&#123;&#125;var member voidset := make(map[string]void) // New empty setset[&quot;Foo&quot;] = member // Addfor k := range set &#123; // Loop fmt.Println(k)&#125;delete(set, &quot;Foo&quot;) // Deletesize := len(set) // Size_, exists := set[&quot;Foo&quot;] // Membership golang-set123456789101112131415161718192021222324252627282930313233343536requiredClasses := mapset.NewSet()requiredClasses.Add(&quot;Cooking&quot;)requiredClasses.Add(&quot;English&quot;)requiredClasses.Add(&quot;Math&quot;)requiredClasses.Add(&quot;Biology&quot;)scienceSlice := []interface&#123;&#125;&#123;&quot;Biology&quot;, &quot;Chemistry&quot;&#125;scienceClasses := mapset.NewSetFromSlice(scienceSlice)electiveClasses := mapset.NewSet()electiveClasses.Add(&quot;Welding&quot;)electiveClasses.Add(&quot;Music&quot;)electiveClasses.Add(&quot;Automotive&quot;)bonusClasses := mapset.NewSet()bonusClasses.Add(&quot;Go Programming&quot;)bonusClasses.Add(&quot;Python Programming&quot;)//Show me all the available classes I can takeallClasses := requiredClasses.Union(scienceClasses).Union(electiveClasses).Union(bonusClasses)fmt.Println(allClasses) //Set&#123;Cooking, English, Math, Chemistry, Welding, Biology, Music, Automotive, Go Programming, Python Programming&#125;//Is cooking considered a science class?fmt.Println(scienceClasses.Contains(&quot;Cooking&quot;)) //false//Show me all classes that are not science classes, since I hate science.fmt.Println(allClasses.Difference(scienceClasses)) //Set&#123;Music, Automotive, Go Programming, Python Programming, Cooking, English, Math, Welding&#125;//Which science classes are also required classes?fmt.Println(scienceClasses.Intersect(requiredClasses)) //Set&#123;Biology&#125;//How many bonus classes do you offer?fmt.Println(bonusClasses.Cardinality()) //2//Do you have the following classes? Welding, Automotive and English?fmt.Println(allClasses.IsSuperset(mapset.NewSetFromSlice([]interface&#123;&#125;&#123;&quot;Welding&quot;, &quot;Automotive&quot;, &quot;English&quot;&#125;))) //true","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang 基础知识 mysql相关","slug":"language/golang/base/mysql-about","date":"2022-06-03T06:35:00.000Z","updated":"2023-12-25T05:56:09.170Z","comments":true,"path":"2022/06/03/language/golang/base/mysql-about/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/base/mysql-about/","excerpt":"","text":"将 sql 建表语句转为 struct1https://sql2struct.js.org/ 将 rows 转为 Map1234567891011121314151617181920212223242526272829303132333435363738// 把rows转换成记录func makeRowsMap(rows *sql.Rows) []map[string]interface&#123;&#125; &#123; colTypes, _ := rows.ColumnTypes() // 列信息 var rowParam = make([]interface&#123;&#125;, len(colTypes)) // 传入到 rows.Scan 的参数 数组 var rowValue = make([]interface&#123;&#125;, len(colTypes)) // 接收数据一行列的数组 for i, colType := range colTypes &#123; rowValue[i] = reflect.New(colType.ScanType()) // 跟据数据库参数类型，创建默认值 和类型 rowParam[i] = reflect.ValueOf(&amp;rowValue[i]).Interface() // 跟据接收的数据的类型反射出值的地址 //logging.Debugf(&quot;%s:%s,%s&quot;,colType.Name(),colType.ScanType().String(),colType.DatabaseTypeName()) // 字段明称：类型，数据库类型 &#125; var arrays []map[string]interface&#123;&#125; = make([]map[string]interface&#123;&#125;,0) for rows.Next() &#123; rows.Scan(rowParam...) record := make(map[string]interface&#123;&#125;) for i, colType := range colTypes &#123; if colType.ScanType().String() == &quot;time.Time&quot; &#123; // 日期要格式化下 record[colType.Name()] = rowValue[i].(time.Time).Format(&quot;2006-01-02 15:04:05&quot;) &#125;else if colType.DatabaseTypeName() == &quot;NUMERIC&quot; &#123; // number 居然是 interface&#123;&#125; 操蛋[]uint8的数组，后来发现转成string 后在转float record[colType.Name()],_ = strconv.ParseFloat(string(rowValue[i].([]byte)),64) &#125;else&#123; record[colType.Name()] = rowValue[i] &#125; //logging.Debugf(&quot;%s(%s-%s):%s&quot;,colType.Name(),colType.ScanType().String(),colType.DatabaseTypeName(),rowValue[i]) &#125; arrays = append(arrays, record) &#125; return arrays&#125;func byteToFloat64(bytes []byte) float64 &#123; bits := binary.LittleEndian.Uint64(bytes) return math.Float64frombits(bits)&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang Cobra 命令行解析组件","slug":"language/golang/libs/cobra","date":"2022-06-03T06:30:00.000Z","updated":"2023-12-25T05:56:09.171Z","comments":true,"path":"2022/06/03/language/golang/libs/cobra/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/libs/cobra/","excerpt":"","text":"Cobra 命令行特性 简单的基于子命令的 CLIs：app server、app fetch 等； 完全兼容 POSIX（可移植操作系统接口） 的标志（包括短版和长版） 嵌套子命令 全局、局部和级联的标志 使用 cobra init appname 和 cobra add cmdname 轻松生成应用程序和命令 智能提示（app srver …did you mean app server） 自动生成命令和标志的帮助 自动识别 -h、--help 等帮助标识 自动为你的应用程序生成的 bash 自动完成 自动为你的应用程序生成 man 手册 命令别名，以便你可以更改内容而不会破坏它们 定义自己的帮助，用法等的灵活性。 可选与 viper 紧密集成，可用于 12factor 应用程序 安装Cobra 非常易用，首先使用 go get 命令安装最新版本。此命令将安装 cobra generator 的可执行文件及其依赖项： 1$ go get -u github.com/spf13/cobra/cobra 概念Cobra 构建在命令（commands）、参数（arguments）和 标志（flags）上。 Commands 代表动作，Args 是事物，Flags 是这些动作的修饰符。 最好的应用程序在使用时会像句子一样读起来。用户将知道如何使用该应用程序，因为他们将自然地了解如何使用它。 遵循的模式是 APPNAME VERB NOUN --ADJECTIVE。 或 APPNAME COMMAND ARG --FLAG 一些真实的例子可以更好地说明这一点。 在以下示例中，server 是命令，port 是标志： 1hugo server --port=1313 在此命令中，我们告诉 Git 克隆 url 的内容： 1git clone URL --bare 命令（Command）命令是应用程序的核心。应用程序提供的每一个交互都包含在 Command 中。一个命令可以有子命令和可选的运行一个动作。 在上面的示例中，server 是命令。 cobra.Command API 标志（Flags）一个标志是一种修饰命令行为的方式。Cobra 支持完全符合 POSIX（可移植操作系统接口） 的标志和 Go flag 包。 Cobra 命令可以定义一直保留到子命令的标志和仅可用于该命令的标志。 在上面的例子中，port 是标志。 标志的功能是 pflag 库提供的，该库是一个标准库的 fork，在维护相同接口的基础上兼容了 POSIX（可移植操作系统接口）。 入门欢迎大家提供自己的项目组织结构，但是通常基于 Cobra 的应用程序将遵循以下组织结构 ​ cmd 放置命令的文件夹 add.go your.go commands.go here.go ​ main.go 应用程序入口 在 Cobra 应用程序中，通常 main.go 文件非常。它有一个目的：初始化 Cobra。 123456789package mainimport ( &quot;&#123;pathToYourApp&#125;/cmd&quot;)func main() &#123; cmd.Execute()&#125; 使用 Cobra 生成器Cobra 提供了 CLI 来创建您的应用程序和添加任意你想要的命令。这是将 Cobra 集成到您的应用程序中的最简单方法。 这里 你可以查看更多关于生成器的资料。 使用 Cobra 库要手动接入 Cobra，您需要创建一个 main.go 文件和 rootCmd 文件。您可以选择提供合适的其他命令。 创建 rootCmdCobra 不需要任何特殊的构造函数。只需创建您的命令。 理想情况下，将其放置在 /cmd/root.go 中： 1234567891011121314151617181920// rootCmd 代表没有调用子命令时的基础命令var rootCmd = &amp;cobra.Command&#123; Use: &quot;hugo&quot;, Short: &quot;Hugo is a very fast static site generator&quot;, Long: `A Fast and Flexible Static Site Generator built with love by spf13 and friends in Go. Complete documentation is available at http://hugo.spf13.com`, // 如果有相关的 action 要执行，请取消下面这行代码的注释 // Run: func(cmd *cobra.Command, args []string) &#123; &#125;,&#125;// Execute 将所有子命令添加到root命令并适当设置标志。// 这由 main.main() 调用。它只需要对 rootCmd 调用一次。func Execute() &#123; if err := rootCmd.Execute(); err != nil &#123; fmt.Println(err) os.Exit(1) &#125;&#125; 您还将在 init() 函数中定义标志并处理配置。例子如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// cmd/root.gopackage cmdimport ( &quot;fmt&quot; &quot;os&quot; &quot;github.com/spf13/cobra&quot; homedir &quot;github.com/mitchellh/go-homedir&quot; &quot;github.com/spf13/viper&quot;)var cfgFile stringvar projectBase stringvar userLicense string// rootCmd 代表没有调用子命令时的基础命令var rootCmd = &amp;cobra.Command&#123; Use: &quot;hugo&quot;, Short: &quot;Hugo is a very fast static site generator&quot;, Long: `A Fast and Flexible Static Site Generator built with love by spf13 and friends in Go. Complete documentation is available at http://hugo.spf13.com`, // 如果有相关的 action 要执行，请取消下面这行代码的注释 // Run: func(cmd *cobra.Command, args []string) &#123; &#125;,&#125;// Execute 将所有子命令添加到root命令并适当设置标志。会被 main.main() 调用一次。func Execute() &#123; if err := rootCmd.Execute(); err != nil &#123; fmt.Println(err) os.Exit(1) &#125;&#125;func init() &#123; cobra.OnInitialize(initConfig) rootCmd.PersistentFlags().StringVar(&amp;cfgFile, &quot;config&quot;, &quot;&quot;, &quot;config file (default is $HOME/.cobra.yaml)&quot;) rootCmd.PersistentFlags().StringVarP(&amp;projectBase, &quot;projectbase&quot;, &quot;b&quot;, &quot;&quot;, &quot;base project directory eg. github.com/spf13/&quot;) rootCmd.PersistentFlags().StringP(&quot;author&quot;, &quot;a&quot;, &quot;YOUR NAME&quot;, &quot;Author name for copyright attribution&quot;) rootCmd.PersistentFlags().StringVarP(&amp;userLicense, &quot;license&quot;, &quot;l&quot;, &quot;&quot;, &quot;Name of license for the project (can provide `licensetext` in config)&quot;) rootCmd.PersistentFlags().Bool(&quot;viper&quot;, true, &quot;Use Viper for configuration&quot;) viper.BindPFlag(&quot;author&quot;, rootCmd.PersistentFlags().Lookup(&quot;author&quot;)) viper.BindPFlag(&quot;projectbase&quot;, rootCmd.PersistentFlags().Lookup(&quot;projectbase&quot;)) viper.BindPFlag(&quot;useViper&quot;, rootCmd.PersistentFlags().Lookup(&quot;viper&quot;)) viper.SetDefault(&quot;author&quot;, &quot;NAME HERE &lt;EMAIL ADDRESS&gt;&quot;) viper.SetDefault(&quot;license&quot;, &quot;apache&quot;)&#125;func initConfig() &#123; // Don&#x27;t forget to read config either from cfgFile or from home directory! if cfgFile != &quot;&quot; &#123; // Use config file from the flag. viper.SetConfigFile(cfgFile) &#125; else &#123; // Find home directory. home, err := homedir.Dir() if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; // Search config in home directory with name &quot;.cobra&quot; (without extension). viper.AddConfigPath(home) viper.SetConfigName(&quot;.cobra&quot;) &#125; if err := viper.ReadInConfig(); err != nil &#123; fmt.Println(&quot;Can&#x27;t read config:&quot;, err) os.Exit(1) &#125;&#125; 创建 main.go有了根命令，你需要一个 main 函数去执行它。为了清晰起见，Execute 应该在根目录上运行，尽管可以在任何命令上调用它。 在 Cobra 应用中，main.go 是非常简单的。它只有一个作用——初始化 Cobra。 1234567891011// main.gopackage mainimport ( &quot;&#123;pathToYourApp&#125;/cmd&quot;)func main() &#123; cmd.Execute()&#125; 创建额外的命令可以定义其他命令，并且通常在 cmd/ 目录中为每个命令提供自己的文件。 如果要创建 version 命令，则可以创建 cmd/version.go 并使用以下命令进行填充： 12345678910111213141516171819202122// cmd/version.gopackage cmdimport ( &quot;fmt&quot; &quot;github.com/spf13/cobra&quot;)func init() &#123; rootCmd.AddCommand(versionCmd)&#125;var versionCmd = &amp;cobra.Command&#123; Use: &quot;version&quot;, Short: &quot;Print the version number of Hugo&quot;, Long: `All software has versions. This is Hugo&#x27;s`, Run: func(cmd *cobra.Command, args []string) &#123; fmt.Println(&quot;Hugo Static Site Generator v0.9 -- HEAD&quot;) &#125;,&#125; 使用标志标志提供修饰符以控制命令的操作方式。 由于标志是在不同位置定义和使用的，我们需要在外部定义一个具有正确作用域的变量，以分配要使用的标志。 123var verbose boolvar source string 这里有两种不同分配标志的方法。 持久标志标志可以是 “persistent” 的，这意味着该标志将可用于分配给它的命令以及该命令下的每个命令。对于全局标志，将标志分配为根上的持久标志。 12rootCmd.PersistentFlags().BoolVarP(&amp;verbose, &quot;verbose&quot;, &quot;v&quot;, false, &quot;verbose output&quot;) 本地标志也可以在本地分配一个标志，该标志仅适用于该特定命令。 12rootCmd.Flags().StringVarP(&amp;source, &quot;source&quot;, &quot;s&quot;, &quot;&quot;, &quot;Source directory to read from&quot;) 父命令上的本地标志默认情况下，Cobra 仅解析目标命令上的本地标志，而忽略父命令上的任何本地标志。通过启用 Command.TraverseChildren，Cobra 将在执行目标命令之前解析每个命令上的本地标志 12345command := cobra.Command&#123; Use: &quot;print [OPTIONS] [COMMANDS]&quot;, TraverseChildren: true,&#125; 用配置绑定标志您还可以将标志与 viper 绑定： 1234567var author stringfunc init() &#123; rootCmd.PersistentFlags().StringVar(&amp;author, &quot;author&quot;, &quot;YOUR NAME&quot;, &quot;Author name for copyright attribution&quot;) viper.BindPFlag(&quot;author&quot;, rootCmd.PersistentFlags().Lookup(&quot;author&quot;))&#125; 在此示例中，持久标记 author 与 viper 绑定。请注意，当用户未提供 --author 标志时，变量 author 不会设置为 config 中的值。 更多信息请查看 viper。 必需标志标志默认是可选的。如果你想在缺少标志时命令报错，请设置该标志为必需： 12345var region stringrootCmd.Flags().StringVarP(&amp;region, &quot;region&quot;, &quot;r&quot;, &quot;&quot;, &quot;AWS region (required)&quot;)rootCmd.MarkFlagRequired(&quot;region&quot;) 位置和自定义参数可以使用 Command 的 Args 字段指定位置参数的验证。 下面的验证符是内置的： NoArgs - 如果有任何位置参数，该命令将报告错误。 ArbitraryArgs - 命令将接受任意参数 OnlyValidArgs - 如果 Command 的 ValidArgs 字段中不存在该位置参数，则该命令将报告错误。 MinimumNArgs(int) - 如果不存在至少 N 个位置参数，则该命令将报告错误。 MaximumNArgs(int) - 如果存在超过 N 个位置参数，则该命令将报告错误。 ExactArgs(int) - 如果不存在 N 个位置参数，则该命令将报告错误。 ExactValidArgs(int) - 如果没有确切的 N 个位置参数，或者如果 Command 的 ValidArgs 字段中不存在该位置参数，则该命令将报告并出错。 RangeArgs(min, max) - 如果 args 的数目不在期望的 args 的最小和最大数目之间，则该命令将报告并出错。 内置验证符使用实例： 123456789var cmd = &amp;cobra.Command&#123; Use: &quot;hello&quot;, Short: &quot;hello&quot;, Args: cobra.MinimumNArgs(2), Run: func(cmd *cobra.Command, args []string) &#123; fmt.Println(&quot;Hello, World!&quot;) &#125;,&#125; 如果只传递一个位置参数会报 Error: requires at least 2 arg(s), only received 1 的警告。 设置自定义验证器的示例： 12345678910111213141516var cmd = &amp;cobra.Command&#123; Short: &quot;hello&quot;, Args: func(cmd *cobra.Command, args []string) error &#123; if len(args) &lt; 1 &#123; return errors.New(&quot;requires at least one arg&quot;) &#125; if myapp.IsValidColor(args[0]) &#123; return nil &#125; return fmt.Errorf(&quot;invalid color specified: %s&quot;, args[0]) &#125;, Run: func(cmd *cobra.Command, args []string) &#123; fmt.Println(&quot;Hello, World!&quot;) &#125;,&#125; 示例在下面的例子中，我们定义了三个命令。两个在顶层，一个（cmdTimes）是子命令。在这种情况下，根目录不可执行，这意味着需要一个子命令。通过不为 rootCmd 提供 Run 来实现。 我们只为一个命令定义了一个标志。 关于标志的文档在 [pflag]github.com&#x2F;spf13&#x2F;pflag…%E3%80%82) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package mainimport ( &quot;fmt&quot; &quot;strings&quot; &quot;github.com/spf13/cobra&quot;)func main() &#123; var echoTimes int var cmdPrint = &amp;cobra.Command&#123; Use: &quot;Print [string to print]&quot;, Short: &quot;Print anything to the screen&quot;, Long: `print is for printing anything back to the screen.For many years people have printed back to the screen.`, Args: cobra.MinimumNArgs(1), Run: func(cmd *cobra.Command, args []string) &#123; fmt.Println(&quot;Print: &quot; + strings.Join(args, &quot; &quot;)) &#125;, &#125; var cmdEcho = &amp;cobra.Command&#123; Use: &quot;echo [string to echo]&quot;, Short: &quot;Echo anything to the screen&quot;, Long: `echo is for echoing anything back.Echo works a lot like print, except it has a child command.`, Args: cobra.MinimumNArgs(1), Run: func(cmd *cobra.Command, args []string) &#123; fmt.Println(&quot;Print: &quot; + strings.Join(args, &quot; &quot;)) &#125;, &#125; var cmdTimes = &amp;cobra.Command&#123; Use: &quot;times [# times] [string to echo]&quot;, Short: &quot;Echo anyting to the screen more times&quot;, Long: `echo things multiple times back to the user y providing a count and a string.`, Args: cobra.MinimumNArgs(1), Run: func(cmd *cobra.Command, args []string) &#123; for i := 0; i &lt; echoTimes; i++ &#123; fmt.Println(&quot;Echo: &quot; + strings.Join(args, &quot; &quot;)) &#125; &#125;, &#125; cmdTimes.Flags().IntVarP(&amp;echoTimes, &quot;times&quot;, &quot;t&quot;, 1, &quot;times to echo the input&quot;) // 设置根命令 var rootCmd = &amp;cobra.Command&#123;Use: &quot;app&quot;&#125; rootCmd.AddCommand(cmdPrint, cmdEcho) cmdEcho.AddCommand(cmdTimes) // 初始化应用 rootCmd.Execute()&#125; 更复杂的应用，请参考 Hugo 或者 GitHub CLI。 帮助命令当你添加了子命令，Cobra 会自动添加一些帮助命令。当你执行 app help 命令时会显示帮助信息。另外，help 还支持其他命令作为输入参数。举例来说，你有一个没有额外配置的 create 命令，app help create 是有效的。每一个命令还会自动获取一个 --help 标志。 示例以下输出由 Cobra 自动生成。 除了命令和标志定义外，什么都不需要。 help 就像其他命令一样。并没有特殊的逻辑或行为。实际上，你可以根据需要提供自己的服务。 定义你自己的 help你可以使用下面的方法提供你自己的 Help 命令或模板。 1234cmd.SetHelpCommand(cmd *Command)cmd.setHelpCommand(f func(*Command, []string))cmd.setHelpTemplate(s string) 后两者也适用于所有子命令。 使用信息当用户提供无效的标志或无效的命令时，Cobra 会通过向用户显示 usage 进行响应。 定义你自己的使用信息你可以提供你自己的 usage 函数或模板。像 help 一样，函数和模板可通过公共方法重写： 123cmd.SetUsageFunc(f func(*Command) error)cmd.SetUsageTemplate(s string) 可以参考 GitHub CLI 的写法。 版本标志如果给根命令设置了 Version 字段，Cobra 会添加一个顶级的 --version 标志。运行带有 –version 标志的应用程序，将使用版本模板将版本打印到 stdout。模板可以使用 cmd.SetVersionTemplate(s string) 函数自定义。 SetVersionTemplate 的使用可以参考 GitHub CLI PreRun 和 PostRun Hooks可以在执行命令之前和之后运行一个函数。PersistentPreRun 和 PreRun 函数将在 Run 之前执行。PersistentPostRun 和 PostRun 会在 Run 之后运行。如果子级未声明自己的 Persistent * Run 函数，则子级将继承父级的。这些函数的执行顺续如下： PersistentPreRun PreRun Run PostRun PersistentPostRun 下面这个包含了两个命令的例子使用了这些特性。当子命令执行时，它会运行根命令的 PersistentPreRun，但是不会运行根命令的 PersistentPostRun： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package mainimport ( &quot;fmt&quot; &quot;github.com/spf13/cobra&quot;)func main() &#123; var rootCmd = &amp;cobra.Command&#123; Use: &quot;root [sub]&quot;, Short: &quot;My root command&quot;, PersistentPreRun: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside rootCmd PersistentPreRun with args: %v\\n&quot;, args) &#125;, PreRun: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside rootCmd PreRun with args: %v\\n&quot;, args) &#125;, Run: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside rootCmd Run with args: %v\\n&quot;, args) &#125;, PostRun: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside rootCmd PostRun with args: %v\\n&quot;, args) &#125;, PersistentPostRun: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside rootCmd PersistentPostRun with args: %v\\n&quot;, args) &#125;, &#125; subCmd := &amp;cobra.Command&#123; Use: &quot;sub [no options!]&quot;, Short: &quot;My subcommand&quot;, PreRun: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside subCmd PreRun with args: %v\\n&quot;, args) &#125;, Run: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside subCmd Run with args: %v\\n&quot;, args) &#125;, PostRun: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside subCmd PostRun with args: %v\\n&quot;, args) &#125;, PersistentPostRun: func(cmd *cobra.Command, args []string) &#123; fmt.Printf(&quot;Inside subCmd PersistentPostRun with args: %v\\n&quot;, args) &#125;, &#125; rootCmd.AddCommand(subCmd) rootCmd.SetArgs([]string&#123;&quot;&quot;&#125;) rootCmd.Execute() fmt.Println() rootCmd.SetArgs([]string&#123;&quot;sub&quot;, &quot;arg1&quot;, &quot;arg2&quot;&#125;) rootCmd.Execute()&#125; 输出： 123456789101112Inside rootCmd PersistentPreRun with args: []Inside rootCmd PreRun with args: []Inside rootCmd Run with args: []Inside rootCmd PostRun with args: []Inside rootCmd PersistentPostRun with args: []Inside rootCmd PersistentPreRun with args: [arg1 arg2]Inside subCmd PreRun with args: [arg1 arg2]Inside subCmd Run with args: [arg1 arg2]Inside subCmd PostRun with args: [arg1 arg2]Inside subCmd PersistentPostRun with args: [arg1 arg2] “unknown command” 时的提示当 &quot;unknown command&quot; 错误发生时，Cobra 会自动打印提示。这和 git 命令的行为一致。比如 12345678$ hugo sreverError: unknown command &quot;srever&quot; for &quot;hugo&quot;Did you mean this? serverRun &#x27;hugo --help&#x27; for usage. 系统会根据注册的每个子命令自动生成建议，并使用萊文斯坦距離的实现。每个匹配最小距离 2（忽略大小写）的注册命令都将显示为建议。 如果需要禁用建议或在命令中调整字符串距离，请使用： 12cmd.DisableSuggestions = true 或 12cmd.SuggestionsMinimumDistance = 1 您还可以使用 SuggestFor 属性显式为给定命令设置建议的名称。这样就可以针对不是距离很近的字符串提出建议，但是对于您的命令集和不希望使用别名的命令来说，它们都是有意义的。比如： 1234567$ kubectl removeError: unknown command &quot;remove&quot; for &quot;kubectl&quot;Did you mean this? deleteRun &#x27;kubectl help&#x27; for usage.","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang gin web组件","slug":"language/golang/libs/gin","date":"2022-06-03T06:25:00.000Z","updated":"2023-12-25T05:56:09.172Z","comments":true,"path":"2022/06/03/language/golang/libs/gin/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/libs/gin/","excerpt":"","text":"Gin Web下载Gin 1234go get -u github.com/gin-gonic/gingo mod init gingo mod edit -require github.com/gin-gonic/gin@latest http服务123456789101112131415161718192021222324package mainimport ( &quot;github.com/gin-gonic/gin&quot; &quot;net/http&quot;)func main() &#123; // 初始化引擎 engine := gin.Default() // 注册一个路由和处理函数 engine.Any(&quot;/&quot;, WebRoot) // 绑定端口，然后启动应用 engine.Run(&quot;:9205&quot;)&#125;/*** 根请求处理函数* 所有本次请求相关的方法都在 context 中，完美* 输出响应 hello, world*/func WebRoot(context *gin.Context) &#123; context.String(http.StatusOK, &quot;hello, world&quot;)&#125; 基础组件构建的http服务 1234567891011121314package mainimport ( &quot;fmt&quot; &quot;net/http&quot;)func handler (writer http.ResponseWriter, request *http.Request)&#123; fmt.Fprintf(writer, &quot;hello %s&quot;, request.URL.Path[1:])&#125;func main() &#123; http.HandleFunc(&quot;/&quot;, handler) http.ListenAndServe(&quot;:8080&quot;, nil)&#125; Route1234567891011121314151617181920package mainimport ( &quot;github.com/gin-gonic/gin&quot; &quot;net/http&quot;)func main() &#123; router := gin.Default() router.GET(&quot;/someGet&quot;,getting) //router.POST(&quot;/somePost&quot;, posting) //router.PUT(&quot;/somePut&quot;, putting) //router.DELETE(&quot;/someDelete&quot;, deleting) //router.PATCH(&quot;/somePatch&quot;, patching) //router.HEAD(&quot;/someHead&quot;, head) //router.OPTIONS(&quot;/someOptions&quot;, options) router.Run()&#125;func getting(context *gin.Context)&#123; context.String(http.StatusOK, &quot;getting&quot;)&#125; 解析路径参数123456789101112131415161718// 注册一个动态路由// 可以匹配 /user/joy// 不能匹配 /user 和 /user/router.GET(&quot;/user/:name&quot;, func(c *gin.Context) &#123; // 使用 c.Param(key) 获取 url 参数 name := c.Param(&quot;name&quot;) c.String(http.StatusOK, &quot;Hello %s&quot;, name)&#125;)// 注册一个高级的动态路由// 该路由会匹配 /user/john/ 和 /user/john/send// 如果没有任何路由匹配到 /user/john, 那么他就会重定向到 /user/john/，从而被该方法匹配到router.GET(&quot;/user/:name/*action&quot;, func(c *gin.Context) &#123; name := c.Param(&quot;name&quot;) action := c.Param(&quot;action&quot;) message := name + &quot; is &quot; + action c.String(http.StatusOK, message)&#125;) 获取Query参数1234567891011121314151617type User struct&#123; Name string `form:&quot;name&quot;` role string `form:&quot;role&quot;`&#125;// 匹配users?name=xxx&amp;role=xxx，role可选r.GET(&quot;/users&quot;, func(c *gin.Context) &#123; name := c.Query(&quot;name&quot;) role := c.DefaultQuery(&quot;role&quot;, &quot;teacher&quot;) var user User err:=ctx.ShouldBindQuery(&amp;user) if err== nil&#123; fmt.Println(&quot;user&quot;, user) &#125; c.String(http.StatusOK, &quot;%s is a %s&quot;, name, role)&#125;) 获取Post参数123456789101112131415161718type User struct&#123; Name string `form:&quot;name&quot;` Pwd int `form:&quot;password&quot;`&#125;// POSTr.POST(&quot;/form&quot;, func(c *gin.Context) &#123; username := c.PostForm(&quot;username&quot;) password := c.DefaultPostForm(&quot;password&quot;, &quot;000000&quot;) // 可设置默认值 var user User err:=ctx.ShouldBind(&amp;user) if err== nil&#123; fmt.Println(&quot;user&quot;, user) &#125; c.JSON(http.StatusOK, gin.H&#123; &quot;username&quot;: username, &quot;password&quot;: password, &#125;)&#125;) Map参数（字典参数）123456789r.POST(&quot;/post&quot;, func(c *gin.Context) &#123; ids := c.QueryMap(&quot;ids&quot;) names := c.PostFormMap(&quot;names&quot;) c.JSON(http.StatusOK, gin.H&#123; &quot;ids&quot;: ids, &quot;names&quot;: names, &#125;)&#125;) 12curl -g &quot;http://localhost:9999/post?ids[Jack]=001&amp;ids[Tom]=002&quot; -X POST -d &#x27;names[a]=Sam&amp;names[b]=David&#x27;&#123;&quot;ids&quot;:&#123;&quot;Jack&quot;:&quot;001&quot;,&quot;Tom&quot;:&quot;002&quot;&#125;,&quot;names&quot;:&#123;&quot;a&quot;:&quot;Sam&quot;,&quot;b&quot;:&quot;David&quot;\\&#125;\\&#125; 分组路由(Grouping Routes)123456789101112131415161718// group routes 分组路由defaultHandler := func(c *gin.Context) &#123; c.JSON(http.StatusOK, gin.H&#123; &quot;path&quot;: c.FullPath(), &#125;)&#125;// group: v1v1 := r.Group(&quot;/v1&quot;)&#123; v1.GET(&quot;/posts&quot;, defaultHandler) v1.GET(&quot;/series&quot;, defaultHandler)&#125;// group: v2v2 := r.Group(&quot;/v2&quot;)&#123; v2.GET(&quot;/posts&quot;, defaultHandler) v2.GET(&quot;/series&quot;, defaultHandler)&#125; 重定向12345678r.GET(&quot;/redirect&quot;, func(c *gin.Context) &#123; c.Redirect(http.StatusMovedPermanently, &quot;/index&quot;)&#125;)r.GET(&quot;/goindex&quot;, func(c *gin.Context) &#123; c.Request.URL.Path = &quot;/&quot; r.HandleContext(c)&#125;) 上传文件12345r.POST(&quot;/upload1&quot;, func(c *gin.Context) &#123; file, _ := c.FormFile(&quot;file&quot;) // c.SaveUploadedFile(file, dst) c.String(http.StatusOK, &quot;%s uploaded!&quot;, file.Filename)&#125;) 123curl -X POST http://localhost:8080/upload \\ -F &quot;file=@/Users/appleboy/test.zip&quot; \\ -H &quot;Content-Type: multipart/form-data&quot; 多个文件1234567891011r.POST(&quot;/upload2&quot;, func(c *gin.Context) &#123; // Multipart form form, _ := c.MultipartForm() files := form.File[&quot;upload[]&quot;] for _, file := range files &#123; log.Println(file.Filename) // c.SaveUploadedFile(file, dst) &#125; c.String(http.StatusOK, &quot;%d files uploaded!&quot;, len(files))&#125;) 1234curl -X POST http://localhost:8080/upload \\ -F &quot;upload[]=@/Users/appleboy/test1.zip&quot; \\ -F &quot;upload[]=@/Users/appleboy/test2.zip&quot; \\ -H &quot;Content-Type: multipart/form-data&quot; 中间件12345678910111213141516171819202122232425262728293031323334353637383940package mainimport ( &quot;github.com/gin-gonic/gin&quot; &quot;log&quot; //&quot;net/http&quot;)func main() &#123; router := gin.Default() // 注册一个路由，使用了 middleware1，middleware2 两个中间件 router.GET(&quot;/someGet&quot;, middleware1, middleware2, handler) // 路由组使用中间件 v1 := router.Group(&quot;/v1&quot;,middleware1) &#123; v1.GET(&quot;/home&quot;,handler) &#125; // 默认绑定 :8080 router.Run()&#125;func middleware1(c *gin.Context) &#123; log.Println(&quot;exec middleware1&quot;) //你可以写一些逻辑代码 // 执行该中间件之后的逻辑 c.Next()&#125;func middleware2(c *gin.Context) &#123; log.Println(&quot;arrive at middleware2&quot;) // 执行该中间件之前，先跳到流程的下一个方法 c.Next() // 流程中的其他逻辑已经执行完了 log.Println(&quot;exec middleware2&quot;) //你可以写一些逻辑代码&#125;func handler(c *gin.Context) &#123; log.Println(&quot;exec handler&quot;)&#125; 123456789101112func Logger() gin.HandlerFunc &#123; return func(c *gin.Context) &#123; t := time.Now() // 给Context实例设置一个值 c.Set(&quot;geektutu&quot;, &quot;1111&quot;) // 请求前 c.Next() // 请求后 latency := time.Since(t) log.Print(latency) &#125;&#125; 热加载12go get -v -u github.com/pilu/freshfresh 日志文件123456789101112131415161718func main() &#123; // Disable Console Color, you don&#x27;t need console color when writing the logs to file. gin.DisableConsoleColor() // Logging to a file. f, _ := os.Create(&quot;gin.log&quot;) gin.DefaultWriter = io.MultiWriter(f) // Use the following code if you need to write the logs to file and console at the same time. // gin.DefaultWriter = io.MultiWriter(f, os.Stdout) router := gin.Default() router.GET(&quot;/ping&quot;, func(c *gin.Context) &#123; c.String(200, &quot;pong&quot;) &#125;) router.Run(&quot;:8080&quot;)&#125; 日志格式化12345678910111213141516171819202122232425262728func main() &#123; router := gin.New() // LoggerWithFormatter middleware will write the logs to gin.DefaultWriter // By default gin.DefaultWriter = os.Stdout router.Use(gin.LoggerWithFormatter(func(param gin.LogFormatterParams) string &#123; // your custom format return fmt.Sprintf(&quot;%s - [%s] \\&quot;%s %s %s %d %s \\&quot;%s\\&quot; %s\\&quot;\\n&quot;, param.ClientIP, param.TimeStamp.Format(time.RFC1123), param.Method, param.Path, param.Request.Proto, param.StatusCode, param.Latency, param.Request.UserAgent(), param.ErrorMessage, ) &#125;)) router.Use(gin.Recovery()) router.GET(&quot;/ping&quot;, func(c *gin.Context) &#123; c.String(200, &quot;pong&quot;) &#125;) router.Run(&quot;:8080&quot;)&#125; 自定义验证器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package mainimport ( &quot;net/http&quot; &quot;time&quot; &quot;github.com/gin-gonic/gin&quot; &quot;github.com/gin-gonic/gin/binding&quot; &quot;github.com/go-playground/validator/v10&quot;)// Booking contains binded and validated data.type Booking struct &#123; CheckIn time.Time `form:&quot;check_in&quot; binding:&quot;required,bookabledate&quot; time_format:&quot;2006-01-02&quot;` CheckOut time.Time `form:&quot;check_out&quot; binding:&quot;required,gtfield=CheckIn&quot; time_format:&quot;2006-01-02&quot;`&#125;var bookableDate validator.Func = func(fl validator.FieldLevel) bool &#123; date, ok := fl.Field().Interface().(time.Time) if ok &#123; today := time.Now() if today.After(date) &#123; return false &#125; &#125; return true&#125;func main() &#123; route := gin.Default() if v, ok := binding.Validator.Engine().(*validator.Validate); ok &#123; v.RegisterValidation(&quot;bookabledate&quot;, bookableDate) &#125; route.GET(&quot;/bookable&quot;, getBookable) route.Run(&quot;:8085&quot;)&#125;func getBookable(c *gin.Context) &#123; var b Booking if err := c.ShouldBindWith(&amp;b, binding.Query); err == nil &#123; c.JSON(http.StatusOK, gin.H&#123;&quot;message&quot;: &quot;Booking dates are valid!&quot;&#125;) &#125; else &#123; c.JSON(http.StatusBadRequest, gin.H&#123;&quot;error&quot;: err.Error()&#125;) &#125;&#125; 12345678$ curl &quot;localhost:8085/bookable?check_in=2030-04-16&amp;check_out=2030-04-17&quot;&#123;&quot;message&quot;:&quot;Booking dates are valid!&quot;&#125;$ curl &quot;localhost:8085/bookable?check_in=2030-03-10&amp;check_out=2030-03-09&quot;&#123;&quot;error&quot;:&quot;Key: &#x27;Booking.CheckOut&#x27; Error:Field validation for &#x27;CheckOut&#x27; failed on the &#x27;gtfield&#x27; tag&quot;&#125;$ curl &quot;localhost:8085/bookable?check_in=2000-03-09&amp;check_out=2000-03-10&quot;&#123;&quot;error&quot;:&quot;Key: &#x27;Booking.CheckIn&#x27; Error:Field validation for &#x27;CheckIn&#x27; failed on the &#x27;bookabledate&#x27; tag&quot;&#125;% 仅绑定查询字段12345678910111213141516171819202122232425262728package mainimport ( &quot;log&quot; &quot;github.com/gin-gonic/gin&quot;)type Person struct &#123; Name string `form:&quot;name&quot;` Address string `form:&quot;address&quot;`&#125;func main() &#123; route := gin.Default() route.Any(&quot;/testing&quot;, startPage) route.Run(&quot;:8085&quot;)&#125;func startPage(c *gin.Context) &#123; var person Person if c.ShouldBindQuery(&amp;person) == nil &#123; log.Println(&quot;====== Only Bind By Query String ======&quot;) log.Println(person.Name) log.Println(person.Address) &#125; c.String(200, &quot;Success&quot;)&#125; 绑定Query和Pose1234567891011121314151617181920212223242526272829303132333435363738package mainimport ( &quot;log&quot; &quot;time&quot; &quot;github.com/gin-gonic/gin&quot;)type Person struct &#123; Name string `form:&quot;name&quot;` Address string `form:&quot;address&quot;` Birthday time.Time `form:&quot;birthday&quot; time_format:&quot;2006-01-02&quot; time_utc:&quot;1&quot;` CreateTime time.Time `form:&quot;createTime&quot; time_format:&quot;unixNano&quot;` UnixTime time.Time `form:&quot;unixTime&quot; time_format:&quot;unix&quot;`&#125;func main() &#123; route := gin.Default() route.GET(&quot;/testing&quot;, startPage) route.Run(&quot;:8085&quot;)&#125;func startPage(c *gin.Context) &#123; var person Person // If `GET`, only `Form` binding engine (`query`) used. // If `POST`, first checks the `content-type` for `JSON` or `XML`, then uses `Form` (`form-data`). // See more at https://github.com/gin-gonic/gin/blob/master/binding/binding.go#L48 if c.ShouldBind(&amp;person) == nil &#123; log.Println(person.Name) log.Println(person.Address) log.Println(person.Birthday) log.Println(person.CreateTime) log.Println(person.UnixTime) &#125; c.String(200, &quot;Success&quot;)&#125; 1curl -X GET &quot;localhost:8085/testing?name=appleboy&amp;address=xyz&amp;birthday=1992-03-15&amp;createTime=1562400033000000123&amp;unixTime=1562400033&quot; 绑定Uri123456789101112131415161718192021package mainimport &quot;github.com/gin-gonic/gin&quot;type Person struct &#123; ID string `uri:&quot;id&quot; binding:&quot;required,uuid&quot;` Name string `uri:&quot;name&quot; binding:&quot;required&quot;`&#125;func main() &#123; route := gin.Default() route.GET(&quot;/:name/:id&quot;, func(c *gin.Context) &#123; var person Person if err := c.ShouldBindUri(&amp;person); err != nil &#123; c.JSON(400, gin.H&#123;&quot;msg&quot;: err&#125;) return &#125; c.JSON(200, gin.H&#123;&quot;name&quot;: person.Name, &quot;uuid&quot;: person.ID&#125;) &#125;) route.Run(&quot;:8088&quot;)&#125; 12curl -v localhost:8088/thinkerou/987fbc97-4bed-5078-9f07-9141ba07c9f3$ curl -v localhost:8088/thinkerou/not-uuid 静态文件Html模板(Template)123456789101112131415161718192021222324type student struct &#123; Name string Age int8&#125;r.LoadHTMLGlob(&quot;templates/*&quot;)// r.LoadHTMLGlob(&quot;templates/**/*&quot;) // 解析目录下的目录 // 设置template使用的方法r.SetFuncMap(template.FuncMap&#123; &quot;isStart&quot;: isStart,&#125;)func isStart(index int) bool &#123; return index == 0&#125;stu1 := &amp;student&#123;Name: &quot;Geektutu&quot;, Age: 20&#125;stu2 := &amp;student&#123;Name: &quot;Jack&quot;, Age: 22&#125;r.GET(&quot;/arr&quot;, func(c *gin.Context) &#123; c.HTML(http.StatusOK, &quot;arr.tmpl&quot;, gin.H&#123; &quot;title&quot;: &quot;Gin&quot;, &quot;stuArr&quot;: [2]*student&#123;stu1, stu2&#125;, &#125;)&#125;) 123456789&lt;!-- templates/arr.tmpl --&gt;&lt;html&gt;&lt;body&gt; &lt;p&gt;hello, \\&#123;\\&#123;.title\\&#125;\\&#125;&lt;/p&gt; \\&#123;\\&#123;range $index, $ele := .stuArr \\&#125;\\&#125; &lt;p&gt;\\&#123;\\&#123; $index \\&#125;\\&#125;: \\&#123;\\&#123; $ele.Name \\&#125;\\&#125; is \\&#123;\\&#123; $ele.Age \\&#125;\\&#125; years old&lt;/p&gt; \\&#123;\\&#123; end \\&#125;\\&#125;&lt;/body&gt;&lt;/html&gt; Gin默认支持使用模板Go语言标准的模板text/template和html/template 静态文件代理123456789func main() &#123; router := gin.Default() router.Static(&quot;/assets&quot;, &quot;./assets&quot;) router.StaticFS(&quot;/more_static&quot;, http.Dir(&quot;my_file_system&quot;)) router.StaticFile(&quot;/favicon.ico&quot;, &quot;./resources/favicon.ico&quot;) // Listen and serve on 0.0.0.0:8080 router.Run(&quot;:8080&quot;)&#125; 123456789101112func main() &#123; router := gin.Default() router.GET(&quot;/local/file&quot;, func(c *gin.Context) &#123; c.File(&quot;local/file.go&quot;) &#125;) var fs http.FileSystem = // ... router.GET(&quot;/fs/file&quot;, func(c *gin.Context) &#123; c.FileFromFS(&quot;fs/file.go&quot;, fs) &#125;)&#125; 下载文件123456789101112131415161718192021func main() &#123; router := gin.Default() router.GET(&quot;/dataFromReader&quot;, func(c *gin.Context) &#123; response, err := http.Get(&quot;https://raw.githubusercontent.com/gin-gonic/logo/master/color.png&quot;) if err != nil || response.StatusCode != http.StatusOK &#123; c.Status(http.StatusServiceUnavailable) return &#125; reader := response.Body contentLength := response.ContentLength contentType := response.Header.Get(&quot;Content-Type&quot;) extraHeaders := map[string]string&#123; &quot;Content-Disposition&quot;: `attachment; filename=&quot;gopher.png&quot;`, &#125; c.DataFromReader(http.StatusOK, contentLength, contentType, reader, extraHeaders) &#125;) router.Run(&quot;:8088&quot;)&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang logrus 日志组件","slug":"language/golang/libs/logrus","date":"2022-06-03T06:20:00.000Z","updated":"2023-12-25T05:56:09.172Z","comments":true,"path":"2022/06/03/language/golang/libs/logrus/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/libs/logrus/","excerpt":"","text":"Logrus 日志1go get github.com/sirupsen/logrus 1234567891011import ( log &quot;github.com/sirupsen/logrus&quot;)func main()&#123; log.SetFormatter(&amp;log.JSONFormatter&#123; TimestampFormat:&quot;2006-01-02 15:04:05&quot;, &#125;) // 日志的输出级别 log.SetLevel(log.InfoLevel)&#125; 1. 实现日志分割 Linux logrotate file-rotatelogs 1go get github.com/lestrrat-go/file-rotatelogs 123456789101112131415161718import ( rotatelogs &quot;github.com/lestrrat-go/file-rotatelogs&quot; &quot;github.com/sirupsen/logrus&quot;)func main() &#123; content, err := rotatelogs.New( &quot;/var/log/cli.log.&quot;+&quot;-%Y%m%d%H%M&quot;, rotatelogs.WithLinkName(&quot;/var/log/cli.log&quot;), // 生成软链，指向最新日志文件 //MaxAge and RotationCount cannot be both set 两者不能同时设置 rotatelogs.WithMaxAge(6*time.Minute), //clear 最小分钟为单位 //rotatelogs.WithRotationCount(5), //number 默认7份 大于7份 或到了清理时间 开始清理 rotatelogs.WithRotationTime(time.Minute), //rotate 最小为1分钟轮询。默认60s 低于1分钟就按1分钟来 rotatelogs.ForceNewFile(), ) logrus.SetOutput(content)&#125; 强行生成新的日志文件 12345678rl := rotatelogs.New(...)signal.Notify(ch, syscall.SIGHUP)go func(ch chan os.Signal) &#123; &lt;-ch rl.R&#125; 2. 自定义Logger12345678910111213141516171819202122232425package mainimport ( &quot;github.com/sirupsen/logrus&quot; &quot;os&quot;)// logrus提供了New()函数来创建一个logrus的实例.// 项目中,可以创建任意数量的logrus实例.var log = logrus.New()func main() &#123; // 为当前logrus实例设置消息的输出,同样地, // 可以设置logrus实例的输出到任意io.writer log.Out = os.Stdout // 为当前logrus实例设置消息输出格式为json格式. // 同样地,也可以单独为某个logrus实例设置日志级别和hook,这里不详细叙述. log.Formatter = &amp;logrus.JSONFormatter&#123;&#125; log.WithFields(logrus.Fields&#123; &quot;animal&quot;: &quot;walrus&quot;, &quot;size&quot;: 10, &#125;).Info(&quot;A group of walrus emerges from the ocean&quot;)&#125; 3. hook接口方法123456// logrus在记录Levels()返回的日志级别的消息时会触发HOOK,// 按照Fire方法定义的内容修改logrus.Entry.type Hook interface &#123; Levels() []Level Fire(*Entry) error&#125; 一个简单自定义hook如下,DefaultFieldHook定义会在所有级别的日志消息中加入默认字段appName&#x3D;”myAppName”. 1234567891011type DefaultFieldHook struct &#123;&#125;func (hook *DefaultFieldHook) Fire(entry *log.Entry) error &#123; entry.Data[&quot;appName&quot;] = &quot;MyAppName&quot; return nil&#125;func (hook *DefaultFieldHook) Levels() []log.Level &#123; return log.AllLevels&#125; hook的使用也很简单,在初始化前调用log.AddHook(hook)添加相应的hook即可. logrus官方仅仅内置了syslog的hook. 3.1 Email Hook1234567891011121314151617181920212223func Email()&#123; logger:= logrus.New() //parameter&quot;APPLICATION_NAME&quot;, &quot;HOST&quot;, PORT, &quot;FROM&quot;, &quot;TO&quot; //首先开启smtp服务,最后两个参数是smtp的用户名和密码 hook, err := logrus_mail.NewMailAuthHook(&quot;testapp&quot;, &quot;smtp.163.com&quot;,25,&quot;username@163.com&quot;,&quot;username@163.com&quot;,&quot;smtp_name&quot;,&quot;smtp_password&quot;) if err == nil &#123; logger.Hooks.Add(hook) &#125; //生成*Entry var filename=&quot;123.txt&quot; contextLogger :=logger.WithFields(logrus.Fields&#123; &quot;file&quot;:filename, &quot;content&quot;: &quot;GG&quot;, &#125;) //设置时间戳和message contextLogger.Time=time.Now() contextLogger.Message=&quot;这是一个hook发来的邮件&quot; //只能发送Error,Fatal,Panic级别的log contextLogger.Level=logrus.FatalLevel //使用Fire发送,包含时间戳,message hook.Fire(contextLogger)&#125; 3.2 Logrus-Hook-Slack1go get github.com/johntdyer/slackrus 12345678910111213141516171819202122232425262728package mainimport ( logrus &quot;github.com/sirupsen/logrus&quot; &quot;github.com/johntdyer/slackrus&quot; &quot;os&quot;)func main() &#123; logrus.SetFormatter(&amp;logrus.JSONFormatter&#123;&#125;) logrus.SetOutput(os.Stderr) logrus.SetLevel(logrus.DebugLevel) logrus.AddHook(&amp;slackrus.SlackrusHook&#123; HookURL: &quot;https://hooks.slack.com/services/abc123/defghijklmnopqrstuvwxyz&quot;, AcceptedLevels: slackrus.LevelThreshold(logrus.DebugLevel), Channel: &quot;#slack-testing&quot;, IconEmoji: &quot;:ghost:&quot;, Username: &quot;foobot&quot;, &#125;) logrus.Warn(&quot;warn&quot;) logrus.Info(&quot;info&quot;) logrus.Debug(&quot;debug&quot;)&#125; 3.3 Logrus-Hook 日志分隔12345678910111213141516171819202122232425262728293031323334353637383940414243444546import ( &quot;github.com/lestrrat-go/file-rotatelogs&quot; &quot;github.com/rifflock/lfshook&quot; log &quot;github.com/sirupsen/logrus&quot; &quot;time&quot;)func newLfsHook(logLevel *string, maxRemainCnt uint) log.Hook &#123; writer, err := rotatelogs.New( logName+&quot;.%Y%m%d%H&quot;, // WithLinkName为最新的日志建立软连接,以方便随着找到当前日志文件 rotatelogs.WithLinkName(logName), // WithRotationTime设置日志分割的时间,这里设置为一小时分割一次 rotatelogs.WithRotationTime(time.Hour), // WithMaxAge和WithRotationCount二者只能设置一个, // WithMaxAge设置文件清理前的最长保存时间, // WithRotationCount设置文件清理前最多保存的个数. //rotatelogs.WithMaxAge(time.Hour*24), rotatelogs.WithRotationCount(maxRemainCnt), ) if err != nil &#123; log.Errorf(&quot;config local file system for logger error: %v&quot;, err) &#125; level, ok := logLevels[*logLevel] if ok &#123; log.SetLevel(level) &#125; else &#123; log.SetLevel(log.WarnLevel) &#125; lfsHook := lfshook.NewHook(lfshook.WriterMap&#123; log.DebugLevel: writer, log.InfoLevel: writer, log.WarnLevel: writer, log.ErrorLevel: writer, log.FatalLevel: writer, log.PanicLevel: writer, &#125;, &amp;log.TextFormatter&#123;DisableColors: true&#125;) return lfsHook&#125; 3.4 Logrus-Dingding-Hook 阿里钉钉群机器人123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108package utilsimport ( &quot;bytes&quot; &quot;encoding/json&quot; &quot;fmt&quot; &quot;log&quot; &quot;net/http&quot; &quot;github.com/sirupsen/logrus&quot;)var allLvls = []logrus.Level&#123; logrus.DebugLevel, logrus.InfoLevel, logrus.WarnLevel, logrus.ErrorLevel, logrus.FatalLevel, logrus.PanicLevel,&#125;func NewDingHook(url, app string, thresholdLevel logrus.Level) *dingHook &#123; temp := []logrus.Level&#123;&#125; for _, v := range allLvls &#123; if v &lt;= thresholdLevel &#123; temp = append(temp, v) &#125; &#125; hook := &amp;dingHook&#123;apiUrl: url, levels: temp, appName: app&#125; hook.jsonBodies = make(chan []byte) hook.closeChan = make(chan bool) //开启chan 队列 执行post dingding hook api go hook.startDingHookQueueJob() return hook&#125;func (dh *dingHook) startDingHookQueueJob() &#123; for &#123; select &#123; case &lt;-dh.closeChan: return case bs := &lt;-dh.jsonBodies: res, err := http.Post(dh.apiUrl, &quot;application/json&quot;, bytes.NewBuffer(bs)) if err != nil &#123; log.Println(err) &#125; if res != nil &amp;&amp; res.StatusCode != 200 &#123; log.Println(&quot;dingHook go chan http post error&quot;, res.StatusCode) &#125; &#125; &#125;&#125;type dingHook struct &#123; apiUrl string levels []logrus.Level appName string jsonBodies chan []byte closeChan chan bool&#125;// Levels sets which levels to sent to slackfunc (dh *dingHook) Levels() []logrus.Level &#123; return dh.levels&#125;//Fire2 这个异步有可能导致 最后一条消息丢失,main goroutine 提前结束到导致 子线程http post 没有发送func (dh *dingHook) Fire2(e *logrus.Entry) error &#123; msg, err := e.String() if err != nil &#123; return err &#125; dm := dingMsg&#123;Msgtype: &quot;text&quot;&#125; dm.Text.Content = fmt.Sprintf(&quot;%s \\n %s&quot;, dh.appName, msg) bs, err := json.Marshal(dm) if err != nil &#123; return err &#125; dh.jsonBodies &lt;- bs return nil&#125;func (dh *dingHook) Fire(e *logrus.Entry) error &#123; msg, err := e.String() if err != nil &#123; return err &#125; dm := dingMsg&#123;Msgtype: &quot;text&quot;&#125; dm.Text.Content = fmt.Sprintf(&quot;%s \\n %s&quot;, dh.appName, msg) bs, err := json.Marshal(dm) if err != nil &#123; return err &#125; res, err := http.Post(dh.apiUrl, &quot;application/json&quot;, bytes.NewBuffer(bs)) if err != nil &#123; return err &#125; if res != nil &amp;&amp; res.StatusCode != 200 &#123; return fmt.Errorf(&quot;dingHook go chan http post error %d&quot;, res.StatusCode) &#125; return nil&#125;type dingMsg struct &#123; Msgtype string `json:&quot;msgtype&quot;` Text struct &#123; Content string `json:&quot;content&quot;` &#125; `json:&quot;text&quot;`&#125; 使用方法 1234567891011func initSlackLogrus() &#123; lvl := logrus.InfoLevel //钉钉群机器人API地址 apiUrl := viper.GetString(&quot;logrus.dingHookUrl&quot;) dingHook := utils.NewDingHook(apiUrl, &quot;Felix&quot;, lvl) logrus.SetLevel(lvl) logrus.SetFormatter(&amp;logrus.JSONFormatter&#123;TimestampFormat: &quot;06-01-02T15:04:05&quot;&#125;) logrus.SetReportCaller(true) logrus.AddHook(dingHook)&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang viper 读取配置文件","slug":"language/golang/libs/viper","date":"2022-06-03T06:15:00.000Z","updated":"2023-12-25T05:56:09.173Z","comments":true,"path":"2022/06/03/language/golang/libs/viper/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/golang/libs/viper/","excerpt":"","text":"Viper配置文件读取Go使用Viper加载配置文件Github 安装 1go get github.com/spf13/viper 1. 初次使用1234567891011func main()&#123; var config Config viper.SetConfigName(&quot;config&quot;) // 设置配置文件名 (不带后缀) viper.AddConfigPath(&quot;.&quot;) // 第一个搜索路径 err := viper.ReadInConfig() // 读取配置数据 if err != nil &#123; panic(fmt.Errorf(&quot;Fatal error config file: %s \\n&quot;, err)) &#125; viper.Unmarshal(&amp;config) // 将配置信息绑定到结构体上 fmt.Println(config)&#125; 2. 读取配置文件12345678910viper.SetConfigFile(&quot;./config.yaml&quot;) // 指定配置文件路径viper.SetConfigName(&quot;config&quot;) // 配置文件名称(无扩展名)viper.SetConfigType(&quot;yaml&quot;) // 如果配置文件的名称中没有扩展名，则需要配置此项viper.AddConfigPath(&quot;/etc/appname/&quot;) // 查找配置文件所在的路径viper.AddConfigPath(&quot;$HOME/.appname&quot;) // 多次调用以添加多个搜索路径viper.AddConfigPath(&quot;.&quot;) // 还可以在工作目录中查找配置err := viper.ReadInConfig() // 查找并读取配置文件if err != nil &#123; // 处理读取配置文件的错误 panic(fmt.Errorf(&quot;Fatal error config file: %s \\n&quot;, err))&#125; 加载配置文件出错，处理特定错误 123456789if err := viper.ReadInConfig(); err != nil &#123; if _, ok := err.(viper.ConfigFileNotFoundError); ok &#123; // 配置文件未找到错误；如果需要可以忽略 &#125; else &#123; // 配置文件被找到，但产生了另外的错误 &#125;&#125;// 配置文件找到并成功解析 监控并重新读取配置文件Viper支持在运行时实时读取配置文件的功能。 只需告诉viper实例watchConfig。可选地，你可以为Viper提供一个回调函数，以便在每次发生更改时运行。 确保在调用WatchConfig()之前添加了所有的配置路径。 12345viper.WatchConfig()viper.OnConfigChange(func(e fsnotify.Event) &#123; // 配置文件发生变更之后会调用的回调函数 fmt.Println(&quot;Config file changed:&quot;, e.Name)&#125;) 从io.Reader读取配置123456789101112131415161718192021viper.SetConfigType(&quot;yaml&quot;) // 或者 viper.SetConfigType(&quot;YAML&quot;)// 任何需要将此配置添加到程序中的方法。var yamlExample = []byte(`Hacker: truename: stevehobbies:- skateboarding- snowboarding- goclothing: jacket: leather trousers: denimage: 35eyes : brownbeard: true`)viper.ReadConfig(bytes.NewBuffer(yamlExample))viper.Get(&quot;name&quot;) // 这里会得到 &quot;steve&quot; 覆盖配置12viper.Set(&quot;Verbose&quot;, true)viper.Set(&quot;LogFile&quot;, LogFile) 注册和使用别名1234567viper.RegisterAlias(&quot;loud&quot;, &quot;Verbose&quot;) // 注册别名（此处loud和Verbose建立了别名）viper.Set(&quot;verbose&quot;, true) // 结果与下一行相同viper.Set(&quot;loud&quot;, true) // 结果与前一行相同viper.GetBool(&quot;loud&quot;) // trueviper.GetBool(&quot;verbose&quot;) // true 使用环境变量Viper完全支持环境变量。这使Twelve-Factor App开箱即用。有五种方法可以帮助与ENV协作: AutomaticEnv() BindEnv(string...) : error SetEnvPrefix(string) SetEnvKeyReplacer(string...) *strings.Replacer AllowEmptyEnv(bool) 使用ENV变量时，务必要意识到Viper将ENV变量视为区分大小写。 Viper提供了一种机制来确保ENV变量是惟一的。通过使用SetEnvPrefix，你可以告诉Viper在读取环境变量时使用前缀。BindEnv和AutomaticEnv都将使用这个前缀。 BindEnv使用一个或两个参数。第一个参数是键名称，第二个是环境变量的名称。环境变量的名称区分大小写。如果没有提供ENV变量名，那么Viper将自动假设ENV变量与以下格式匹配：前缀+ “_” +键名全部大写。当你显式提供ENV变量名（第二个参数）时，它 不会 自动添加前缀。例如，如果第二个参数是“id”，Viper将查找环境变量“ID”。 在使用ENV变量时，需要注意的一件重要事情是，每次访问该值时都将读取它。Viper在调用BindEnv时不固定该值。 AutomaticEnv是一个强大的助手，尤其是与SetEnvPrefix结合使用时。调用时，Viper会在发出viper.Get请求时随时检查环境变量。它将应用以下规则。它将检查环境变量的名称是否与键匹配（如果设置了EnvPrefix）。 SetEnvKeyReplacer允许你使用strings.Replacer对象在一定程度上重写 Env 键。如果你希望在Get()调用中使用-或者其他什么符号，但是环境变量里使用_分隔符，那么这个功能是非常有用的。可以在viper_test.go中找到它的使用示例。 或者，你可以使用带有NewWithOptions工厂函数的EnvKeyReplacer。与SetEnvKeyReplacer不同，它接受StringReplacer接口，允许你编写自定义字符串替换逻辑。 默认情况下，空环境变量被认为是未设置的，并将返回到下一个配置源。若要将空环境变量视为已设置，请使用AllowEmptyEnv方法。 Env 示例：123456SetEnvPrefix(&quot;spf&quot;) // 将自动转为大写BindEnv(&quot;id&quot;)os.Setenv(&quot;SPF_ID&quot;, &quot;13&quot;) // 通常是在应用程序之外完成的id := Get(&quot;id&quot;) // 13 3. 写入配置文件从配置文件中读取配置文件是有用的，但是有时你想要存储在运行时所做的所有修改。为此，可以使用下面一组命令，每个命令都有自己的用途: WriteConfig - 将当前的viper配置写入预定义的路径并覆盖（如果存在的话）。如果没有预定义的路径，则报错。 SafeWriteConfig - 将当前的viper配置写入预定义的路径。如果没有预定义的路径，则报错。如果存在，将不会覆盖当前的配置文件。 WriteConfigAs - 将当前的viper配置写入给定的文件路径。将覆盖给定的文件(如果它存在的话)。 SafeWriteConfigAs - 将当前的viper配置写入给定的文件路径。不会覆盖给定的文件(如果它存在的话)。 根据经验，标记为safe的所有方法都不会覆盖任何文件，而是直接创建（如果不存在），而默认行为是创建或截断。 12345viper.WriteConfig() // 将当前配置写入“viper.AddConfigPath()”和“viper.SetConfigName”设置的预定义路径viper.SafeWriteConfig()viper.WriteConfigAs(&quot;/path/to/my/.config&quot;)viper.SafeWriteConfigAs(&quot;/path/to/my/.config&quot;) // 因为该配置文件写入过，所以会报错viper.SafeWriteConfigAs(&quot;/path/to/my/.other_config&quot;) 4. 读取远程配置在Viper中启用远程支持，需要在代码中匿名导入viper/remote这个包。 1import _ &quot;github.com/spf13/viper/remote&quot; Viper将读取从Key&#x2F;Value存储（例如etcd或Consul）中的路径检索到的配置字符串（如JSON、TOML、YAML、HCL、envfile和Java properties格式）。这些值的优先级高于默认值，但是会被从磁盘、flag或环境变量检索到的配置值覆盖。（译注：也就是说Viper加载配置值的优先级为：磁盘上的配置文件&gt;命令行标志位&gt;环境变量&gt;远程Key&#x2F;Value存储&gt;默认值。） Viper使用crypt从K&#x2F;V存储中检索配置，这意味着如果你有正确的gpg密匙，你可以将配置值加密存储并自动解密。加密是可选的。 你可以将远程配置与本地配置结合使用，也可以独立使用。 crypt有一个命令行助手，你可以使用它将配置放入K&#x2F;V存储中。crypt默认使用在http://127.0.0.1:4001的etcd。 12$ go get github.com/bketelsen/crypt/bin/crypt$ crypt set -plaintext /config/hugo.json /Users/hugo/settings/config.json 确认值已经设置： 1$ crypt get -plaintext /config/hugo.json 有关如何设置加密值或如何使用Consul的示例，请参见crypt文档。 etcd123viper.AddRemoteProvider(&quot;etcd&quot;, &quot;http://127.0.0.1:4001&quot;,&quot;/config/hugo.json&quot;)viper.SetConfigType(&quot;json&quot;) // 因为在字节流中没有文件扩展名，所以这里需要设置下类型。支持的扩展名有 &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;properties&quot;, &quot;props&quot;, &quot;prop&quot;, &quot;env&quot;, &quot;dotenv&quot;err := viper.ReadRemoteConfig() consul你需要 Consul Key&#x2F;Value存储中设置一个Key保存包含所需配置的JSON值。例如，创建一个keyMY_CONSUL_KEY将下面的值存入Consul key&#x2F;value 存储： 1234&#123; &quot;port&quot;: 8080, &quot;hostname&quot;: &quot;liwenzhou.com&quot;&#125; 123456viper.AddRemoteProvider(&quot;consul&quot;, &quot;localhost:8500&quot;, &quot;MY_CONSUL_KEY&quot;)viper.SetConfigType(&quot;json&quot;) // 需要显示设置成jsonerr := viper.ReadRemoteConfig()fmt.Println(viper.Get(&quot;port&quot;)) // 8080fmt.Println(viper.Get(&quot;hostname&quot;)) // liwenzhou.com Firestore123viper.AddRemoteProvider(&quot;firestore&quot;, &quot;google-cloud-project-id&quot;, &quot;collection/document&quot;)viper.SetConfigType(&quot;json&quot;) // 配置的格式: &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;err := viper.ReadRemoteConfig() 当然，你也可以使用SecureRemoteProvider 远程k&#x2F;v存储-加密123viper.AddSecureRemoteProvider(&quot;etcd&quot;,&quot;http://127.0.0.1:4001&quot;,&quot;/config/hugo.json&quot;,&quot;/etc/secrets/mykeyring.gpg&quot;)viper.SetConfigType(&quot;json&quot;) // 因为在字节流中没有文件扩展名，所以这里需要设置下类型。支持的扩展名有 &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;properties&quot;, &quot;props&quot;, &quot;prop&quot;, &quot;env&quot;, &quot;dotenv&quot;err := viper.ReadRemoteConfig() 监控etcd中的更改-未加密12345678910111213141516171819202122232425262728// 或者你可以创建一个新的viper实例var runtime_viper = viper.New()runtime_viper.AddRemoteProvider(&quot;etcd&quot;, &quot;http://127.0.0.1:4001&quot;, &quot;/config/hugo.yml&quot;)runtime_viper.SetConfigType(&quot;yaml&quot;) // 因为在字节流中没有文件扩展名，所以这里需要设置下类型。支持的扩展名有 &quot;json&quot;, &quot;toml&quot;, &quot;yaml&quot;, &quot;yml&quot;, &quot;properties&quot;, &quot;props&quot;, &quot;prop&quot;, &quot;env&quot;, &quot;dotenv&quot;// 第一次从远程读取配置err := runtime_viper.ReadRemoteConfig()// 反序列化runtime_viper.Unmarshal(&amp;runtime_conf)// 开启一个单独的goroutine一直监控远端的变更go func()&#123; for &#123; time.Sleep(time.Second * 5) // 每次请求后延迟一下 // 目前只测试了etcd支持 err := runtime_viper.WatchRemoteConfig() if err != nil &#123; log.Errorf(&quot;unable to read remote config: %v&quot;, err) continue &#125; // 将新配置反序列化到我们运行时的配置结构体中。你还可以借助channel实现一个通知系统更改的信号 runtime_viper.Unmarshal(&amp;runtime_conf) &#125;&#125;() 5. 从Viper中获取值在Viper中，有几种方法可以根据值的类型获取值。存在以下功能和方法: Get(key string) : interface&#123;&#125; GetBool(key string) : bool GetFloat64(key string) : float64 GetInt(key string) : int GetIntSlice(key string) : []int GetString(key string) : string GetStringMap(key string) : map[string]interface&#123;&#125; GetStringMapString(key string) : map[string]string GetStringSlice(key string) : []string GetTime(key string) : time.Time GetDuration(key string) : time.Duration IsSet(key string) : bool AllSettings() : map[string]interface&#123;&#125; 需要认识到的一件重要事情是，每一个Get方法在找不到值的时候都会返回零值。为了检查给定的键是否存在，提供了IsSet()方法。 1234viper.GetString(&quot;logfile&quot;) // 不区分大小写的设置和获取if viper.GetBool(&quot;verbose&quot;) &#123; fmt.Println(&quot;verbose enabled&quot;)&#125; 访问嵌套的键访问器方法也接受深度嵌套键的格式化路径。例如，如果加载下面的JSON文件： 12345678910111213141516&#123; &quot;host&quot;: &#123; &quot;address&quot;: &quot;localhost&quot;, &quot;port&quot;: 5799 &#125;, &quot;datastore&quot;: &#123; &quot;metric&quot;: &#123; &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 3099 &#125;, &quot;warehouse&quot;: &#123; &quot;host&quot;: &quot;198.0.0.1&quot;, &quot;port&quot;: 2112 &#125; &#125;&#125; Viper可以通过传入.分隔的路径来访问嵌套字段： 1GetString(&quot;datastore.metric.host&quot;) // (返回 &quot;127.0.0.1&quot;) 提取子树从Viper中提取子树。 例如，viper实例现在代表了以下配置： 1234567app: cache1: max-items: 100 item-size: 64 cache2: max-items: 200 item-size: 80 执行后： 1subv := viper.Sub(&quot;app.cache1&quot;) subv现在就代表： 12max-items: 100item-size: 64 假设我们现在有这么一个函数： 1func NewCache(cfg *Viper) *Cache &#123;...&#125; 它基于subv格式的配置信息创建缓存。现在，可以轻松地分别创建这两个缓存，如下所示： 12345cfg1 := viper.Sub(&quot;app.cache1&quot;)cache1 := NewCache(cfg1)cfg2 := viper.Sub(&quot;app.cache2&quot;)cache2 := NewCache(cfg2)","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"lxml 简单使用","slug":"spider/lxml","date":"2022-06-03T06:10:00.000Z","updated":"2023-12-25T05:56:09.178Z","comments":true,"path":"2022/06/03/spider/lxml/","link":"","permalink":"https://blog.ifan.host/2022/06/03/spider/lxml/","excerpt":"","text":"语法 表达式 描述 nodename 选取此节点的所有子节点 / 从根节点选取 // 选择所有符合条件的节点，不管位置 . 选择当前节点 .. 当前节点的父节点 @ 选取属性 谓语 路径表达式 结果 /bookstore/book[1] 选取属于 bookstore 子元素的第一个 book 元素。 /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素。 /bookstore/book[last()-1] 选取属于 bookstore 子元素的倒数第二个 book 元素。 /bookstore/book[position()&lt;3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。 //title[@lang] 选取所有拥有名为 lang 的属性的 title 元素。 //title[@lang=&#39;eng&#39;] 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。 /bookstore/book[price&gt;35.00] 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00 。 /bookstore/book[price&gt;35.00]/title 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00 。 选取若干路径 路径表达式 结果 //book/title | //book/price 选取 book 元素的所有 title 和 price 元素。 //title | //price 选取文档中的所有 title 和 price 元素。 /bookstore/book/title | //price 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 轴 轴名称 结果 ancestor 选取当前节点的所有先辈（父、祖父等）。 ancestor-or-self 选取当前节点的所有先辈（父、祖父等）以及当前节点本身。 attribute 选取当前节点的所有属性。 child 选取当前节点的所有子元素。 descendant 选取当前节点的所有后代元素（子、孙等）。 descendant-or-self 选取当前节点的所有后代元素（子、孙等）以及当前节点本身。 following 选取文档中当前节点的结束标签之后的所有节点。 following-sibling 选取当前节点之后的所有兄弟节点 namespace 选取当前节点的所有命名空间节点。 parent 选取当前节点的父节点。 preceding 选取文档中当前节点的开始标签之前的所有节点。 preceding-sibling 选取当前节点之前的所有同级节点。 self 选取当前节点。 运算符 运算符 描述 实例 返回值 &#96; &#96; 计算两个节点集 //book + 加法 6 + 4 10 - 减法 6 - 4 2 * 乘法 6 * 4 24 div 除法 8 div 4 2 = 等于 price=9.80 如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回 false。 != 不等于 price!=9.80 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 &lt; 小于 price&lt;9.80 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 &lt;= 小于或等于 price&lt;=9.80 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 &gt; 大于 price&gt;9.80 如果 price 是 9.90，则返回 true。 如果 price 是 9.80，则返回 false。 &gt;= 大于或等于 price&gt;=9.80 如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。 or 或 price=9.80 or price=9.70 如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。 and 与 price&gt;9.00 and price&lt;9.90 如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。 not 取反 not(true) false mod 计算除法的余数 5 mod 2 1 函数 函数名 描述 count() 计数 concat(str,str,...) 字符串拼接 string-length(str) 返回 str 的长度 local-name(node) 解析 node 节点的名称 string(str) 返回 str 字符串 substring(string,start,len) \\n fn:substring(string,start) 返回从 start 位置开始的指定长度的子字符串。第一个字符的下标是 1。如果省略 len 参数，则返回从位置 start 到字符串末尾的子字符串。 translate(string1,string2,string3) 把 string1 中的 string2 替换为 string3。 contains(string1,string2) 如果 string1 包含 string2，则返回 true，否则返回 false。 starts-with(string1,string2) 如果 string1 以 string2 开始，则返回 true，否则返回 false substring-before(string1,string2) 返回 string2 在 string1 中出现之前的子字符串。 substring-after(string1,string2) 返回 string2 在 string1 中出现之后的子字符串。 lxml中没有ends-with，可以使用下面的方法 1substring(@class, string-length(@class) - strting-length(&#x27;-wrapper&#x27;) +1 ) = &#x27;-wrapper&#x27; 将 Element 对象转为 str 123from lxml import etreehtml = etree.HTML(txt)txt = etree.tostring(html)","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/"},{"name":"lxml","slug":"爬虫/lxml","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/lxml/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"lxml","slug":"lxml","permalink":"https://blog.ifan.host/tags/lxml/"}]},{"title":"xpath 简单使用","slug":"spider/xpath","date":"2022-06-03T06:05:00.000Z","updated":"2023-12-25T05:56:09.180Z","comments":true,"path":"2022/06/03/spider/xpath/","link":"","permalink":"https://blog.ifan.host/2022/06/03/spider/xpath/","excerpt":"","text":"xpath语法1、匹配某节点下的所有.////获取文档中所有匹配的节点，.获取当前节点，有的时候我们需要获取当前节点下的所有节点，.//一定要结合.使用//，否则都会获取整个文档的匹配结果. 2、匹配包含某属性的所有的属性值//@lang12print tree.xpath(&#x27;//@code&#x27;) #匹配所有带有code属性的属性值&gt;&gt;[&#x27;84&#x27;, &#x27;104&#x27;, &#x27;223&#x27;] 3、选取若干路径这个符号用于在一个xpath中写多个表达式用，用|分开，每个表达式互不干扰 12print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/h2/text() | //li[@data]/text()&#x27;) #多个匹配条件&gt;&gt;[u&#x27;\\u8fd9\\u91cc\\u662f\\u4e2a\\u5c0f\\u6807\\u9898&#x27;, &#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;] 4、 Axes（轴） child：选取当前节点的所有子元素 123456789&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/child::ul/li/text()&#x27;) #child子节点定位&gt;&gt;[&#x27;84&#x27;, &#x27;104&#x27;, &#x27;223&#x27;]&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/child::*&#x27;) #child::*当前节点的所有子元素&gt;&gt;[&lt;Element h2 at 0x21bd148&gt;, &lt;Element ol at 0x21bd108&gt;, &lt;Element ul at 0x21bd0c8&gt;]&gt;&gt;#定位某节点下为ol的子节点下的所有节点&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/child::ol/child::*/text()&#x27;) &gt;&gt;[&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;] attribute：选取当前节点的所有属性 12345&gt;&gt;print tree.xpath(&#x27;//div/attribute::id&#x27;) #attribute定位id属性值&gt;&gt;[&#x27;testid&#x27;, &#x27;go&#x27;]&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/attribute::*&#x27;) #定位当前节点的所有属性&gt;&gt;[&#x27;testid&#x27;, &#x27;first&#x27;] ancestor：父辈元素 &#x2F; ancestor-or-self：父辈元素及当前元素 1234567&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/ancestor::div/@price&#x27;) #定位父辈div元素的price属性&gt;&gt;[&#x27;99.8&#x27;]&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/ancestor::div&#x27;) #所有父辈div元素&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/ancestor-or-self::div&#x27;) #所有父辈及当前节点div元素&gt;&gt;[&lt;Element div at 0x23fc108&gt;]&gt;&gt;[&lt;Element div at 0x23fc108&gt;, &lt;Element div at 0x23fc0c8&gt;] descendant：后代 &#x2F; descendant-or-self：后代及当前节点本身 使用方法同上 following :选取文档中当前节点的结束标签之后的所有节点 123#定位testid之后不包含id属性的div标签下所有的li中第一个li的text属性&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/following::div[not(@id)]/.//li[1]/text()&#x27;) &gt;&gt;[&#x27;test1&#x27;] namespace：选取当前节点的所有命名空间节点 12&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/namespace::*&#x27;) #选取命名空间节点&gt;&gt;[(&#x27;xml&#x27;, &#x27;http://www.w3.org/XML/1998/namespace&#x27;)] parent：选取当前节点的父节点 1234&gt;&gt;#选取data值为one的父节点的子节点中最后一个节点的值&gt;&gt;print tree.xpath(&#x27;//li[@data=&quot;one&quot;]/parent::ol/li[last()]/text()&#x27;) &gt;&gt;[&#x27;3&#x27;]&gt;&gt;#注意这里的用法，parent::父节点的名字 preceding：选取文档中当前节点的开始标签之前的所有节点 12345678&gt;&gt;#记住是标签开始之前，同级前节点及其子节点&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/preceding::div/ul/li[1]/text()&#x27;)[0] &gt;&gt;时间&gt;&gt;#下面这两条可以看到其顺序是靠近testid节点的优先&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/preceding::li[1]/text()&#x27;)[0]&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/preceding::li[3]/text()&#x27;)[0]&gt;&gt;任务&gt;&gt;时间 preceding-sibling：选取当前节点之前的所有同级节点 12345&gt;&gt;#记住只能是同级节点&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/preceding-sibling::div/ul/li[2]/text()&#x27;)[0]&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/preceding-sibling::li&#x27;) #这里返回的就是空的了&gt;&gt;地点&gt;&gt;[] self：选取当前节点 123&gt;&gt;#选取带id属性值的div中包含data-h属性的标签的所有属性值&gt;&gt;print tree.xpath(&#x27;//div[@id]/self::div[@data-h]/attribute::*&#x27;) &gt;&gt;[&#x27;testid&#x27;, &#x27;first&#x27;] 组合拳 123#定位id值为testid下的ol下的li属性值data为two的父元素ol的兄弟前节点h2的text值&gt;&gt;print tree.xpath(&#x27;//*[@id=&quot;testid&quot;]/ol/li[@data=&quot;two&quot;]/parent::ol/preceding-sibling::h2/text()&#x27;)[0] &gt;&gt;这里是个小标题 5、position定位12&gt;&gt;print tree.xpath(&#x27;//*[@id=&quot;testid&quot;]/ol/li[position()=2]/text()&#x27;)[0] &gt;&gt;2 6、条件123&gt;&gt;定位所有h2标签中text值为`这里是个小标题`&gt;&gt;print tree.xpath(u&#x27;//h2[text()=&quot;这里是个小标题&quot;]/text()&#x27;)[0]&gt;&gt;这里是个小标题 7、函数 count：统计 12&gt;&gt;print tree.xpath(&#x27;count(//li[@data])&#x27;) #节点统计&gt;&gt;3.0 concat：字符串连接 12&gt;&gt;print tree.xpath(&#x27;concat(//li[@data=&quot;one&quot;]/text(),//li[@data=&quot;three&quot;]/text())&#x27;)&gt;&gt;13 string：解析当前节点下的字符 123&gt;&gt;#string只能解析匹配到的第一个节点下的值，也就是作用于list时只匹配第一个&gt;&gt;print tree.xpath(&#x27;string(//li)&#x27;) &gt;&gt;时间 local-name：解析节点名称 12&gt;&gt;print tree.xpath(&#x27;local-name(//*[@id=&quot;testid&quot;])&#x27;) #local-name解析节点名称&gt;&gt;div contains(string1,string2)：如果 string1 包含 string2，则返回 true，否则返回 false 1234567&gt;&gt;tree.xpath(&#x27;//h3[contains(text(),&quot;H3&quot;)]/a/text()&#x27;)[0] #使用字符内容来辅助定位&gt;&gt;百度一下&gt;&gt;一记组合拳&gt;&gt;#匹配带有href属性的a标签的先辈节点中的div，其兄弟节点中前一个div节点下ul下li中text属性包含“务”字的节点的值&gt;&gt;print tree.xpath(u&#x27;//a[@href]/ancestor::div/preceding::div/ul/li[contains(text(),&quot;务&quot;)]/text()&#x27;)[0] &gt;&gt;任务 注意：兄弟节点后一个节点可以使用：following-sibling not：布尔值（否） 12&gt;&gt;print tree.xpath(&#x27;count(//li[not(@data)])&#x27;) #不包含data属性的li标签统计&gt;&gt;18.0 string-length：返回指定字符串的长度 123&gt;&gt;#string-length函数+local-name函数定位节点名长度小于2的元素&gt;&gt;print tree.xpath(&#x27;//*[string-length(local-name())&lt;2]/text()&#x27;)[0] &gt;&gt;百度一下 组合拳2 123&gt;&gt;#contains函数+local-name函数定位节点名包含di的元素&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/following::div[contains(local-name(),&quot;di&quot;)]&#x27;) &gt;&gt;[&lt;Element div at 0x225e108&gt;, &lt;Element div at 0x225e0c8&gt;] or：多条件匹配 12345&gt;&gt;print tree.xpath(&#x27;//li[@data=&quot;one&quot; or @code=&quot;84&quot;]/text()&#x27;) #or匹配多个条件&gt;&gt;[&#x27;1&#x27;, &#x27;84&#x27;]&gt;&gt;#也可使用|&gt;&gt;print tree.xpath(&#x27;//li[@data=&quot;one&quot;]/text() | //li[@code=&quot;84&quot;]/text()&#x27;) #|匹配多个条件&gt;&gt;[&#x27;1&#x27;, &#x27;84&#x27;] 组合拳3：floor + div除法 + ceiling 123&gt;&gt;#position定位+last+div除法，选取中间两个&gt;&gt;tree.xpath(&#x27;//div[@id=&quot;go&quot;]/ul/li[position()=floor(last() div 2+0.5) or position()=ceiling(last() div 2+0.5)]/text()&#x27;) &gt;&gt;[&#x27;5&#x27;, &#x27;6&#x27;] 组合拳4隔行定位：position+mod取余 12&gt;&gt;#position+取余运算隔行定位&gt;&gt;tree.xpath(&#x27;//div[@id=&quot;go&quot;]/ul/li[position()=((position() mod 2)=0)]/text()&#x27;) starts-with：以。。开始 123&gt;&gt;#starts-with定位属性值以8开头的li元素&gt;&gt;print tree.xpath(&#x27;//li[starts-with(@code,&quot;8&quot;)]/text()&#x27;)[0]&gt;&gt;84 xpath 没有 ends-with方法 1substring(@class, string-length(@class) - strting-length(&#x27;-wrapper&#x27;) +1 ) = &#x27;-wrapper&#x27; 8、数值比较 &lt;：小于 123&gt;&gt;#所有li的code属性小于200的节点&gt;&gt;print tree.xpath(&#x27;//li[@code&lt;200]/text()&#x27;)&gt;&gt;[&#x27;84&#x27;, &#x27;104&#x27;] div：对某两个节点的属性值做除法 12&gt;&gt;print tree.xpath(&#x27;//div[@id=&quot;testid&quot;]/ul/li[3]/@code div //div[@id=&quot;testid&quot;]/ul/li[1]/@code&#x27;)&gt;&gt;2.65476190476 组合拳4：根据节点下的某一节点数量定位 123&gt;&gt;#选取所有ul下li节点数大于5的ul节点&gt;&gt;print tree.xpath(&#x27;//ul[count(li)&gt;5]/li/text()&#x27;)&gt;&gt;[&#x27;1&#x27;, &#x27;2&#x27;, &#x27;3&#x27;, &#x27;4&#x27;, &#x27;5&#x27;, &#x27;6&#x27;, &#x27;7&#x27;, &#x27;8&#x27;, &#x27;9&#x27;, &#x27;10&#x27;] 9、将对象还原为字符串123456&gt;&gt;&gt; s = tree.xpath(&#x27;//*[@id=&quot;testid&quot;]&#x27;)[0] #使用xpath定位一个节点&gt;&gt;&gt; s&lt;Element div at 0x2b6ffc8&gt;&gt;&gt;&gt; s2 = etree.tostring(s) #还原这个对象为html字符串&gt;&gt;&gt; s2&#x27;&lt;div id=&quot;testid&quot;&gt;\\n\\t\\t&lt;h2&gt;&amp;#213;&amp;#226;&amp;#192;&amp;#239;&amp;#202;&amp;#199;&amp;#184;&amp;#246;&amp;#208;&amp;#161;&amp;#177;&amp;#234;&amp;#204;&amp;#226;&lt;/h2&gt;\\n\\t\\t&lt;ol&gt;\\n\\t\\t\\t&lt;li data=&quot;one&quot;&gt;1&lt;/li&gt;\\n\\t\\t\\t&lt;li data=&quot;two&quot;&gt;2&lt;/li&gt;\\n\\t\\t\\t&lt;li data=&quot;three&quot;&gt;3&lt;/li&gt;\\n\\t\\t&lt;/ol&gt;\\n\\t\\t&lt;ul&gt;\\n\\t\\t\\t&lt;li code=&quot;84&quot;&gt;84&lt;/li&gt;\\n\\t\\t\\t&lt;li code=&quot;104&quot;&gt;104&lt;/li&gt;\\n\\t\\t\\t&lt;li code=&quot;223&quot;&gt;223&lt;/li&gt;\\n\\t\\t&lt;/ul&gt;\\n\\t&lt;/div&gt;\\n\\t&#x27; 10、选取一个属性中的多个值12345678举例：&lt;div class=&quot;mp-city-list-container mp-privince-city&quot; mp-role=&quot;provinceCityList&quot;&gt;选择这个div的方案网上有说用and的，但是似乎只能针对不同的属性的单个值本次使用contains&gt;&gt;.xpath(&#x27;div[contains(@class,&quot;mp-city-list-container mp-privince-city&quot;)]&#x27;)&gt;&gt;当然也可以直接选取其属性的第二个值&gt;&gt;.xpath(&#x27;div[contains(@class,&quot;mp-privince-city&quot;)]&#x27;)&gt;&gt;重点是class需要添加一个@符号本次验证否定了网上的and，使用了contains,验证环境在scrapy的response.xpath下","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/"},{"name":"xpath","slug":"爬虫/xpath","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/xpath/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"xpath","slug":"xpath","permalink":"https://blog.ifan.host/tags/xpath/"}]},{"title":"boto3 简单使用","slug":"language/python/libs/boto3","date":"2022-06-03T05:55:00.000Z","updated":"2023-12-25T05:56:09.177Z","comments":true,"path":"2022/06/03/language/python/libs/boto3/","link":"","permalink":"https://blog.ifan.host/2022/06/03/language/python/libs/boto3/","excerpt":"","text":"使用 boto3 下载文件 1pip install boto3 123456789101112131415161718192021222324252627from boto3.session import Sessionfrom botocore.config import Configimport os # 使用代理os.environ[&quot;HTTP_PROXY&quot;] = &quot;http://proxy:9999&quot;os.environ[&quot;HTTPS_PROXY&quot;] = &quot;https://proxy:9999&quot;# Client初始化access_key = &quot;username&quot;secret_key = &quot;password&quot;url = &quot;http://localhost:9000&quot;session = Session(access_key, secret_key)session = Session(access_key, secret_key)s3_client = session.client(&#x27;s3&#x27;, endpoint_url=url)# Client初始化结束# 列出该用户拥有的桶print([bucket[&#x27;Name&#x27;] for bucket in s3_client.list_buckets()[&#x27;Buckets&#x27;]])# 列举文件resp = s3_client.list_objects(Bucket=&quot;data&quot;, Delimiter=&#x27;/&#x27;, Prefix=&#x27;spider/&#x27;)for path in resp.get(&quot;CommonPrefixes&quot;): print(path.get(&quot;Prefix&quot;))# 下载文件resp = s3_client.get_object(Bucket=&quot;data&quot;, Key=&quot;2.pdf&quot;)with open(&quot;aa.pdf&quot;, &quot;wb&quot;) as f: f.write(resp[&quot;Body&quot;].read())","categories":[{"name":"boto3","slug":"boto3","permalink":"https://blog.ifan.host/categories/boto3/"},{"name":"文件存储","slug":"boto3/文件存储","permalink":"https://blog.ifan.host/categories/boto3/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"boto3","slug":"boto3","permalink":"https://blog.ifan.host/tags/boto3/"},{"name":"文件存储","slug":"文件存储","permalink":"https://blog.ifan.host/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"}]},{"title":"mitmproxy 简单使用","slug":"spider/minproxy","date":"2022-06-03T05:50:00.000Z","updated":"2023-12-25T05:56:09.179Z","comments":true,"path":"2022/06/03/spider/minproxy/","link":"","permalink":"https://blog.ifan.host/2022/06/03/spider/minproxy/","excerpt":"","text":"mitmproxy安装在 linux 中： 1sudo pip3 install mitmproxy 在 windows 中，以管理员身份运行 cmd 或 power shell： 1pip3 install mitmproxy 完成后，系统将拥有 mitmproxy、mitmdump、mitmweb 三个命令，由于 mitmproxy 命令不支持在 windows 系统中运行（这没关系，不用担心），我们可以拿 mitmdump 测试一下安装是否成功，执行： 1mitmdump --version 应当可以看到类似于这样的输出： 1234Mitmproxy: 4.0.1Python: 3.6.5OpenSSL: OpenSSL 1.1.0h 27 Mar 2018Platform: Windows-10-10.0.16299-SP0 组件mitmproxy 命令启动后，会提供一个命令行界面，用户可以实时看到发生的请求，并通过命令过滤请求，查看请求数据。 mitmweb 命令启动后，会提供一个 web 界面，用户可以实时看到发生的请求，并通过 GUI 交互来过滤请求，查看请求数据。 mitmdump 命令启动后，你应该猜到了，没有界面，程序默默运行，所以 mitmdump 无法提供过滤请求、查看数据的功能，只能结合自定义脚本，默默工作。 12# chrome 指定代理&quot;chrome&quot; --proxy-server=127.0.0.1:8080 --ignore-certificate-errors 脚本官方内置的一些 addon 123456789import mitmproxy.httpfrom mitmproxy import ctxnum = 0def request(flow: mitmproxy.http.HTTPFlow): global num num = num + 1 ctx.log.info(&quot;We&#x27;ve seen %d flows&quot; % num) 1234567891011121314import mitmproxy.httpfrom mitmproxy import ctxclass Counter: def __init__(self): self.num = 0 def request(self, flow: mitmproxy.http.HTTPFlow): self.num = self.num + 1 ctx.log.info(&quot;We&#x27;ve seen %d flows&quot; % self.num)addons = [ Counter()] 1mitmweb -s addons.py 各种生命周期Http1def http_connect(self, flow: mitmproxy.http.HTTPFlow): 收到了来自客户端的 HTTP CONNECT 请求。在 flow 上设置非 2xx 响应将返回该响应并断开连接。CONNECT 不是常用的 HTTP 请求方法，目的是与服务器建立代理连接，仅是 client 与 proxy 的之间的交流，所以 CONNECT 请求不会触发 request、response 等其他常规的 HTTP 事件。 1def requestheaders(self, flow: mitmproxy.http.HTTPFlow): 来自客户端的 HTTP 请求的头部被成功读取。此时 flow 中的 request 的 body 是空的。 1def request(self, flow: mitmproxy.http.HTTPFlow): 来自客户端的 HTTP 请求被成功完整读取。 1def responseheaders(self, flow: mitmproxy.http.HTTPFlow): 来自服务端的 HTTP 响应的头部被成功读取。此时 flow 中的 response 的 body 是空的。 1def response(self, flow: mitmproxy.http.HTTPFlow): 来自服务端端的 HTTP 响应被成功完整读取。 1def error(self, flow: mitmproxy.http.HTTPFlow): 发生了一个 HTTP 错误。比如无效的服务端响应、连接断开等。注意与“有效的 HTTP 错误返回”不是一回事，后者是一个正确的服务端响应，只是 HTTP code 表示错误而已。 TCP1def tcp_start(self, flow: mitmproxy.tcp.TCPFlow): 建立了一个 TCP 连接。 1def tcp_message(self, flow: mitmproxy.tcp.TCPFlow): TCP 连接收到了一条消息，最近一条消息存于 flow.messages[-1]。消息是可修改的。 1def tcp_error(self, flow: mitmproxy.tcp.TCPFlow): 发生了 TCP 错误。 1def tcp_end(self, flow: mitmproxy.tcp.TCPFlow): TCP 连接关闭。 Websocket 生命周期1def websocket_handshake(self, flow: mitmproxy.http.HTTPFlow): 客户端试图建立一个 websocket 连接。可以通过控制 HTTP 头部中针对 websocket 的条目来改变握手行为。flow 的 request 属性保证是非空的的。 1def websocket_start(self, flow: mitmproxy.websocket.WebSocketFlow): 建立了一个 websocket 连接。 1def websocket_message(self, flow: mitmproxy.websocket.WebSocketFlow): 收到一条来自客户端或服务端的 websocket 消息。最近一条消息存于 flow.messages[-1]。消息是可修改的。目前有两种消息类型，对应 BINARY 类型的 frame 或 TEXT 类型的 frame。 1def websocket_error(self, flow: mitmproxy.websocket.WebSocketFlow): 发生了 websocket 错误。 1def websocket_end(self, flow: mitmproxy.websocket.WebSocketFlow): websocket 连接关闭。 针对网络连接生命周期1def clientconnect(self, layer: mitmproxy.proxy.protocol.Layer): 客户端连接到了 mitmproxy。注意一条连接可能对应多个 HTTP 请求。 1def clientdisconnect(self, layer: mitmproxy.proxy.protocol.Layer): 客户端断开了和 mitmproxy 的连接。 1def serverconnect(self, conn: mitmproxy.connections.ServerConnection): mitmproxy 连接到了服务端。注意一条连接可能对应多个 HTTP 请求。 1def serverdisconnect(self, conn: mitmproxy.connections.ServerConnection): mitmproxy 断开了和服务端的连接。 1def next_layer(self, layer: mitmproxy.proxy.protocol.Layer): 网络 layer 发生切换。你可以通过返回一个新的 layer 对象来改变将被使用的 layer。 通用生命周期1def configure(self, updated: typing.Set[str]): 配置发生变化。updated 参数是一个类似集合的对象，包含了所有变化了的选项。在 mitmproxy 启动时，该事件也会触发，且 updated 包含所有选项。 1def done(self): addon 关闭或被移除，又或者 mitmproxy 本身关闭。由于会先等事件循环终止后再触发该事件，所以这是一个 addon 可以看见的最后一个事件。由于此时 log 也已经关闭，所以此时调用 log 函数没有任何输出。 1def load(self, entry: mitmproxy.addonmanager.Loader): addon 第一次加载时。entry 参数是一个 Loader 对象，包含有添加选项、命令的方法。这里是 addon 配置它自己的地方。 1def log(self, entry: mitmproxy.log.LogEntry): 通过 mitmproxy.ctx.log 产生了一条新日志。小心不要在这个事件内打日志，否则会造成死循环。 1def running(self): mitmproxy 完全启动并开始运行。此时，mitmproxy 已经绑定了端口，所有的 addon 都被加载了。 1def update(self, flows: typing.Sequence[mitmproxy.flow.Flow]): 一个或多个 flow 对象被修改了，通常是来自一个不同的 addon。 示例","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/"},{"name":"mitmproxy","slug":"爬虫/mitmproxy","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/mitmproxy/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"mitmproxy","slug":"mitmproxy","permalink":"https://blog.ifan.host/tags/mitmproxy/"}]},{"title":"puppeteer 简单使用","slug":"spider/puppeteer/puppeteer","date":"2022-06-03T05:24:00.000Z","updated":"2023-12-25T05:56:09.179Z","comments":true,"path":"2022/06/03/spider/puppeteer/puppeteer/","link":"","permalink":"https://blog.ifan.host/2022/06/03/spider/puppeteer/puppeteer/","excerpt":"","text":"Puppeteer下载安装 API中文网址 12npm i puppeteer# yarn add puppeteer 下载的时候还会下载 Chromium ,因为网速慢，总是出错，所以单独下载 Chromium 12npm i puppeteer-core# yarn add puppeteer-core Chromium 可以在这里下载 安装完了运行一遍脚本试试 123456789101112131415161718192021222324252627282930313233const puppeteer = require(&#x27;puppeteer-core&#x27;);(async () =&gt; &#123; const conf = &#123; headless: false, // 指定无头模式 executablePath: &#x27;/path/to/Chrome&#x27; // 指定 Chromium 的位置 args: [ &#x27;–disable-gpu&#x27;, // GPU硬件加速 &#x27;–disable-dev-shm-usage&#x27;, // 创建临时文件共享内存 &#x27;–disable-setuid-sandbox&#x27;, // uid沙盒 &#x27;–no-first-run&#x27;, // 没有设置首页。在启动的时候，就会打开一个空白页面。 &#x27;–no-sandbox&#x27;, // 沙盒模式 &#x27;–no-zygote&#x27;, &#x27;–single-process&#x27; // 单进程运行 ] &#125; const browser = await puppeteer.launch(); const page = await browser.newPage(); await page.goto(&#x27;https://example.com&#x27;); // 执行 javascript 并返回结果 const dimensions = await page.evaluate(() =&gt; &#123; return &#123; width: document.documentElement.clientWidth, height: document.documentElement.clientHeight, deviceScaleFactor: window.devicePixelRatio &#125;; &#125;); console.log(&#x27;Dimensions:&#x27;, dimensions); // 保存 png 图片 await page.screenshot(&#123;path: &#x27;example.png&#x27;&#125;); await browser.close();&#125;)(); conf 的一些参数 参数名称 参数类型 参数说明 ignoreHTTPSErrors boolean 在请求的过程中是否忽略 Https 报错信息，默认为 false headless boolean 是否以无头的模式运行 chrome, 也就是不显示 UI， 默认为 true executablePath string 可执行文件的路劲，Puppeteer 默认是使用它自带的 chrome webdriver, 如果你想指定一个自己的 webdriver 路径，可以通过这个参数设置 slowMo number 使 Puppeteer 操作减速，单位是毫秒。如果你想看看 Puppeteer 的整个工作过程，这个参数将非常有用。 args Array(String) 传递给 chrome 实例的其他参数，比如你可以使用–ash-host-window-bounds=1024x768 来设置浏览器窗口大小。更多参数参数列表可以参考这里 handleSIGINT boolean 是否允许通过进程信号控制 chrome 进程，也就是说是否可以使用 CTRL+C 关闭并退出浏览器. timeout number 等待 Chrome 实例启动的最长时间。默认为30000（30秒）。如果传入 0 的话则不限制时间 dumpio boolean 是否将浏览器进程stdout和stderr导入到process.stdout和process.stderr中。默认为false。 userDataDir string 设置用户数据目录，默认linux 是在 ~/.config 目录，window 默认在 C:\\Users\\AppData\\Local\\Google\\Chrome\\User Data, 其中 USER 代表当前登录的用户名 env Object 指定对Chromium可见的环境变量。默认为process.env。 devtools boolean 是否为每个选项卡自动打开DevTools面板， 这个选项只有当 headless 设置为 false 的时候有效 Browser 的一些方法 方法名称 返回值 说明 browser.close() Promise 关闭浏览器 browser.disconnect() void 断开浏览器连接 browser.newPage() Promise(Page) 创建一个 Page 实例 browser.pages() Promise(Array(Page)) 获取所有打开的 Page 实例 browser.targets() Array(Target) 获取所有活动的 targets browser.version() Promise(String) 获取浏览器的版本 browser.wsEndpoint() String 返回浏览器实例的 socket 连接 URL, 可以通过这个 URL 重连接 chrome 实例 做个人吧在我们使用 Puppeteer 做爬虫时，我们希望尽量像个正常的浏览器一样去做一些事，但是 Puppeteer 最初还是隐含了一些变量，该变量告诉web网站，我们并不是真正的人。 我们可以使用一些封装好的插件，来隐藏这些变量。 很多时候，我们会使用 该网站 来查看我们是不是像个人了。 npm文档 12yarn add puppeteer puppeteer-extra# npm install puppeteer puppeteer-extra 12345678910111213141516171819202122232425262728293031// puppeteer-extra is a drop-in replacement for puppeteer,// it augments the installed puppeteer with plugin functionality.// Any number of plugins can be added through `puppeteer.use()`const puppeteer = require(&#x27;puppeteer-extra&#x27;)// Add stealth plugin and use defaults (all tricks to hide puppeteer usage)const StealthPlugin = require(&#x27;puppeteer-extra-plugin-stealth&#x27;)puppeteer.use(StealthPlugin())// Add adblocker plugin to block all ads and trackers (saves bandwidth)const AdblockerPlugin = require(&#x27;puppeteer-extra-plugin-adblocker&#x27;)puppeteer.use(AdblockerPlugin(&#123; blockTrackers: true &#125;))// That&#x27;s it, the rest is puppeteer usage as normal ????puppeteer.launch(&#123; headless: true &#125;).then(async browser =&gt; &#123; const page = await browser.newPage() await page.setViewport(&#123; width: 800, height: 600 &#125;) console.log(`Testing adblocker plugin..`) await page.goto(&#x27;https://www.vanityfair.com&#x27;) await page.waitFor(1000) await page.screenshot(&#123; path: &#x27;adblocker.png&#x27;, fullPage: true &#125;) console.log(`Testing the stealth plugin..`) await page.goto(&#x27;https://bot.sannysoft.com&#x27;) await page.waitFor(5000) await page.screenshot(&#123; path: &#x27;stealth.png&#x27;, fullPage: true &#125;) console.log(`All done, check the screenshots. ✨`) await browser.close()&#125;) 你还想跑快点？首先：我们要知道，Puppeteer 并不能很好的实现并发，而且消耗的内存，CPU都是相对来说比较高的，而且使用不好，还容易出现大量失效的 chrome 进程。 然后：我们需要知道这货在哪里比较耗时，针对性的优化。 Puppeteer 在第一次启动的时候消耗了很多时间，我们可以只启动一次，然后将一些API提供出去，供调用者使用，结合 express 实现web接口的调用。 在请求一个页面的时候，会有很多的 css, js, font 文件一块加载了，很多时候都是没用的，我们可以拦截这些请求，进行快速返回，就不用等待那么长时间了。 我们总是要等待页面渲染完成之后，才会进行下一步的操作，那么什么时候渲染完成呢？不知道！ 等待固定的时间，emmm，最好别，因为网络总是不稳定的。 等待固定元素渲染完成，出现。 等待一段时间内没有网络请求。 多看看启动的参数，里面也有一些加速的选项，如GPU加速,缓存文件加速等等","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/"},{"name":"puppeteer","slug":"爬虫/puppeteer","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/puppeteer/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"puppeteer","slug":"puppeteer","permalink":"https://blog.ifan.host/tags/puppeteer/"}]},{"title":"pyppeteer 简单使用","slug":"spider/puppeteer/pyppeteer","date":"2022-06-03T04:58:00.000Z","updated":"2023-12-25T05:56:09.179Z","comments":true,"path":"2022/06/03/spider/puppeteer/pyppeteer/","link":"","permalink":"https://blog.ifan.host/2022/06/03/spider/puppeteer/pyppeteer/","excerpt":"","text":"API文档 初始化环境1pip install pyppeteer 下载依赖 linux: https://storage.googleapis.com/chromium-browser-snapshots/Linux_x64/575458/chrome-linux.zip mac: https://storage.googleapis.com/chromium-browser-snapshots/Mac/575458/chrome-mac.zip win32: https://storage.googleapis.com/chromium-browser-snapshots/Win/575458/chrome-win32.zip win64: https://storage.googleapis.com/chromium-browser-snapshots/Win_x64/575458/chrome-win32.zip 启动docker容器1docker run -d -p 3000:3000 browserless/chrome 访问 localhost:3000 打开页面API调用 API简单链接测试12345678910111213141516import asynciofrom pyppeteer import launchasync def main(): remote_browser_api = &quot;ws://localhost:30386&quot; browser = await connect( &#123;&quot;browserWSEndpoint&quot;: remote_browser_api, &quot;ignoreHTTPSErrors&quot;: True&#125; ) page = await browser.newPage() url = &#x27;https://intoli.com/blog/not-possible-to-block-chrome-headless/chrome-headless-test.html&#x27; await page.goto(url) # 截图 await page.screenshot(&#123;&#x27;path&#x27;: &#x27;chrome-headless-test.png&#x27;&#125;) await browser.close()asyncio.get_event_loop().run_until_complete(main()) 过检测1234567891011121314151617181920212223242526272829303132333435import asynciofrom pyppeteer import connectasync def main(): remote_browser_api = &quot;ws://localhost:3000&quot; browser = await connect(&#123; &quot;browserWSEndpoint&quot;: remote_browser_api, &quot;ignoreHTTPSErrors&quot;: True, &quot;args&quot;: [&#x27;--no-sandbox&#x27;], &quot;headless&quot;: True, &#125;) page = await browser.newPage() userAgent = &#x27;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.39 Safari/537.36&#x27; await page.setUserAgent(userAgent) await page.evaluateOnNewDocument(&#x27;() =&gt;&#123; Object.defineProperties(navigator, &#123; webdriver:&#123; get: () =&gt; false &#125; &#125;) &#125;&#x27;) await page.evaluateOnNewDocument(&quot;&quot;&quot;() =&gt; &#123;window.navigator.chrome = &#123; runtime: &#123;&#125;,&#125;&#125;&quot;&quot;&quot;) await page.evaluateOnNewDocument(&quot;&quot;&quot;() =&gt; &#123; const originalQuery = window.navigator.permissions.query; return window.navigator.permissions.query = (parameters) =&gt; ( parameters.name === &#x27;notifications&#x27; ? Promise.resolve(&#123; state: Notification.permission &#125;) : originalQuery(parameters) )&#125;&quot;&quot;&quot;) await page.evaluateOnNewDocument(&quot;&quot;&quot;() =&gt; &#123; Object.defineProperty(navigator, &#x27;plugins&#x27;, &#123; get: () =&gt; [1, 2, 3, 4, 5], &#125;);&#125;&quot;&quot;&quot;) url = &#x27;https://bot.sannysoft.com/&#x27; await page.goto(url) w = await page.evaluate(&quot;!window.chrome&quot;) print(w) # 截图 await page.screenshot(&#123;&#x27;path&#x27;: &#x27;chrome-headless-test.png&#x27;&#125;) await browser.close()asyncio.get_event_loop().run_until_complete(main()) 或者 1pip install pyppeteer_stealth 1234567891011121314import asynciofrom pyppeteer import launchfrom pyppeteer_stealth import stealthasync def main(): browser = await launch(headless=True) page = await browser.newPage() await stealth(page) # &lt;-- Here await page.goto(&quot;https://bot.sannysoft.com/&quot;) await page.screenshot(&#123;&#x27;path&#x27;: &#x27;chrome-headless-test.png&#x27;&#125;) await browser.close()asyncio.get_event_loop().run_until_complete(main()) 一些常用的方法设置代理12345678910111213141516171819browser = await launch(&#123;&#x27;headless&#x27;: True, &#x27;timeout&#x27;: 500, &#x27;args&#x27;: [&#x27;--disable-extensions&#x27;, &#x27;--hide-scrollbars&#x27;, &#x27;--disable-bundled-ppapi-flash&#x27;, &#x27;--mute-audio&#x27;, &#x27;--no-sandbox&#x27;, &#x27;--disable-setuid-sandbox&#x27;, &#x27;--disable-gpu&#x27;, &#x27;--proxy-server=localhost:1080&#x27;, ], &#125;)# 设置代理ip验证await page.authenticate(&#123; &#x27;username&#x27;: &#x27;用户名&#x27;, &#x27;password&#x27;: &#x27;密码&#x27;&#125;)# 设置User-Agentpage = await browser.newPage()await page.setUserAgent(&quot;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&quot;) 执行JS123456789# 在网页上执行js 脚本dimensions = await page.evaluate(pageFunction=&#x27;&#x27;&#x27;() =&gt; &#123; return &#123; width: document.documentElement.clientWidth, // 页面宽度 height: document.documentElement.clientHeight, // 页面高度 deviceScaleFactor: window.devicePixelRatio, // 像素比 1.0000000149011612 &#125;&#125;&#x27;&#x27;&#x27;, force_expr=False) # force_expr=False 执行的是函数print(dimensions) 设置页面视图大小1await page.setViewport(viewport=&#123;&#x27;width&#x27;: 1280, &#x27;height&#x27;: 800&#125;) 超时间见 1000 毫秒123res = await page.goto(&#x27;https://www.toutiao.com/&#x27;, options=&#123;&#x27;timeout&#x27;: 1000&#125;)resp_headers = res.headers # 响应头resp_status = res.status # 响应状态 等待123await asyncio.sleep(2)# 第二种方法，在while循环里强行查询某元素进行等待await page.querySelector(&#x27;.t&#x27;) 滚动到页面底部1await page.evaluate(&#x27;window.scrollBy(0, document.body.scrollHeight)&#x27;) 截图 保存图片1await page.screenshot(&#123;&#x27;path&#x27;: &#x27;toutiao.png&#x27;&#125;) 打印页面cookies1print(await page.cookies()) 获取所有 html 内容1print(await page.content()) 监听请求和响应1234567891011121314151617page.on(&#x27;request&#x27;, intercept_response)page.on(&quot;response&quot;, request_check)# 请求处理函数async def request_check(req): &#x27;&#x27;&#x27;请求过滤&#x27;&#x27;&#x27; if req.resourceType in [&#x27;image&#x27;, &#x27;media&#x27;, &#x27;eventsource&#x27;, &#x27;websocket&#x27;]: await req.abort() else: await req.continue_()# 响应处理函数async def intercept_response(res): resourceType = res.request.resourceType if resourceType in [&#x27;image&#x27;, &#x27;media&#x27;]: resp = await res.text() print(resp) 使用 xvfb 做有屏幕的显示Docker地址 本地安装 12sudo apt-get updatesudo apt-get install xvfb 12345678import timefrom selenium.webdriver import Chromedriver = Chrome(&#x27;./chromedriver&#x27;)driver.get(&#x27;https://bot.sannysoft.com/&#x27;)time.sleep(5)driver.save_screenshot(&#x27;screenshot.png&#x27;)driver.close()print(&#x27;运行完成&#x27;) 1xvfb-run python3 test.py -s -screen 0 1920x1080x16 DemoFunc123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257# coding:utf8import reimport asyncioimport pyppeteerfrom pyppeteer import launcherif pyppeteer.version &lt;= &quot;0.0.25&quot;: # hook 禁用 防止监测webdriver launcher.AUTOMATION_ARGS.remove(&quot;--enable-automation&quot;)from pyppeteer import launchfrom pyppeteer.network_manager import Request, Responsefrom pyppeteer.dialog import Dialogproxy = &quot;http://127.0.0.1:1080&quot;args = [ &quot;--start-maximized&quot;, &quot;--no-sandbox&quot;, &quot;--ignore-certificate-errors&quot;, &quot;--log-level=3&quot;, &quot;--enable-extensions&quot;, &quot;--window-size=1920,1080&quot;, # &quot;--proxy-server=&#123;&#125;&quot;.format(proxy), &quot;--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36&quot;,]if pyppeteer.version &lt;= &quot;0.0.25&quot;: args.append(&quot;--disable-infobars&quot;)else: args.append(&quot;--disable-blink-features=AutomationControlled&quot;)launch_args = &#123; &quot;headless&quot;: False, &quot;args&quot;: args, &quot;autoClose&quot;: False, &quot;dumpio&quot;: True,&#125;if pyppeteer.version &gt; &quot;0.0.25&quot;: launch_args.update( &#123;&quot;ignoreDefaultArgs&quot;: [&quot;--enable-automation&quot;, &quot;--disable-extensions&quot;]&#125; )async def modify_url(request: Request): &quot;&quot;&quot; # 启用拦截器 await page.setRequestInterception(True) page.on(&quot;request&quot;, use_proxy_base) :param request: :return: &quot;&quot;&quot; if request.url == &quot;https://www.baidu.com/&quot;: await request.continue_(&#123;&quot;url&quot;: &quot;https://www.baidu.com/s?wd=ip&amp;ie=utf-8&quot;&#125;) else: await request.continue_()async def get_content(response: Response): &quot;&quot;&quot; # 注意这里不需要设置 page.setRequestInterception(True) page.on(&quot;response&quot;, get_content) :param response: :return: &quot;&quot;&quot; if response.url == &quot;https://www.baidu.com/&quot;: content = await response.text() title = re.search(b&quot;&lt;title&gt;(.*?)&lt;/title&gt;&quot;, content) print(title.group(1))async def handle_dialog(dialog: Dialog): &quot;&quot;&quot; page.on(&quot;dialog&quot;, get_content) :param dialog: :return: &quot;&quot;&quot; await dialog.dismiss()import aiohttpaiohttp_session = aiohttp.ClientSession(loop=asyncio.get_event_loop())async def use_proxy_base(request: Request): &quot;&quot;&quot; # 启用拦截器 await page.setRequestInterception(True) page.on(&quot;request&quot;, use_proxy_base) :param request: :return: &quot;&quot;&quot; # 构造请求并添加代理 req = &#123; &quot;headers&quot;: request.headers, &quot;data&quot;: request.postData, &quot;proxy&quot;: proxy, # 使用全局变量 则可随意切换 &quot;timeout&quot;: 5, &quot;ssl&quot;: False, &#125; try: # 使用第三方库获取响应 async with aiohttp_session.request( method=request.method, url=request.url, **req ) as response: body = await response.read() except Exception as e: await request.abort() return # 数据返回给浏览器 resp = &#123;&quot;body&quot;: body, &quot;headers&quot;: response.headers, &quot;status&quot;: response.status&#125; await request.respond(resp) return# 静态资源缓存static_cache = &#123;&#125;async def use_proxy_and_cache(request: Request): &quot;&quot;&quot; # 启用拦截器 await page.setRequestInterception(True) page.on(&quot;request&quot;, use_proxy_base) :param request: :return: &quot;&quot;&quot; global static_cache if request.url not in static_cache: # 构造请求并添加代理 req = &#123; &quot;headers&quot;: request.headers, &quot;data&quot;: request.postData, &quot;proxy&quot;: proxy, # 使用全局变量 则可随意切换 &quot;timeout&quot;: 5, &quot;ssl&quot;: False, &#125; try: # 使用第三方库获取响应 async with aiohttp_session.request( method=request.method, url=request.url, **req ) as response: body = await response.read() except Exception as e: await request.abort() return # 数据返回给浏览器 resp = &#123;&quot;body&quot;: body, &quot;headers&quot;: response.headers, &quot;status&quot;: response.status&#125; # 判断数据类型 如果是静态文件则缓存起来 content_type = response.headers.get(&quot;Content-Type&quot;) if content_type and (&quot;javascript&quot; in content_type or &quot;/css&quot; in content_type): static_cache[request.url] = resp else: resp = static_cache[request.url] await request.respond(resp) returnasync def pass_webdriver(request: Request): &quot;&quot;&quot; # 启用拦截器 await page.setRequestInterception(True) page.on(&quot;request&quot;, use_proxy_base) :param request: :return: &quot;&quot;&quot; # 构造请求并添加代理 req = &#123; &quot;headers&quot;: request.headers, &quot;data&quot;: request.postData, &quot;proxy&quot;: proxy, # 使用全局变量 则可随意切换 &quot;timeout&quot;: 5, &quot;ssl&quot;: False, &#125; try: # 使用第三方库获取响应 async with aiohttp_session.request( method=request.method, url=request.url, **req ) as response: body = await response.read() except Exception as e: await request.abort() return if request.url == &quot;https://www.baidu.com/&quot;: with open(&quot;pass_webdriver.js&quot;) as f: js = f.read() # 在html源码头部添加js代码 修改navigator属性 body = body.replace(b&quot;&lt;title&gt;&quot;, b&quot;&lt;script&gt;%s&lt;/script&gt;&lt;title&gt;&quot; % js.encode()) # 数据返回给浏览器 resp = &#123;&quot;body&quot;: body, &quot;headers&quot;: response.headers, &quot;status&quot;: response.status&#125; await request.respond(resp) returnasync def interception_test(): # 启动浏览器 browser = await launch(**launch_args) # 新建标签页 page = await browser.newPage() # 设置页面打开超时时间 page.setDefaultNavigationTimeout(10 * 1000) # 设置窗口大小 await page.setViewport(&#123;&quot;width&quot;: 1920, &quot;height&quot;: 1080&#125;) # 设置拦截器 # 1. 修改请求的url if 0: # 启用拦截器 await page.setRequestInterception(True) # 捕获request，response if pyppeteer.version &lt;= &quot;0.0.25&quot;: page.on(&quot;request&quot;, modify_url) else: page.on(&quot;request&quot;, lambda r: asyncio.ensure_future(modify_url(r))) # 2. 获取响应内容 if 0: # 注意这里不需要设置 page.setRequestInterception(True) if pyppeteer.version &lt;= &quot;0.0.25&quot;: page.on(&quot;response&quot;, get_content) else: page.on(&quot;response&quot;, lambda r: asyncio.ensure_future(get_content(r))) # 3. 使用代理 if 0: # 启用拦截器 await page.setRequestInterception(True) if pyppeteer.version &lt;= &quot;0.0.25&quot;: page.on(&quot;request&quot;, pass_webdriver) # page.on(&quot;request&quot;, use_proxy_base) # page.on(&quot;request&quot;, use_proxy_and_cache) else: page.on(&quot;request&quot;, lambda r: asyncio.ensure_future(pass_webdriver(r))) await page.goto(&quot;http://www.baidu.com&quot;) await asyncio.sleep(10) # 关闭浏览器 await page.close() await browser.close() returnif __name__ == &quot;__main__&quot;: loop = asyncio.get_event_loop() loop.run_until_complete(interception_test())","categories":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/"},{"name":"pyppeteer","slug":"爬虫/pyppeteer","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/pyppeteer/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"pyppeteer","slug":"pyppeteer","permalink":"https://blog.ifan.host/tags/pyppeteer/"}]},{"title":"nginx正向代理、tinyproxy和goproxy代理","slug":"tools/proxy","date":"2022-06-03T00:56:00.000Z","updated":"2023-12-25T05:56:09.185Z","comments":true,"path":"2022/06/03/tools/proxy/","link":"","permalink":"https://blog.ifan.host/2022/06/03/tools/proxy/","excerpt":"","text":"nginx 正向代理1. 安装 openresty1234567891011121314151617181920yum -y install patchwget https://openresty.org/download/openresty-1.19.9.1.tar.gztar -zvxf openresty-1.19.9.1.tar.gzcd openresty-1.19.9.1/mkdir modules/cd modules/git clone https://github.com/chobits/ngx_http_proxy_connect_module.gitcd .. ./configure \\ --add-module=/root/openresty-1.19.9.1/modules/ngx_http_proxy_connect_module \\ --prefix=/root/software/openresty \\ --with-luajit \\ --without-http_redis2_module \\ --with-http_iconv_module \\ --with-http_postgres_module \\ --with-mail \\ --with-streampatch -d build/nginx-1.19.9/ -p1 &lt; /root/openresty-1.19.9.1/modules/ngx_http_proxy_connect_module/patch/proxy_connect_rewrite_1018.patchgmake &amp;&amp; gmake install 2. 配置代理12345678910111213141516171819202122server &#123; listen 1080; access_log logs/proxy.log; resolver 114.114.114.114; #指定DNS服务器IP地址 server_name proxy.ifan.tutulis.com; proxy_connect; proxy_connect_allow 80 443; proxy_connect_connect_timeout 10s; proxy_connect_read_timeout 10s; proxy_connect_send_timeout 10s; # forward proxy for non-CONNECT request location / &#123; # proxy_pass $scheme://$http_host$request_uri proxy_pass http://$host; proxy_set_header Host $host; proxy_buffers 256 4k; proxy_max_temp_file_size 0; proxy_connect_timeout 10s; &#125;&#125; 3. 测试代理1curl https://www.baidu.com -v -x 127.0.0.1 tinyproxy使用Docker搭建1docker run -d --name tinyproxy --restart always -p 8888:8888 stilleshan/tinyproxy BasicAuth 加密1docker run -d --name tinyproxy --restart always -p 8888:8888 -v /opt/tinyproxy/tinyproxy.conf:/etc/tinyproxy/tinyproxy.conf stilleshan/tinyproxy 测试1curl -x https://IP:8888 cip.cc Goproxy官网Github 下载使用 1234mkdir proxy &amp;&amp; cd proxywget https://github.91chi.fun//https://github.com//snail007/goproxy/releases/download/v11.8/proxy-linux-amd64.tar.gztar -zxvf proxy-linux-amd64.tar.gzrm ._* 配置文件启动创建 configfile.txt 1234http -t tcp -p :33080 --forever 启动 1proxy @configfile.txt 守护状态1proxy http -p &quot;:9090&quot; --forever --log proxy.log --daemon","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"proxy","slug":"工具/proxy","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/proxy/"},{"name":"爬虫","slug":"工具/proxy/爬虫","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/proxy/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"proxy","slug":"proxy","permalink":"https://blog.ifan.host/tags/proxy/"}]},{"title":"GoLang resty http请求","slug":"language/golang/libs/resty","date":"2022-04-29T13:00:00.000Z","updated":"2023-12-25T05:56:09.172Z","comments":true,"path":"2022/04/29/language/golang/libs/resty/","link":"","permalink":"https://blog.ifan.host/2022/04/29/language/golang/libs/resty/","excerpt":"","text":"安装1go get -u github.com/go-resty/resty/v2 基本请求1234567891011121314151617181920212223242526272829303132333435363738394041424344func main() &#123; // Create a Resty Client client := resty.New() resp, err := client.R(). EnableTrace(). Get(&quot;https://httpbin.org/get&quot;) // Post() // Put() // Patch() // Head() // Option() // Delete() // Explore response object fmt.Println(&quot;Response Info:&quot;) fmt.Println(&quot; Error :&quot;, err) fmt.Println(&quot; Status Code:&quot;, resp.StatusCode()) // 状态码，如 200； fmt.Println(&quot; Status :&quot;, resp.Status()) // 状态码和状态信息，如 200 OK； fmt.Println(&quot; Proto :&quot;, resp.Proto()) // 协议，如 HTTP/1.1； fmt.Println(&quot; Time :&quot;, resp.Time()) // 从发送请求到收到响应的时间； fmt.Println(&quot; Received At:&quot;, resp.ReceivedAt()) // 接收到响应的时刻； fmt.Println(&quot; Size :&quot;, resp.Size()) // 响应大小； fmt.Println(&quot; Headers :&quot;, resp.Header()) // 响应首部信息，以http.Header类型返回，即map[string][]string； fmt.Println(&quot; Cookies :&quot;, resp.Cookies()) // 服务器通过Set-Cookie首部设置的 cookie 信息。 fmt.Println(&quot; Body :\\n&quot;, resp) fmt.Println() // Explore trace info fmt.Println(&quot;Request Trace Info:&quot;) ti := resp.Request.TraceInfo() fmt.Println(&quot; DNSLookup :&quot;, ti.DNSLookup) // DNS 查询时间，如果提供的是一个域名而非 `IP`，就需要向 `DNS` 系统查询对应 `IP` 才能进行后续操作； fmt.Println(&quot; ConnTime :&quot;, ti.ConnTime) // 获取一个连接的耗时，可能从连接池获取，也可能新建； fmt.Println(&quot; TCPConnTime :&quot;, ti.TCPConnTime) // `TCP` 连接耗时，从 `DNS` 查询结束到 `TCP` 连接建立； fmt.Println(&quot; TLSHandshake :&quot;, ti.TLSHandshake) // `TLS` 握手耗时； fmt.Println(&quot; ServerTime :&quot;, ti.ServerTime) // 服务器处理耗时，计算从连接建立到客户端收到第一个字节的时间间隔； fmt.Println(&quot; ResponseTime :&quot;, ti.ResponseTime) // 响应耗时，从接收到第一个响应字节，到接收到完整响应之间的时间间隔； fmt.Println(&quot; TotalTime :&quot;, ti.TotalTime) // 整个流程的耗时； fmt.Println(&quot; IsConnReused :&quot;, ti.IsConnReused) // `TCP` 连接是否复用了； fmt.Println(&quot; IsConnWasIdle :&quot;, ti.IsConnWasIdle) // 连接是否是从空闲的连接池获取的； fmt.Println(&quot; ConnIdleTime :&quot;, ti.ConnIdleTime) // 连接空闲时间； fmt.Println(&quot; RequestAttempt:&quot;, ti.RequestAttempt) // 请求执行流程中的请求次数，包括重试次数； fmt.Println(&quot; RemoteAddr :&quot;, ti.RemoteAddr.String()) // 远程的服务地址，`IP:PORT`格式。&#125; 通用设置请求参数1234567891011121314151617181920212223func main() &#123; client := resty.New() resp, err := client.R(). SetQueryParams(map[string]string&#123; &quot;page_no&quot;: &quot;1&quot;, &quot;limit&quot;: &quot;20&quot;, &quot;sort&quot;:&quot;name&quot;, &quot;order&quot;: &quot;asc&quot;, &quot;random&quot;:strconv.FormatInt(time.Now().Unix(), 10), &#125;). // .SetQueryString(&quot;productId=232&amp;template=fresh-sample&amp;cat=resty&amp;source=google&amp;kw=buy a lot more&quot;). SetHeader(&quot;Accept&quot;, &quot;application/json&quot;). // 设置请求头 SetAuthToken(&quot;BC594900518B4F7EAC75BD37F019E08FBC594900518B4F7EAC75BD37F019E08F&quot;). Get(&quot;/search_result&quot;) // 请求路径中传参 client.R(). SetPathParams(map[string]string&#123; &quot;user&quot;: &quot;ifan&quot;, &#125;). Get(&quot;/users/&#123;user&#125;/details&quot;)&#125; 绑定返回信息1234567891011121314151617181920212223func main() &#123; type JsonData struct &#123; Name string Latest string &#125; type Datas struct &#123; Results []*JsonData &#125; client := resty.New() datas := &amp;Datas&#123;&#125; client.R().SetResult(datas). ForceContentType(&quot;application/json&quot;). // 设置接受的返回数据类型 Get(&quot;json_data_url&quot;) fmt.Printf(&quot;%d data len \\n&quot;, len(datas.Results)) for _, data := range datas.Results &#123; fmt.Printf(&quot;name:%s latest:%s\\n&quot;, data.Name, data.Latest) break &#125;&#125; 设置Body参数12345678910111213141516171819202122232425func main() &#123; // 字符串形式 resp, err := client.R(). SetHeader(&quot;Content-Type&quot;, &quot;application/json&quot;). SetBody(`&#123;&quot;username&quot;:&quot;testuser&quot;, &quot;password&quot;:&quot;testpass&quot;&#125;`). .Post(&quot;url&quot;) resp, err := client.R(). SetHeader(&quot;Content-Type&quot;, &quot;application/json&quot;). SetBody([]byte(`&#123;&quot;username&quot;:&quot;testuser&quot;, &quot;password&quot;:&quot;testpass&quot;&#125;`)). Post(&quot;url&quot;) // 使用结构体 resp, err := client.R(). SetBody(User&#123;Username: &quot;testuser&quot;, Password: &quot;testpass&quot;&#125;). Post(&quot;url&quot;) // 上传文件 fileBytes, _ := ioutil.ReadFile(&quot;/Users/jeeva/mydocument.pdf&quot;) resp, err := client.R(). SetBody(fileBytes). SetContentLength(true). // 自动计算请求体长度 SetError(&amp;DropboxError&#123;&#125;). // 设置错误返回的解析内容 Post(&quot;url&quot;)&#125; 上传多个文件12345678910111213141516func main() &#123; profileImgBytes, _ := ioutil.ReadFile(&quot;/Users/jeeva/test-img.png&quot;) notesBytes, _ := ioutil.ReadFile(&quot;/Users/jeeva/text-file.txt&quot;) // Create a Resty Client client := resty.New() resp, err := client.R(). SetFileReader(&quot;profile_img&quot;, &quot;test-img.png&quot;, bytes.NewReader(profileImgBytes)). SetFileReader(&quot;notes&quot;, &quot;text-file.txt&quot;, bytes.NewReader(notesBytes)). SetFormData(map[string]string&#123; &quot;first_name&quot;: &quot;Jeevanandam&quot;, &quot;last_name&quot;: &quot;M&quot;, &#125;). Post(&quot;http://myapp.com/upload&quot;)&#125; 1234567891011121314151617181920212223242526272829303132func main() &#123; // Create a Resty Client client := resty.New() // Single file scenario resp, err := client.R(). SetFile(&quot;profile_img&quot;, &quot;/Users/jeeva/test-img.png&quot;). Post(&quot;http://myapp.com/upload&quot;) // Multiple files scenario resp, err := client.R(). SetFiles(map[string]string&#123; &quot;profile_img&quot;: &quot;/Users/jeeva/test-img.png&quot;, &quot;notes&quot;: &quot;/Users/jeeva/text-file.txt&quot;, &#125;). Post(&quot;http://myapp.com/upload&quot;) // Multipart of form fields and files resp, err := client.R(). SetFiles(map[string]string&#123; &quot;profile_img&quot;: &quot;/Users/jeeva/test-img.png&quot;, &quot;notes&quot;: &quot;/Users/jeeva/text-file.txt&quot;, &#125;). SetFormData(map[string]string&#123; &quot;first_name&quot;: &quot;Jeevanandam&quot;, &quot;last_name&quot;: &quot;M&quot;, &quot;zip_code&quot;: &quot;00001&quot;, &quot;city&quot;: &quot;my city&quot;, &quot;access_token&quot;: &quot;C6A79608-782F-4ED0-A11D-BD82FAD829CD&quot;, &#125;). Post(&quot;http://myapp.com/profile&quot;)&#125; 设置回调方法12345678910111213141516171819202122func main()&#123; client := resty.New() // 请求前 client.OnBeforeRequest(func(c *resty.Client, req *resty.Request) error &#123; return nil &#125;) // 响应后 client.OnAfterResponse(func(c *resty.Client, resp *resty.Response) error &#123; return nil &#125;) // 发生错误时 client.OnError(func(req *resty.Request, err error) &#123; if v, ok := err.(*resty.ResponseError); ok &#123; &#125; &#125;)&#125; 重定向策略123456789101112131415161718192021222324252627282930313233343536func main()&#123; client := resty.New() // Assign Client Redirect Policy. Create one as per you need client.SetRedirectPolicy(resty.FlexibleRedirectPolicy(15)) // Wanna multiple policies such as redirect count, domain name check, etc client.SetRedirectPolicy(resty.FlexibleRedirectPolicy(20), resty.DomainCheckRedirectPolicy(&quot;host1.com&quot;, &quot;host2.org&quot;, &quot;host3.net&quot;)) // 自定义 // Using raw func into resty.SetRedirectPolicy client.SetRedirectPolicy(resty.RedirectPolicyFunc(func(req *http.Request, via []*http.Request) error &#123; // Implement your logic here // return nil for continue redirect otherwise return error to stop/prevent redirect return nil &#125;)) //--------------------------------------------------- // Using struct create more flexible redirect policy type CustomRedirectPolicy struct &#123; // variables goes here &#125; func (c *CustomRedirectPolicy) Apply(req *http.Request, via []*http.Request) error &#123; // Implement your logic here // return nil for continue redirect otherwise return error to stop/prevent redirect return nil &#125; // Registering in resty client.SetRedirectPolicy(CustomRedirectPolicy&#123;/* initialize variables */&#125;)&#125; 自定义证书1234567891011121314151617181920212223242526272829303132333435363738394041func main() &#123; // Create a Resty Client client := resty.New() // Custom Root certificates, just supply .pem file. // you can add one or more root certificates, its get appended client.SetRootCertificate(&quot;/path/to/root/pemFile1.pem&quot;) client.SetRootCertificate(&quot;/path/to/root/pemFile2.pem&quot;) // ... and so on! // Adding Client Certificates, you add one or more certificates // Sample for creating certificate object // Parsing public/private key pair from a pair of files. The files must contain PEM encoded data. cert1, err := tls.LoadX509KeyPair(&quot;certs/client.pem&quot;, &quot;certs/client.key&quot;) if err != nil &#123; log.Fatalf(&quot;ERROR client certificate: %s&quot;, err) &#125; // ... // You add one or more certificates client.SetCertificates(cert1, cert2, cert3) // Custom Root certificates from string // You can pass you certificates throught env variables as strings // you can add one or more root certificates, its get appended client.SetRootCertificateFromString(&quot;-----BEGIN CERTIFICATE-----content-----END CERTIFICATE-----&quot;) client.SetRootCertificateFromString(&quot;-----BEGIN CERTIFICATE-----content-----END CERTIFICATE-----&quot;) // ... and so on! // Adding Client Certificates, you add one or more certificates // Sample for creating certificate object // Parsing public/private key pair from a pair of files. The files must contain PEM encoded data. cert1, err := tls.X509KeyPair([]byte(&quot;-----BEGIN CERTIFICATE-----content-----END CERTIFICATE-----&quot;), []byte(&quot;-----BEGIN CERTIFICATE-----content-----END CERTIFICATE-----&quot;)) if err != nil &#123; log.Fatalf(&quot;ERROR client certificate: %s&quot;, err) &#125; // ... // You add one or more certificates client.SetCertificates(cert1, cert2, cert3)&#125; 自定义代理1234567891011func main() &#123; // Create a Resty Client client := resty.New() // Setting a Proxy URL and Port // 验证呢？ client.SetProxy(&quot;http://proxyserver:8888&quot;) // 移除代理 client.RemoveProxy()&#125; 重试12345678910111213141516171819202122232425262728func main() &#123; // Create a Resty Client client := resty.New() // Retries are configured per client client. // Set retry count to non zero to enable retries SetRetryCount(3). // You can override initial retry wait time. // Default is 100 milliseconds. SetRetryWaitTime(5 * time.Second). // MaxWaitTime can be overridden as well. // Default is 2 seconds. SetRetryMaxWaitTime(20 * time.Second). // SetRetryAfter sets callback to calculate wait time between retries. // Default (nil) implies exponential backoff with jitter SetRetryAfter(func(client *resty.Client, resp *resty.Response) (time.Duration, error) &#123; return 0, errors.New(&quot;quota exceeded&quot;) &#125;) // 自定义重试策略 client.AddRetryCondition( // RetryConditionFunc type is for retry condition function // input: non-nil Response OR request execution error func(r *resty.Response, err error) bool &#123; return r.StatusCode() == http.StatusTooManyRequests &#125;, )&#125; Allow GET request with Payload12345678func main() &#123; // Create a Resty Client client := resty.New() // Allow GET request with Payload. This is disabled by default. client.SetAllowGetMethodPayload(true)&#125; 其他的一些参数设置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667func main() &#123; client := resty.New() // Unique settings at Client level //-------------------------------- // Debug 模式 client.SetDebug(true) // 设置自定义证书 Refer: http://golang.org/pkg/crypto/tls/#example_Dial client.SetTLSClientConfig(&amp;tls.Config&#123; RootCAs: roots &#125;) // 禁用安全检查 client.SetTLSClientConfig(&amp;tls.Config&#123; InsecureSkipVerify: true &#125;) // 设置链接超时 client.SetTimeout(1 * time.Minute) // You can override all below settings and options at request level if you want to //-------------------------------------------------------------------------------- // 设置请求的根地址，后面的请求中可以使用相对路径 client.SetHostURL(&quot;http://httpbin.org&quot;) // 设置所有请求的请求头 client.SetHeader(&quot;Accept&quot;, &quot;application/json&quot;) client.SetHeaders(map[string]string&#123; &quot;Content-Type&quot;: &quot;application/json&quot;, &quot;User-Agent&quot;: &quot;My custom User Agent String&quot;, &#125;) // 设置所有请求的Cookie client.SetCookie(&amp;http.Cookie&#123; Name:&quot;go-resty&quot;, Value:&quot;This is cookie value&quot;, Path: &quot;/&quot;, Domain: &quot;sample.com&quot;, MaxAge: 36000, HttpOnly: true, Secure: false, &#125;) client.SetCookies(cookies) // 设置所有请求的 query parameters client.SetQueryParam(&quot;user_id&quot;, &quot;00001&quot;) client.SetQueryParams(map[string]string&#123; // sample of those who use this manner &quot;api_key&quot;: &quot;api-key-here&quot;, &quot;api_secert&quot;: &quot;api-secert&quot;, &#125;) client.R().SetQueryString(&quot;productId=232&amp;template=fresh-sample&amp;cat=resty&amp;source=google&amp;kw=buy a lot more&quot;) // 所有请求的Form data，POST and PUT client.SetFormData(map[string]string&#123; &quot;access_token&quot;: &quot;BC594900-518B-4F7E-AC75-BD37F019E08F&quot;, &#125;) // Basic Auth for all request client.SetBasicAuth(&quot;myuser&quot;, &quot;mypass&quot;) // Bearer Auth Token for all request client.SetAuthToken(&quot;BC594900518B4F7EAC75BD37F019E08FBC594900518B4F7EAC75BD37F019E08F&quot;) // 自动设置请求体的长度 client.SetContentLength(true) // 设置所有请求的异常处理 client.SetError(&amp;Error&#123;&#125;) // or resty.SetError(Error&#123;&#125;)&#125; Unix Socket1234567891011121314151617181920func main()&#123; unixSocket := &quot;/var/run/my_socket.sock&quot; // Create a Go&#x27;s http.Transport so we can set it in resty. transport := http.Transport&#123; Dial: func(_, _ string) (net.Conn, error) &#123; return net.Dial(&quot;unix&quot;, unixSocket) &#125;, &#125; // Create a Resty Client client := resty.New() // Set the previous transport that we created, set the scheme of the communication to the // socket and set the unixSocket as the HostURL. client.SetTransport(&amp;transport).SetScheme(&quot;http&quot;).SetHostURL(unixSocket) // No need to write the host&#x27;s URL on the request, just the path. client.R().Get(&quot;/index.html&quot;)&#125; 模拟发送请求1234567func main() &#123; // Create a Resty Client client := resty.New() // Get the underlying HTTP Client and set it to Mock httpmock.ActivateNonDefault(client.GetClient())&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang ants 高性能的 goroutine 池","slug":"language/golang/libs/ants","date":"2022-04-28T02:00:00.000Z","updated":"2023-12-25T05:56:09.171Z","comments":true,"path":"2022/04/28/language/golang/libs/ants/","link":"","permalink":"https://blog.ifan.host/2022/04/28/language/golang/libs/ants/","excerpt":"","text":"ants 自动调度海量的 goroutines，复用 goroutines 定期清理过期的 goroutines，进一步节省资源 提供了大量有用的接口：任务提交、获取运行中的 goroutine 数量、动态调整 Pool 大小、释放 Pool、重启 Pool 优雅处理 panic，防止程序崩溃 资源复用，极大节省内存使用量；在大规模批量并发任务场景下比原生 goroutine 并发具有更高的性能 非阻塞机制 安装1go get -u github.com/panjf2000/ants/v2 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package mainimport ( &quot;fmt&quot; &quot;sync&quot; &quot;sync/atomic&quot; &quot;time&quot; &quot;github.com/panjf2000/ants/v2&quot;)var sum int32func myFunc(i interface&#123;&#125;) &#123; n := i.(int32) atomic.AddInt32(&amp;sum, n) fmt.Printf(&quot;run with %d\\n&quot;, n)&#125;func demoFunc() &#123; time.Sleep(10 * time.Millisecond) fmt.Println(&quot;Hello World!&quot;)&#125;func main() &#123; defer ants.Release() runTimes := 1000 // Use the common pool. var wg sync.WaitGroup syncCalculateSum := func() &#123; demoFunc() wg.Done() &#125; for i := 0; i &lt; runTimes; i++ &#123; wg.Add(1) _ = ants.Submit(syncCalculateSum) &#125; wg.Wait() fmt.Printf(&quot;running goroutines: %d\\n&quot;, ants.Running()) fmt.Printf(&quot;finish all tasks.\\n&quot;) // Use the pool with a function, // set 10 to the capacity of goroutine pool and 1 second for expired duration. p, _ := ants.NewPoolWithFunc(10, func(i interface&#123;&#125;) &#123; myFunc(i) wg.Done() &#125;) defer p.Release() // Submit tasks one by one. for i := 0; i &lt; runTimes; i++ &#123; wg.Add(1) _ = p.Invoke(int32(i)) &#125; wg.Wait() fmt.Printf(&quot;running goroutines: %d\\n&quot;, p.Running()) fmt.Printf(&quot;finish all tasks, result is %d\\n&quot;, sum)&#125; Pool 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// Option represents the optional function.type Option func(opts *Options)// Options contains all options which will be applied when instantiating a ants pool.type Options struct &#123; // 方法的持续时间，超过该时间，会被清理掉 ExpiryDuration time.Duration // 是否进行内存预分配 PreAlloc bool // 最大阻塞任务数量。即池中 goroutine 数量已到池容量，且所有 goroutine 都处理繁忙状态，这时到来的任务会在阻塞列表等待。 // 这个选项设置的是列表的最大长度。 // 阻塞的任务数量达到这个值后，后续任务提交直接返回失败 MaxBlockingTasks int // 池是否阻塞，默认阻塞。 // 提交任务时，如果ants池中 goroutine 已到上限且全部繁忙，阻塞的池会将任务添加的阻塞列表等待（当然受限于阻塞列表长度，见上一个选项）。 // 非阻塞的池直接返回失败 Nonblocking bool // panic 处理。遇到 panic 会调用这里设置的处理函数 PanicHandler func(interface&#123;&#125;) // 指定日志记录器 Logger Logger&#125;// WithOptions accepts the whole options config.func WithOptions(options Options) Option &#123; return func(opts *Options) &#123; *opts = options &#125;&#125;// WithExpiryDuration sets up the interval time of cleaning up goroutines.func WithExpiryDuration(expiryDuration time.Duration) Option &#123; return func(opts *Options) &#123; opts.ExpiryDuration = expiryDuration &#125;&#125;// WithPreAlloc indicates whether it should malloc for workers.func WithPreAlloc(preAlloc bool) Option &#123; return func(opts *Options) &#123; opts.PreAlloc = preAlloc &#125;&#125;// WithMaxBlockingTasks sets up the maximum number of goroutines that are blocked when it reaches the capacity of pool.func WithMaxBlockingTasks(maxBlockingTasks int) Option &#123; return func(opts *Options) &#123; opts.MaxBlockingTasks = maxBlockingTasks &#125;&#125;// WithNonblocking indicates that pool will return nil when there is no available workers.func WithNonblocking(nonblocking bool) Option &#123; return func(opts *Options) &#123; opts.Nonblocking = nonblocking &#125;&#125;// WithPanicHandler sets up panic handler.func WithPanicHandler(panicHandler func(interface&#123;&#125;)) Option &#123; return func(opts *Options) &#123; opts.PanicHandler = panicHandler &#125;&#125;// WithLogger sets up a customized logger.func WithLogger(logger Logger) Option &#123; return func(opts *Options) &#123; opts.Logger = logger &#125;&#125; 动态调整 goroutine 池容量需要动态调整 goroutine 池容量可以通过调用Tune(int)： 12pool.Tune(1000) // Tune its capacity to 1000pool.Tune(100000) // Tune its capacity to 100000 释放 Pool1pool.Release() 重启 Pool12// 只要调用 Reboot() 方法，就可以重新激活一个之前已经被销毁掉的池，并且投入使用。pool.Reboot()","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"GoLang zap 日志框架","slug":"language/golang/libs/zap","date":"2022-04-28T02:00:00.000Z","updated":"2023-12-25T05:56:09.173Z","comments":true,"path":"2022/04/28/language/golang/libs/zap/","link":"","permalink":"https://blog.ifan.host/2022/04/28/language/golang/libs/zap/","excerpt":"","text":"zap安装1go get -u go.uber.org/zap 配置 SugaredLogger: 慢，支持结构化和printf风格的日志记录。 Logger: 快，内存分配次数也更少，只支持强类型的结构化日志记录。 1234567891011121314151617181920func main() &#123; // Example适合用在测试代码中，Development在开发环境中使用，Production用在生成环境。 logger := zap.NewExample() defer logger.Sync() url := &quot;http://example.org/api&quot; logger.Info(&quot;failed to fetch URL&quot;, zap.String(&quot;url&quot;, url), zap.Int(&quot;attempt&quot;, 3), zap.Duration(&quot;backoff&quot;, time.Second), ) sugar := logger.Sugar() sugar.Infow(&quot;failed to fetch URL&quot;, &quot;url&quot;, url, &quot;attempt&quot;, 3, &quot;backoff&quot;, time.Second, ) sugar.Infof(&quot;Failed to fetch URL: %s&quot;, url)&#125; 使用Json配置初始化123456789101112131415161718192021222324252627func main&#123; // 使用Config 获取Logger rawJSON := []byte(`&#123; &quot;level&quot;:&quot;debug&quot;, &quot;encoding&quot;:&quot;json&quot;, &quot;outputPaths&quot;: [&quot;stdout&quot;, &quot;server.log&quot;], &quot;errorOutputPaths&quot;: [&quot;stderr&quot;], &quot;initialFields&quot;:&#123;&quot;name&quot;:&quot;dj&quot;&#125;, &quot;encoderConfig&quot;: &#123; &quot;messageKey&quot;: &quot;message&quot;, &quot;levelKey&quot;: &quot;level&quot;, &quot;levelEncoder&quot;: &quot;lowercase&quot; &#125; &#125;`) var cfg zap.Config if err := json.Unmarshal(rawJSON, &amp;cfg); err != nil &#123; panic(err) &#125; logger, err := cfg.Build(zap.AddCaller()) if err != nil &#123; panic(err) &#125; defer logger.Sync() logger.Info(&quot;server start work successfully!&quot;)&#125; 自定义日志输出123456789101112131415func main() &#123; // 定义日志的输出 file, _ := os.Create(&quot;./test.log&quot;) // 多输出 writeSyncer := zapcore.NewMultiWriteSyncer(file, os.Stdout) encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder encoderConfig.EncodeLevel = zapcore.CapitalLevelEncoder encoder := zapcore.NewJSONEncoder(zap.NewProductionEncoderConfig()) core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger := zap.New(core) sugarLogger := logger.Sugar() sugarLogger.Info(&quot;test&quot;)&#125; 日志切割1go get -u github.com/natefinch/lumberjack 12345678910111213141516func main() &#123; // 日志切割 lumberJackLogger := &amp;lumberjack.Logger&#123; Filename: &quot;./test.log&quot;, // 日志文件的位置 MaxSize: 10, // 文件的最大大小 MB MaxBackups: 5, // 保留旧文件个数 MaxAge: 30, // 保留旧文件最大天数 Compress: false, // 是否压缩/归档旧文件 &#125; writeSyncer := zapcore.AddSync(lumberJackLogger) encoder := zapcore.NewJSONEncoder(zap.NewProductionEncoderConfig()) core := zapcore.NewCore(encoder, writeSyncer, zapcore.DebugLevel) logger := zap.New(core) sugarLogger := logger.Sugar() sugarLogger.Info(&quot;test lumberJackLogger&quot;)&#125; 输出调用栈123456789func main() &#123; // 输出调用堆栈 logger, _ := zap.NewProduction(zap.AddStacktrace(zapcore.WarnLevel)) defer logger.Sync() zap.ReplaceGlobals(logger) zap.L().Warn(&quot;test warn&quot;)&#125; 与标准日志库搭配使用1234567891011func main() &#123; // 与标准日志库搭配使用 logger := zap.NewExample() defer logger.Sync() undo := zap.RedirectStdLog(logger) log.Print(&quot;redirected standard library12&quot;) undo() log.Print(&quot;restored standard library&quot;)&#125;","categories":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"}]},{"title":"Jupyter Lab Docker 配置文件","slug":"language/python/env/jupyter-lab-docker","date":"2022-04-20T01:00:00.000Z","updated":"2023-12-25T05:56:09.176Z","comments":true,"path":"2022/04/20/language/python/env/jupyter-lab-docker/","link":"","permalink":"https://blog.ifan.host/2022/04/20/language/python/env/jupyter-lab-docker/","excerpt":"","text":"Dockerfile_base1234567891011121314151617181920FROM python:3.9WORKDIR /usr/src/appENV LANG C.UTF-8COPY requirements.txt .RUN rm -f /etc/apt/sources.list \\ &amp;&amp; echo &quot;deb https://mirrors.tencent.com/debian/ bullseye main non-free contrib&quot; &gt;&gt; /etc/apt/sources.list \\ &amp;&amp; echo &quot;deb https://mirrors.tencent.com/debian-security/ bullseye-security main&quot; &gt;&gt; /etc/apt/sources.list \\ &amp;&amp; echo &quot;deb https://mirrors.tencent.com/debian/ bullseye-updates main non-free contrib&quot; &gt;&gt; /etc/apt/sources.list \\ &amp;&amp; echo &quot;deb https://mirrors.tencent.com/debian/ bullseye-backports main non-free contrib&quot; &gt;&gt; /etc/apt/sources.list \\ &amp;&amp; ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\ &amp;&amp; echo &#x27;Asia/Shanghai&#x27; &gt; /etc/timezone \\ &amp;&amp; apt update -y \\ &amp;&amp; apt install -y wget curl git vim nodejs tree &amp;&amp; apt clean \\ &amp;&amp; mkdir /usr/src/app/workspaceRUN pip install -r requirements.txt -i https://pypi.doubanio.com/simple/ \\ &amp;&amp; jupyter lab build requirements.txt123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103alembic==1.7.7anyio==3.5.0argon2-cffi==21.3.0argon2-cffi-bindings==21.2.0asttokens==2.0.5async-generator==1.10attrs==21.4.0Babel==2.9.1backcall==0.2.0beautifulsoup4==4.10.0bleach==4.1.0certifi==2021.10.8certipy==0.1.3cffi==1.15.0charset-normalizer==2.0.12colorama==0.4.4cryptography==36.0.2debugpy==1.6.0decorator==5.1.1defusedxml==0.7.1entrypoints==0.4executing==0.8.3fastjsonschema==2.15.3gitdb==4.0.9GitPython==3.1.27greenlet==1.1.2idna==3.3ipykernel==6.9.2ipython==8.1.1ipython-genutils==0.2.0ipywidgets==7.7.0jedi==0.18.1Jinja2==3.1.0json5==0.9.6jsonschema==4.4.0jupyter==1.0.0jupyter-client==7.1.2jupyter-console==6.4.3jupyter-core==4.9.2jupyter-server==1.15.6jupyter-server-mathjax==0.2.5jupyter-telemetry==0.1.0jupyterhub==2.2.2jupyterlab==3.0.0jupyterlab-git==0.36.0jupyterlab-language-pack-zh-CN==3.3.post2jupyterlab-pygments==0.1.2jupyterlab-server==2.11.2jupyterlab-widgets==1.1.0Mako==1.2.0MarkupSafe==2.1.1matplotlib-inline==0.1.3mistune==0.8.4nbclassic==0.3.7nbclient==0.5.13nbconvert==6.4.4nbdime==3.1.1nbformat==5.2.0nest-asyncio==1.5.4notebook==6.4.10notebook-shim==0.1.0oauthlib==3.2.0packaging==21.3pamela==1.0.0pandocfilters==1.5.0parso==0.8.3pexpect==4.8.0pickleshare==0.7.5prometheus-client==0.13.1prompt-toolkit==3.0.28psutil==5.9.0ptyprocess==0.7.0pure-eval==0.2.2pycparser==2.21Pygments==2.11.2pyOpenSSL==22.0.0pyparsing==3.0.7pyrsistent==0.18.1python-dateutil==2.8.2python-json-logger==2.0.2pytz==2022.1pyzmq==22.3.0qtconsole==5.3.0QtPy==2.0.1requests==2.27.1ruamel.yaml==0.17.21ruamel.yaml.clib==0.2.6Send2Trash==1.8.0six==1.16.0smmap==5.0.0sniffio==1.2.0soupsieve==2.3.1SQLAlchemy==1.4.32stack-data==0.2.0terminado==0.13.3testpath==0.6.0tornado==6.1traitlets==5.1.1urllib3==1.26.9wcwidth==0.2.5webencodings==0.5.1websocket-client==1.3.1widgetsnbextension==3.6.0 jupyter_server_config.py1234567# Configuration file for jupyter-notebook.c.ServerApp.notebook_dir = &#x27;/usr/src/app/workspace&#x27;c.ServerApp.open_browser = Falsec.ServerApp.port = 8888c.ServerApp.token = &#x27;xxxxx&#x27;c.ServerApp.ip = &quot;*&quot; Dockerfile123456789101112131415FROM jupyterlab:base COPY requirements_ifan.txt .COPY jupyter_server_config.py /root/.jupyter/COPY vimrc /root/.vimrc SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]RUN python3 -m venv /usr/src/app/envs/ifan \\ &amp;&amp; source /usr/src/app/envs/ifan/bin/activate \\ &amp;&amp; pip install -r requirements_ifan.txt -i https://pypi.doubanio.com/simple/ \\ &amp;&amp; python -m ipykernel install --user --name ifan --display-name &quot;ifan&quot; \\ &amp;&amp; deactivateCMD [&quot;jupyter&quot;, &quot;lab&quot;, &quot;--allow-root&quot;] requirements_ifan.txt123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596aiomysql==0.1.0aliyun-python-sdk-core==2.13.36aliyun-python-sdk-kms==2.15.0anyio==3.5.0APScheduler==3.9.1asgiref==3.5.0async-timeout==4.0.2attrs==21.4.0Automat==20.2.0cachetools==5.0.0certifi==2021.10.8cffi==1.15.0charset-normalizer==2.0.12click==8.1.2constantly==15.1.0crcmod==1.7cryptography==36.0.2cssselect==1.1.0DBUtils==3.0.2Deprecated==1.2.13Django==4.0.4elasticsearch==5.3.0fastapi==0.75.2filelock==3.6.0Flask==2.1.1gevent==21.12.0google-auth==2.6.5greenlet==1.1.2h11==0.12.0hiredis==2.0.0httpcore==0.14.7httpx==0.22.0hyperlink==21.0.0idna==3.3importlib-metadata==4.11.3incremental==21.3.0itemadapter==0.5.0itemloaders==1.0.4itsdangerous==2.1.2Jinja2==3.1.1jmespath==0.10.0kubernetes==23.3.0lxml==4.8.0MarkupSafe==2.1.1motor==2.5.1numpy==1.22.3oauthlib==3.2.0oss2==2.15.0packaging==21.3pandas==1.4.2parsel==1.6.0Protego==0.2.1pyasn1==0.4.8pyasn1-modules==0.2.8pycparser==2.21pycryptodome==3.14.1pydantic==1.9.0PyDispatcher==2.0.5pymongo==3.12.3PyMySQL==1.0.2pyOpenSSL==22.0.0pyparsing==3.0.8python-dateutil==2.8.2python-snappy==0.6.1pytz==2022.1pytz-deprecation-shim==0.1.0.post0PyYAML==6.0queuelib==1.6.2redis==4.2.2requests==2.27.1requests-file==1.5.1requests-oauthlib==1.3.1rfc3986==1.5.0rsa==4.8Scrapy==2.6.1service-identity==21.1.0six==1.16.0sniffio==1.2.0SQLAlchemy==1.4.35sqlparse==0.4.2starlette==0.17.1tldextract==3.2.1tornado==6.1Twisted==22.4.0typing_extensions==4.2.0tzdata==2022.1tzlocal==4.2urllib3==1.26.9w3lib==1.22.0websocket-client==1.3.2Werkzeug==2.1.1wrapt==1.14.0zipp==3.8.0zope.event==4.5.0zope.interface==5.4.0ipykernel core.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556apiVersion: apps/v1kind: Deploymentmetadata: labels: k8s-app: jupyterlab-ifan name: jupyterlab-ifanspec: selector: matchLabels: k8s-app: jupyterlab-ifan template: metadata: labels: k8s-app: jupyterlab-ifan spec: containers: - image: jupyterlab:ifan imagePullPolicy: Always name: jupyterlab-ifan ports: - containerPort: 8888 protocol: TCP resources: limits: cpu: 500m memory: 500Mi requests: cpu: 100m memory: 300Mi volumeMounts: - mountPath: /usr/src/app/workspace/ subPath: jupyter-ifan/workspace/ name: jupyterlab-pvc imagePullSecrets: - name: rd volumes: - name: jupyterlab-pvc persistentVolumeClaim: claimName: jupyter-lab-ifan---apiVersion: v1kind: Servicemetadata: name: jupyterlab-ifanspec: ports: - nodePort: 32004 port: 80 protocol: TCP targetPort: 8888 selector: k8s-app: jupyterlab-ifan sessionAffinity: None type: NodePort start.sh1234567891011#!/bin/bashDOCKER_HUB_IMAGE=jupyterlab:ifandocker stop jupyterlab &amp;&amp; docker rm jupyterlabdocker build . -t jupyterlab:base -f Dockerfile_basedocker build . -t jupyterlab -f Dockerfiledocker tag jupyterlab $DOCKER_HUB_IMAGEdocker push $DOCKER_HUB_IMAGE#docker run -d --name jupyterlab -p 8888:8888 -v /workspace/jupyter:/usr/src/app/workspace jupyterlab#docker logs -f jupyterlab","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/categories/Python/"},{"name":"jupyterlab","slug":"Python/jupyterlab","permalink":"https://blog.ifan.host/categories/Python/jupyterlab/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/tags/Python/"},{"name":"jupyterlab","slug":"jupyterlab","permalink":"https://blog.ifan.host/tags/jupyterlab/"}]},{"title":"NodeJS 版本管理 nvm 安装","slug":"language/javascript/env/nvm-install","date":"2022-04-09T01:38:00.000Z","updated":"2023-12-25T05:56:09.175Z","comments":true,"path":"2022/04/09/language/javascript/env/nvm-install/","link":"","permalink":"https://blog.ifan.host/2022/04/09/language/javascript/env/nvm-install/","excerpt":"","text":"安装GITHUB 官方提供的安装命令 1curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash 自定义安装的路径，他会自己在后面加上nvm 1export XDG_CONFIG_HOME=/opt 修改安装脚本，将下面的地址修改为镜像的地址 169 NVM_SOURCE_URL=&quot;https://gitclone.com/github.com/$&#123;NVM_GITHUB_REPO&#125;.git&quot; 安装完毕后添加环境变量，默认是加上的 1234# NODEexport NVM_DIR=&quot;$HOME/.nvm&quot;[ -s &quot;$NVM_DIR/nvm.sh&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/nvm.sh&quot; # This loads nvm[ -s &quot;$NVM_DIR/bash_completion&quot; ] &amp;&amp; \\. &quot;$NVM_DIR/bash_completion&quot; # This loads nvm bash_completion 命令1234567891011121314151617181920nvm install stable ## 安装最新稳定版 nodenvm install &lt;version&gt; ## 安装指定版本，可模糊安装，如：安装v4.4.0，既可nvm install v4.4.0，又可nvm install 4.4nvm uninstall &lt;version&gt; ## 删除已安装的指定版本，语法与install类似nvm use &lt;version&gt; ## 切换使用指定的版本nodenvm ls ## 列出所有安装的版本nvm ls-remote ## 列出所有远程服务器的版本（官方node version list）nvm current ## 显示当前的版本nvm alias &lt;name&gt; &lt;version&gt; ## 给不同的版本号添加别名nvm alias default v16 ## 设置默认版本nvm unalias &lt;name&gt; ## 删除已定义的别名nvm reinstall-packages &lt;version&gt; ## 在当前版本 node 环境下，重新全局安装指定版本号的 npm 包","categories":[{"name":"NodeJS","slug":"NodeJS","permalink":"https://blog.ifan.host/categories/NodeJS/"},{"name":"nvm","slug":"NodeJS/nvm","permalink":"https://blog.ifan.host/categories/NodeJS/nvm/"}],"tags":[{"name":"NodeJS","slug":"NodeJS","permalink":"https://blog.ifan.host/tags/NodeJS/"},{"name":"nvm","slug":"nvm","permalink":"https://blog.ifan.host/tags/nvm/"}]},{"title":"JupyterLab 服务器安装使用Nginx做代理","slug":"language/python/env/jupyter-lab-install","date":"2022-04-09T01:15:00.000Z","updated":"2023-12-25T05:56:09.176Z","comments":true,"path":"2022/04/09/language/python/env/jupyter-lab-install/","link":"","permalink":"https://blog.ifan.host/2022/04/09/language/python/env/jupyter-lab-install/","excerpt":"","text":"1pip install jupyterlab 生成配置文件 1jupyter lab --generate-config 修改配置文件 1234567891011# 工作目录c.NotebookApp.notebook_dir = &#x27;/opt/jupyterhub&#x27;# 不打开浏览器c.NotebookApp.open_browser = False# 允许所有IP访问c.ServerApp.allow_origin = &#x27;*&#x27;c.ServerApp.allow_remote_access = Truec.ServerApp.ip = &#x27;0.0.0.0&#x27;c.ServerApp.local_hostnames = [&#x27;*&#x27;]# 设置Tokenc.ServerApp.token = &#x27;&#x27; Nginx配置 12345678910111213141516171819202122232425262728293031server &#123; listen 80; listen 443 ssl http2; server_name jupyter.ifan.host; ssl_certificate /opt/openresty/nginx/cert/ifan/fullchain.cer; ssl_certificate_key /opt/openresty/nginx/cert/ifan/ifan.host.key; ssl_protocols TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; access_log /var/log/openresty/nginx/logs/jupyter.access.log; error_log /var/log/openresty/nginx/logs/jupyter.error.log; location / &#123; proxy_http_version 1.1; proxy_set_header Accept-Encoding gzip; # WebSocket support proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &quot;upgrade&quot;; proxy_read_timeout 120s; proxy_next_upstream error; proxy_set_header Host $http_host; proxy_redirect off; proxy_buffering off; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Real-IP $remote_addr; proxy_connect_timeout 3s; proxy_pass http://localhost:8888; &#125;&#125; 设置多环境 123jupyter kernelspec list # 查看所有的jupyter核pip install ipykernelpython -m ipykernel install --name ifan-env 设置中文 1pip install jupyterlab-language-pack-zh-CN==3.3.post2","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/categories/Python/"},{"name":"Jupyter","slug":"Python/Jupyter","permalink":"https://blog.ifan.host/categories/Python/Jupyter/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/tags/Python/"},{"name":"Jupyter","slug":"Jupyter","permalink":"https://blog.ifan.host/tags/Jupyter/"}]},{"title":"Python 环境管理 Pyenv 安装","slug":"language/python/env/pyenv-install","date":"2022-04-09T01:15:00.000Z","updated":"2023-12-25T05:56:09.176Z","comments":true,"path":"2022/04/09/language/python/env/pyenv-install/","link":"","permalink":"https://blog.ifan.host/2022/04/09/language/python/env/pyenv-install/","excerpt":"","text":"pyenv1. 安装Centos 7 的， 其他平台请参考网址 1sudo yum install -y zlib-devel bzip2 bzip2-devel readline-devel sqlite sqlite-devel openssl-devel xz xz-devel libffi-devel findutils git gcc 1.1 自动安装1curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 1.2 文件信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#!/usr/bin/env bashset -e[ -n &quot;$PYENV_DEBUG&quot; ] &amp;&amp; set -xif [ -z &quot;$PYENV_ROOT&quot; ]; then PYENV_ROOT=&quot;$&#123;HOME&#125;/.pyenv&quot;ficolorize() &#123; if [ -t 1 ]; then printf &quot;\\e[%sm%s\\e[m&quot; &quot;$1&quot; &quot;$2&quot; else echo -n &quot;$2&quot; fi&#125;# Checks for `.pyenv` file, and suggests to remove it for installingif [ -d &quot;$&#123;PYENV_ROOT&#125;&quot; ]; then &#123; echo colorize 1 &quot;WARNING&quot; echo &quot;: Can not proceed with installation. Kindly remove the &#x27;$&#123;PYENV_ROOT&#125;&#x27; directory first.&quot; echo &#125; &gt;&amp;2 exit 1fishell=&quot;$1&quot;if [ -z &quot;$shell&quot; ]; then shell=&quot;$(ps c -p &quot;$PPID&quot; -o &#x27;ucomm=&#x27; 2&gt;/dev/null || true)&quot; shell=&quot;$&#123;shell##-&#125;&quot; shell=&quot;$&#123;shell%% *&#125;&quot; shell=&quot;$(basename &quot;$&#123;shell:-$SHELL&#125;&quot;)&quot;fifailed_checkout() &#123; echo &quot;Failed to git clone $1&quot; exit -1&#125;checkout() &#123; [ -d &quot;$2&quot; ] || git clone --depth 1 &quot;$1&quot; &quot;$2&quot; || failed_checkout &quot;$1&quot;&#125;if ! command -v git 1&gt;/dev/null 2&gt;&amp;1; then echo &quot;pyenv: Git is not installed, can&#x27;t continue.&quot; &gt;&amp;2 exit 1fiif [ -n &quot;$&#123;USE_GIT_URI&#125;&quot; ]; then GITHUB=&quot;git://github.com&quot;else GITHUB=&quot;https://github.com&quot;ficheckout &quot;$&#123;GITHUB&#125;/pyenv/pyenv.git&quot; &quot;$&#123;PYENV_ROOT&#125;&quot;checkout &quot;$&#123;GITHUB&#125;/pyenv/pyenv-doctor.git&quot; &quot;$&#123;PYENV_ROOT&#125;/plugins/pyenv-doctor&quot;checkout &quot;$&#123;GITHUB&#125;/pyenv/pyenv-installer.git&quot; &quot;$&#123;PYENV_ROOT&#125;/plugins/pyenv-installer&quot;checkout &quot;$&#123;GITHUB&#125;/pyenv/pyenv-update.git&quot; &quot;$&#123;PYENV_ROOT&#125;/plugins/pyenv-update&quot;checkout &quot;$&#123;GITHUB&#125;/pyenv/pyenv-virtualenv.git&quot; &quot;$&#123;PYENV_ROOT&#125;/plugins/pyenv-virtualenv&quot;checkout &quot;$&#123;GITHUB&#125;/pyenv/pyenv-which-ext.git&quot; &quot;$&#123;PYENV_ROOT&#125;/plugins/pyenv-which-ext&quot;if ! command -v pyenv 1&gt;/dev/null; then &#123; echo colorize 1 &quot;WARNING&quot; echo &quot;: seems you still have not added &#x27;pyenv&#x27; to the load path.&quot; echo &#125; &gt;&amp;2 case &quot;$shell&quot; in bash ) profile=&quot;~/.bashrc&quot; ;; zsh ) profile=&quot;~/.zshrc&quot; ;; ksh ) profile=&quot;~/.profile&quot; ;; fish ) profile=&quot;~/.config/fish/config.fish&quot; ;; * ) profile=&quot;your profile&quot; ;; esac &#123; echo &quot;# Load pyenv automatically by adding&quot; echo &quot;# the following to $&#123;profile&#125;:&quot; echo case &quot;$shell&quot; in fish ) echo &quot;set -x PATH \\&quot;$&#123;PYENV_ROOT&#125;/bin\\&quot; \\$PATH&quot; echo &#x27;status --is-interactive; and . (pyenv init -|psub)&#x27; echo &#x27;status --is-interactive; and . (pyenv virtualenv-init -|psub)&#x27; ;; * ) echo &quot;export PATH=\\&quot;$&#123;PYENV_ROOT&#125;/bin:\\$PATH\\&quot;&quot; echo &quot;eval \\&quot;\\$(pyenv init -)\\&quot;&quot; echo &quot;eval \\&quot;\\$(pyenv virtualenv-init -)\\&quot;&quot; ;; esac &#125; &gt;&amp;2fi 因为网络的限制，可以将GitHub地址设置为镜像的地址 1234# 设置安装的位置export PYENV_ROOT=/opt/pyenv需要在文件内部修改 在46行GITHUB=&quot;https://gitclone.com/github.com&quot; 2. 添加到 ~/.bashrc123echo &#x27;export PATH=~/.pyenv/bin:$PATH&#x27; &gt;&gt; ~/.bashrcecho &#x27;export PYENV_ROOT=~/.pyenv&#x27; &gt;&gt; ~/.bashrcecho &#x27;eval &quot;$(pyenv init -)&quot;&#x27; &gt;&gt; ~/.bashrc 3. 一些常用命令1234567891011121314151617181920212223242526272829# 列出可以下载的python版本pyenv install -l# 下载 指定的python 版本pyenv install 3.6.8# 刷新本地环境pyenv rehash # 卸载指定版本的pythonpyenv uninstall 3.6.8# 指定shell使用的python版本pyenv shell 3.6.8# 创建虚拟环境pyenv virtualenv 版本号 虚拟环境名称 # 删除虚拟环境pyenv uninstall 虚拟环境名称# 列出创建的虚拟环境pyenv virtualenvs# 激活虚拟环境pyenv activate 虚拟环境名称# 退出当前的虚拟环境pynev deactivate 4. 提高下载速度1234# 去官方下载python版本wget https://www.python.org/ftp/python/3.6.8/Python-3.6.8.tar.xz# 将其安装在pyenv下的cache目录下面 （不存在就创建）pyenv install 3.6.8","categories":[{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/categories/Python/"},{"name":"pyenv","slug":"Python/pyenv","permalink":"https://blog.ifan.host/categories/Python/pyenv/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/tags/Python/"},{"name":"pyenv","slug":"pyenv","permalink":"https://blog.ifan.host/tags/pyenv/"}]},{"title":"Mysql8 Centos8 编译安装","slug":"database/mysql/mysql8-compile-install","date":"2022-04-08T10:00:00.000Z","updated":"2023-12-25T05:56:09.166Z","comments":true,"path":"2022/04/08/database/mysql/mysql8-compile-install/","link":"","permalink":"https://blog.ifan.host/2022/04/08/database/mysql/mysql8-compile-install/","excerpt":"","text":"mysql下载地址 Boost下载地址 rpcsvc-proto 安装依赖1sudo yum -y install wget tar make cmake gcc gcc-c++ ncurses ncurses-devel libaio-devel openssl openssl-devel libtirpc-devel bison 安装rpcsvc123tar -Jxf rpcsvc-proto-1.4.2.tar.xz &amp;&amp; cd rpcsvc-proto-1.4.2./configuremake &amp;&amp; make install 解压boost1tar -zvxf boost_1_73_0.tar.gz 安装Mysql123456cmake .. -DCMAKE_INSTALL_PREFIX=/opt/mysql -DINSTALL_DATADIR=/opt/mysql/data -DDEFAULT_CHARSET=utf8 \\ -DDEFAULT_COLLATION=utf8_general_ci -DWITH_SSL=yes -DWITH_INNOBASE_STORAGE_ENGINE=1 -DWITH_EMBEDDED_SERVER=1 \\ -DFORCE_INSOURCE_BUILD=1 -DWITH_MYISAM_STORAGE_ENGINE=1 -DENABLED_LOCAL_INFILE=1 -DEXTRA_CHARSETS=all \\ -DWITH_BOOST=/opt/boost_1_73_0 # boost解压的路径gmake &amp;&amp; gmake install 初始化数据库12# 这个阶段完成之后会有一个密码，记得保存一下 ./bin/mysqld --user=ifan --initialize --basedir=/opt/mysql/mysql --datadir=/opt/mysql/data 创建my.cnf12345ln -s /opt/mysql/my.cnf /etc/my.cnf# 把Mysql加入系统启动cp /usr/local/mysql/support-files/mysql.server /etc/rc.d/init.d/mysqld # 增加执行权限chmod 755 /etc/init.d/mysqld 修改启动文件，并加入开机启动12345678# 编辑vi /etc/rc.d/init.d/mysqld# MySQL程序安装路径basedir=/usr/local/mysql # MySQl数据库存放目录datadir=/data/mysql # 加入开机启动chkconfig mysqld on 12345678910111213141516171819202122[client]port=3306socket=/tmp/mysql.sock[mysqld]port=3306user = mysqlsocket=/tmp/mysql.socktmpdir = /tmpkey_buffer_size=16Mmax_allowed_packet=128Mdefault_authentication_plugin=mysql_native_passwordopen_files_limit = 60000server-id = 1character-set-server = utf8max_connections = 1000max_connect_errors = 100000interactive_timeout = 86400wait_timeout = 86400sync_binlog=0back_log=100default-storage-engine = InnoDBlog_slave_updates = 1 更新root的密码12345mysql -uroot -p&gt; 输入之前获得的密码ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;new-root-password&#x27;;flush privileges; root远程访问123456# 设置远程访问ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;root-password&#x27;;use mysql;update user set host=&#x27;%&#x27; where user =&#x27;root&#x27;;# 授权GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; WITH GRANT OPTION; 创建数据库并授权1234567891011# 创建数据库CREATE DATABASE IF NOT EXISTS ifandb default charset utf8mb4 COLLATE utf8_general_ci;# 创建用户CREATE USER &#x27;ifan&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;ifan&#x27;;# 加密方式为mysql_native_passwordCREATE USER &#x27;ifan&#x27;@&#x27;127.0.0.1&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;ifan&#x27;;# 授权用户 ifan 对数据库 ifandb 具有全部操作权限GRANT ALL privileges on ifandb.* to &#x27;ifan&#x27;@&#x27;localhost&#x27;;GRANT ALL privileges on ifandb.* to &#x27;ifan&#x27;@&#x27;127.0.0.1&#x27;;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Mysql8 Centos8 使用编译好的包安装","slug":"database/mysql/mysql8-no-compile-install","date":"2022-04-08T10:00:00.000Z","updated":"2023-12-25T05:56:09.166Z","comments":true,"path":"2022/04/08/database/mysql/mysql8-no-compile-install/","link":"","permalink":"https://blog.ifan.host/2022/04/08/database/mysql/mysql8-no-compile-install/","excerpt":"","text":"mysql8安装下载编译好的安装包清华镜像站下载安装包 下载安装包1wget https://mirrors.tuna.tsinghua.edu.cn/mysql/downloads/MySQL-8.0/mysql-8.0.11-el7-x86_64.tar.gz 创建新用户123useradd mysqltar xvfz mysql-8.0.11-el7-x86_64.tar.gzcd mysql-8.0.11-el7-x86_64 初始化数据库1mysqld --initialize --user=mysql 创建 my.cnf 文件123456789[mysqld]datadir = /usr/local/mysql/datalog-error = /usr/local/mysql/data/error.logpid-file = /usr/local/mysql/data/mysql.piduser = mysqltmpdir = /tmp[client]socket=/var/lib/mysql/mysql.sock 配置服务12345cp support-files/mysql.server /etc/init.d/mysqldchmod a+x /etc/init.d/mysqldchkconfig --add mysqldchkconfig mysqld onservice mysqld start 初始化表和权限1mysql_secure_installation 修改 root 密码12alter user &#x27;root&#x27;@&#x27;localhost&#x27; identified by &#x27;newpassword&#x27;;flush privileges;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"}]},{"title":"Lua 关于时间的相关的方法","slug":"language/lua/lua-time-test","date":"2022-04-03T02:28:00.000Z","updated":"2023-12-25T05:56:09.175Z","comments":true,"path":"2022/04/03/language/lua/lua-time-test/","link":"","permalink":"https://blog.ifan.host/2022/04/03/language/lua/lua-time-test/","excerpt":"","text":"关于时间的相关的方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798function print_r ( t ) local print_r_cache=&#123;&#125; local function sub_print_r(t,indent) if (print_r_cache[tostring(t)]) then print(indent..&quot;*&quot;..tostring(t)) else print_r_cache[tostring(t)]=true if (type(t)==&quot;table&quot;) then for pos,val in pairs(t) do if (type(val)==&quot;table&quot;) then print(indent..&quot;[&quot;..pos..&quot;] =&gt; &quot;..tostring(t)..&quot; &#123;&quot;) sub_print_r(val,indent..string.rep(&quot; &quot;,string.len(pos)+8)) print(indent..string.rep(&quot; &quot;,string.len(pos)+6)..&quot;&#125;&quot;) elseif (type(val)==&quot;string&quot;) then print(indent..&quot;[&quot;..pos..&#x27;] =&gt; &quot;&#x27;..val..&#x27;&quot;&#x27;) else print(indent..&quot;[&quot;..pos..&quot;] =&gt; &quot;..tostring(val)) end end else print(indent..tostring(t)) end end end if (type(t)==&quot;table&quot;) then print(tostring(t)..&quot; &#123;&quot;) sub_print_r(t,&quot; &quot;) print(&quot;&#125;&quot;) else sub_print_r(t,&quot; &quot;) end print()endlocal nowTime = os.time()print(&quot;当前时间戳:&quot;..nowTime)local nowDate = os.date(&quot;%Y-%m-%d %H:%M:%S&quot;, nowTime)print(&quot;转换成时间:&quot;..nowDate)local nowTable = os.date(&quot;*t&quot;, nowTime)print(&quot;时间戳转为table:&quot;)print_r(nowTable)print(&quot;table转为时间戳:&quot;.. os.time(nowTable))print(&quot;自定义时间:&quot;..os.time(&#123;year=2020, month=1, day=1,hour=0,min=0,sec=0&#125;))print(&quot;获取时间戳对应的日期&quot;)--获取时间戳对应的天数（天数规则不是自然天，是(hour)4点前当前一天算）function GetDayKeyByUnixTime(unixTime,hour) if hour == nil then hour = 0 end local retStr = os.date(&quot;%Y-%m-%d %H:%M:%S&quot;,unixTime) local time = unixTime local data = os.date(&quot;*t&quot;,time) --dump(data) --(hour)4点前按前一天算 if data.hour &lt; hour then time = time - 24*60*60 end local data2 = os.date(&quot;*t&quot;,time) --dump(data2) data2.hour = 0 data2.min = 0 data2.sec = 0 local time2 = os.time(data2) local dayKey = os.date(&quot;%Y%m%d&quot;,time2) local timeBase = time2 --天数key，日期格式字符串，天数key 0点的时间戳 return dayKey,retStr,timeBaseendlocal time1 = os.time(&#123;year=2020,month=1,day=1&#125;)local time2 = os.time(&#123;year=2020,month=1,day=10&#125;)local dayKey,str = GetDayKeyByUnixTime(time1,4)print(&quot;dayKey = &quot;..dayKey..&quot; str = &quot;..str)dayKey,str = GetDayKeyByUnixTime(time2,4)print(&quot;dayKey = &quot;..dayKey..&quot; str = &quot;..str)--两个时间的天数差 --时间戳1 时间戳2 多少点开始算第二天function NumberOfDaysInterval(unixTime1,unixTime2,dayFlagHour) if dayFlagHour == nil then dayFlagHour = 0 end local key1,str1,time1 = GetDayKeyByUnixTime(unixTime1,dayFlagHour) local key2,str2,time2 = GetDayKeyByUnixTime(unixTime2,dayFlagHour) local sub = math.abs(time2 - time1)/(24*60*60) print(str1..&quot; 与 &quot;..str2..&quot;相差的天数：&quot;..sub) return subendlocal sub = NumberOfDaysInterval(time1,time2,0)print(sub) 12345678910111213141516171819202122当前时间戳:1611828502转换成时间:2021-01-28 18:08:22时间戳转为table:table: 0x13c89a0 &#123; [hour] =&gt; 18 [min] =&gt; 8 [wday] =&gt; 5 [day] =&gt; 28 [month] =&gt; 1 [year] =&gt; 2021 [sec] =&gt; 22 [yday] =&gt; 28 [isdst] =&gt; false&#125;table转为时间戳:1611828502自定义时间:1577808000获取时间戳对应的日期dayKey = 20200101 str = 2020-01-01 12:00:00dayKey = 20200110 str = 2020-01-10 12:00:002020-01-01 12:00:00 与 2020-01-10 12:00:00相差的天数：99","categories":[{"name":"Lua","slug":"Lua","permalink":"https://blog.ifan.host/categories/Lua/"}],"tags":[{"name":"Lua","slug":"Lua","permalink":"https://blog.ifan.host/tags/Lua/"}]},{"title":"Nginx Log日志切割","slug":"web/nginx/nginx-logrotate","date":"2022-04-03T02:25:00.000Z","updated":"2023-12-25T05:56:09.188Z","comments":true,"path":"2022/04/03/web/nginx/nginx-logrotate/","link":"","permalink":"https://blog.ifan.host/2022/04/03/web/nginx/nginx-logrotate/","excerpt":"","text":"Log日志切割logrotate123456789101112131415vim /etc/logrotate.d/nginx/usr/local/nginx/logs/*.log &#123; daily # 指定转储周期为每天 rotate 7 # 保留7份 missingok # 如果日志丢失，不报错继续滚动下一个日志 notifempty # 当日志文件为空时，不进行轮转 dateext # 使用当期日期作为命名格式 sharedscripts # 运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本 postrotate # 在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行 if [ -f /usr/local/nginx/logs/nginx.pid ]; then kill -USR1 `cat /usr/local/nginx/logs/nginx.pid` # ## 向 Nginx 主进程发送 USR1 信号。USR1 信号是重新打开日志文件 fi endscript&#125; Nginx1234567891011log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;map $time_iso8601 $logdate &#123; &#x27;~^(?&lt;ymd&gt;\\d&#123;4&#125;-\\d&#123;2&#125;-\\d&#123;2&#125;)&#x27; $ymd; default &#x27;date-not-found&#x27;;&#125;access_log logs/access-$logdate.log main;open_log_file_cache max=10; shell脚本12345678910111213141516171819#!/bin/bashyesterday=`date -d &quot;-1 days&quot; +&#x27;%Y%m%d&#x27;`cd `dirname $0`basedir=`pwd`logdir=&quot;$&#123;basedir&#125;/bak&quot;bindir=&quot;$&#123;basedir%/*&#125;/sbin&quot;mkdir -p $&#123;logdir&#125;for log in `ls *.log 2&gt;/dev/null`do mv $&#123;log&#125; $&#123;logdir&#125;/$&#123;log&#125;.$&#123;yesterday&#125;.bak # gzip $&#123;logdir&#125;/$&#123;log&#125;.$&#123;yesterday&#125;done$&#123;bindir&#125;/nginx -s reloadcd $&#123;logdir&#125;find . -type f -name &quot;*.bak&quot; -mtime +7 | xargs rm -f 日志压缩12345678910#!/usr/bin/sh#根据系统/服务/日志保留天数三个参数压缩日志#usage: sh clearlog.sh sysname appname keepdayssysName=$1appName=$2keepDay=$3logDir=/var/log/$&#123;sysName&#125;/$&#123;appName&#125;logFile=$&#123;appName&#125;.*[0-9][0-9].logcd $&#123;logDir&#125;find ./ -name &quot;$&#123;logFile&#125;&quot; -mtime -$&#123;keepDay&#125; -exec gzip &#123;&#125; \\;","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"Nginx","slug":"WEB/Nginx","permalink":"https://blog.ifan.host/categories/WEB/Nginx/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.ifan.host/tags/Nginx/"}]},{"title":"Lua 字符串拼接方法测试","slug":"language/lua/lua-string-test","date":"2022-04-03T02:21:00.000Z","updated":"2023-12-25T05:56:09.175Z","comments":true,"path":"2022/04/03/language/lua/lua-string-test/","link":"","permalink":"https://blog.ifan.host/2022/04/03/language/lua/lua-string-test/","excerpt":"","text":"字符串拼接方法测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function test_spot(count, str) local result = &quot;&quot; for i=0,count do result = result..str endendfunction test_table_concat(count, str) local tbl = &#123;&#125; for i=0,count do table.insert(tbl, str) end table.concat(tbl)endprint(&quot;单个字符串拼接多次&quot;)local startTime = os.clock()test_spot(10000, &quot;123&quot;)local endTime = os.clock()print(string.format(&quot;spot use time =&gt; %.4f&quot;, endTime - startTime))startTime = os.clock()test_table_concat(10000, &quot;123&quot;)endTime = os.clock()print(string.format(&quot;table_concat use time =&gt; %.4f&quot;, endTime - startTime))function test_little_spot(count, str) for i=0,count do local tmp = str..str..str..str..str endendfunction test_little_table_concat(count, str) for i=0,count do local tbl = &#123;&#125; for i=0,5 do table.insert(tbl, str) end table.concat(tbl) endendprint(&quot;小字符串拼接多次&quot;)startTime = os.clock()test_little_spot(10000, &quot;123&quot;)endTime = os.clock()print(string.format(&quot;little_spot use time =&gt; %.4f&quot;, endTime - startTime))startTime = os.clock()test_little_table_concat(10000, &quot;123&quot;)endTime = os.clock()print(string.format(&quot;little_table_concat use time =&gt; %.4f&quot;, endTime - startTime)) 123456单个字符串拼接多次spot use time =&gt; 0.0300table_concat use time =&gt; 0.0000小字符串拼接多次little_spot use time =&gt; 0.0000little_table_concat use time =&gt; 0.0200","categories":[{"name":"Lua","slug":"Lua","permalink":"https://blog.ifan.host/categories/Lua/"}],"tags":[{"name":"Lua","slug":"Lua","permalink":"https://blog.ifan.host/tags/Lua/"}]},{"title":"Nginx 将网站升级到https","slug":"web/nginx/nginx-https","date":"2022-04-03T02:20:00.000Z","updated":"2023-12-25T05:56:09.187Z","comments":true,"path":"2022/04/03/web/nginx/nginx-https/","link":"","permalink":"https://blog.ifan.host/2022/04/03/web/nginx/nginx-https/","excerpt":"","text":"添加证书使用httpsLetsencrypt 生成./letsencrypt-auto 之前使用该命令生成，现在不管用了 安装epel 1yum install epel-release 安装snapd 1yum install snapd 启用 snapd.socket 1systemctl enable --now snapd.socket 创建软连接 1ln -s /var/lib/snapd/snap /snap 升级snapd到最新版本 123snap install coresnap refresh core 6.1 卸载之前存在的 certbot 1yum remove certbot 6.2 删除相关文件 1rm /usr/local/bin/certbot-auto 6.3 删除 certbot 附加软件包 1rm -rf /opt/eff.org/certbot 安装 certbot 1snap install --classic certbot 创建软连接 1ln -s /snap/bin/certbot /usr/bin/certbot 生成证书 要保证Nginx正在运行 80端口 1234certbot certonly --preferred-challenges http \\ --nginx \\ # 指定服务器为Nginx --nginx-ctl /apps/openresty/nginx/sbin/nginx \\ # 指定Nginx命令的位置 --nginx-server-root=/apps/openresty/nginx/conf # 指定Nginx配置文件位置 /etc/letsencrypt/live 生成位置 这里会提示不支持 openresty，没事 配置Nginx 1234567891011121314151617181920212223server &#123; listen 80; server_name ifan.tutulis.com; # http升级到https return 301 https://$server_name$request_uri; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125;server &#123; listen 443 ssl; # 新版本是在这里加ssl，以前的是 ssl on; server_name ifan.tutulis.com; ssl_certificate /etc/letsencrypt/live/ifan.tutulis.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/ifan.tutulis.com/privkey.pem; index index.html index.htm; location / &#123; root /docs/blog/_site/; &#125;&#125; 添加定时任务，自动更新证书 123crontab -e 0 0 * * * certbot renew &gt;&gt; /var/logs/update_https.log 自己生成 参数 意义 C 国家 ST 州 L 本地名称 O 组织名称 OU 组织单元名称 CN 命令名称 1234567891011121314151617181920212223echo &#x27;创建私钥&#x27;openssl genrsa -out ca-key.pem 2048echo &#x27;创建csr证书请求&#x27;openssl req -new -key ca-key.pem -out ca-req.csr -subj &quot;/C=CN/ST=BJ/L=BJ/O=SJ/OU=shangjian/CN=CA&quot;echo &#x27;生成crt证书&#x27;openssl x509 -req -in ca-req.csr -out ca-cert.pem -signkey ca-key.pem -days 3650echo &#x27;服务器端证书&#x27;echo &#x27;创建服务器端私钥&#x27;openssl genrsa -out server-key.pem 2048echo &#x27;创建csr证书&#x27;openssl req -new -out server-req.csr -key server-key.pem -subj &quot;/C=CN/ST=BJ/L=BJ/O=SJ/OU=shangjian/CN=*.ifan.tutulis.com&quot;echo &#x27;生成crt证书&#x27;openssl x509 -req -in server-req.csr -out server-cert.pem -signkey server-key.pem -CA ca-cert.pem -CAkey ca-key.pem -CAcreateserial -days 3650echo &#x27;确认证书&#x27;openssl verify -CAfile ca-cert.pem server-cert.pemecho &#x27;创建客户端私钥&#x27;openssl genrsa -out client-key.pem 2048echo &#x27;创建csr证书&#x27;openssl req -new -out client-req.csr -key client-key.pem -subj &quot;/C=CN/ST=BJ/L=BJ/O=SJ/OU=shangjian/CN=Client&quot;echo &#x27;生成crt证书&#x27;openssl x509 -req -in client-req.csr -out client-cert.pem -signkey client-key.pem -CA ca-cert.pem -CAkey ca-key.pem -CAcreateserial -days 3650echo &#x27;确认证书&#x27;openssl verify -CAfile ca-cert.pem client-cert.pem 单向认证12345678910111213141516server &#123; listen 443 ssl; server_name ifan.tutulis.com; # 刚刚生成的服务器端公钥和私钥文件 ssl_certificate /etc/nginx/ssl/server-cert.pem; ssl_certificate_key /etc/nginx/ssl/server-key.pem; # 据官方文档所述，cache中的1m可以存放4000个session。 ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; &#125; 双向认证123456789101112131415161718server &#123; listen 443 ssl; server_name ifan.tutulis.com; ssl_certificate /etc/nginx/ssl/server-cert.pem; ssl_certificate_key /etc/nginx/ssl/server-key.pem; ssl_verify_client on; #ssl_verify_depth 2; ssl_client_certificate /etc/nginx/ssl/ca-cert.pem; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; &#125; 1234# 使用curl测试链接curl -k --cert client-cert.pem --key client-key.pem https://ifan.tutulis.com# 忽略证书curl -k -v https://ifan.tutulis.com HTTPS 加密过程 客户端生成一个随机数 random-client，传到服务器端（Say Hello) 服务器端生成一个随机数 random-server，和着公钥，一起回馈给客户端（I got it) 客户端收到的东西原封不动，加上 premaster secret（通过 random-client、random-server 经过一定算法生成的东西），再一次送给服务器端，这次传过去的东西会使用公钥加密 服务器端先使用私钥解密，拿到 premaster secret，此时客户端和服务器端都拥有了三个要素：random-client、random-server 和 premaster secret 此时安全通道已经建立，以后的交流都会校检上面的三个要素通过算法算出的 session key 参考Nginx配置Nginx配置","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"Nginx","slug":"WEB/Nginx","permalink":"https://blog.ifan.host/categories/WEB/Nginx/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.ifan.host/tags/Nginx/"}]},{"title":"OpenResty 安装并申请https证书","slug":"web/nginx/openresty-install","date":"2022-04-03T02:15:00.000Z","updated":"2023-12-25T05:56:09.188Z","comments":true,"path":"2022/04/03/web/nginx/openresty-install/","link":"","permalink":"https://blog.ifan.host/2022/04/03/web/nginx/openresty-install/","excerpt":"","text":"安装Openresty 下载地址 12wget https://openresty.org/download/openresty-1.19.9.1.tar.gztar -zvxf openresty-1.19.9.1.tar.gz 下载依赖 1yum install pcre-devel openssl-devel gcc curl 编译安装 1234567891011121314151617181920212223 ./configure --prefix=/opt/openresty \\ --user=nginx \\ --group=nginx \\ --with-luajit \\ --with-threads \\ --with-file-aio \\ --with-pcre \\ --with-pcre-jit \\ --with-http_v2_module \\ --with-http_ssl_module \\ --with-http_realip_module \\ --with-http_gunzip_module \\ --with-http_gzip_static_module \\ --with-http_secure_link_module \\ --with-http_degradation_module \\ --with-http_iconv_module \\ --with-http_stub_status_module \\ --without-lua_resty_memcached \\ --without-http_memcached_module \\ --with-http_postgres_module \\ --with-mail \\ --with-stream gmake &amp;&amp; gmake install 注册服务 1vim /etc/systemd/system/nginx.service 123456789101112131415161718[Unit]Description=Nginx(OpenResty ) - high performance web serverAfter=network-online.target remote-fs.target nss-lookup.targetWants=network-online.target[Service]User=rootGroup=rootType=forkingPIDFile=/opt/openresty/nginx/logs/nginx.pidExecStartPre=/opt/openresty/nginx/sbin/nginx -t -c /opt/openresty/nginx/conf/nginx.confExecStart=/opt/openresty/nginx/sbin/nginx -c /opt/openresty/nginx/conf/nginx.confExecReload=/bin/kill -s HUP \\$MAINPIDExecStop=/bin/kill -s TERM \\$MAINPIDLimitNOFILE=65535[Install]WantedBy=multi-user.target 123systemctl daemon-reload # 重新加载systemctl restart nginx.service # 重启服务systemctl status nginx.service # 查看服务状态 使用acme申请证书 安装 1234curl https://get.acme.sh | shalias acme.sh=~/.acme.sh/acme.sh# 升级一下acme.sh --upgrade 使用DNS验证申请泛域名 可能会失败，DNS的厂商无法很快的解析地址，重新申请就好了 123export DP_Id=&quot;&quot;export DP_Key=&quot;&quot;acme.sh --issue --dns dns_dp -d ifan.host -d *.ifan.host 安装证书 1234acme.sh --install-cert -d ifan.host -d *.ifan.host \\ --key-file /opt/openresty/nginx/cert/ifan/ifan.host.key \\ --fullchain-file /opt/openresty/nginx/cert/ifan/fullchain.cer \\ --reloadcmd &quot;service nginx force-reload&quot; acme给当前用户添加了一个定时任务，定时更新证书 1crontab -l 使用证书1234567891011121314151617server &#123; listen 80; listen 443 ssl http2; server_name ifan.host blog.ifan.host; index index.php index.html index.htm; root html/blog; ssl_certificate /opt/openresty/nginx/cert/ifan/fullchain.cer; ssl_certificate_key /opt/openresty/nginx/cert/ifan/ifan.host.key; ssl_protocols TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; access_log /var/log/openresty/blog.access.log; error_log /var/log/openresty/blog.error.log;&#125;","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"Nginx","slug":"WEB/Nginx","permalink":"https://blog.ifan.host/categories/WEB/Nginx/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.ifan.host/tags/Nginx/"}]},{"title":"Http Refer","slug":"web/http/http-refer","date":"2022-04-03T02:11:00.000Z","updated":"2023-12-25T05:56:09.187Z","comments":true,"path":"2022/04/03/web/http/http-refer/","link":"","permalink":"https://blog.ifan.host/2022/04/03/web/http/http-refer/","excerpt":"","text":"ReferHttp请求头中的常见字段，其含义为来源网站的信息，主要发生在点击连接，发送表单，加载静态信息的时候，告诉服务器用户访问该资源之前的位置。可以做用户追踪。一般用做图片外链，爬虫检测，资源外链，分析用户行为等场景中。 如果不想添加默认的Ref属性，则可以通过rel=&quot;noreferrer&quot;，来禁止浏览器发送。也可以通过跳转到后台在重定向实现。 在爬虫的使用过程中，有时候也是需要注意该字段的信息，因为有的网站的详情页是必须通过列表页才能跳转过去的，上一个列表页只有通过其他的列表页才能跳转过去，所以此时爬虫的请求头中需要模拟出该信息。 设置Referrer Policy摘自HTTP Referer 教程 12345678910111213141516171819202122232425262728293031（1）no-referrer不发送Referer字段。（2）no-referrer-when-downgrade如果从 HTTPS 网址链接到 HTTP 网址，不发送Referer字段，其他情况发送（包括 HTTP 网址链接到 HTTP 网址）。这是浏览器的默认行为。（3）same-origin链接到同源网址（协议+域名+端口 都相同）时发送，否则不发送。注意，https://foo.com链接到http://foo.com也属于跨域。（4）originReferer字段一律只发送源信息（协议+域名+端口），不管是否跨域。（5）strict-origin如果从 HTTPS 网址链接到 HTTP 网址，不发送Referer字段，其他情况只发送源信息。（6）origin-when-cross-origin同源时，发送完整的Referer字段，跨域时发送源信息。（7）strict-origin-when-cross-origin同源时，发送完整的Referer字段；跨域时，如果 HTTPS 网址链接到 HTTP 网址，不发送Referer字段，否则发送源信息。（8）unsafe-urlReferer字段包含源信息、路径和查询字符串，不包含锚点、用户名和密码。 使用方法（1）HTTP 头信息 服务器发送网页的时候，通过 HTTP 头信息的Referrer-Policy告诉浏览器。 1Referrer-Policy: origin （2）&lt;meta&gt;标签 也可以使用&lt;meta&gt;标签，在网页头部设置。 1&lt;meta name=&quot;referrer&quot; content=&quot;origin&quot;&gt; （3）referrerpolicy属性 &lt;a&gt;、&lt;area&gt;、&lt;img&gt;、&lt;iframe&gt;和&lt;link&gt;标签，可以设置referrerpolicy 属性。 1&lt;a href=&quot;...&quot; referrerpolicy=&quot;origin&quot; target=&quot;_blank&quot;&gt;xxx&lt;/a&gt;","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"HTTP","slug":"WEB/HTTP","permalink":"https://blog.ifan.host/categories/WEB/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/tags/WEB/"}]},{"title":"Nginx 安装","slug":"web/nginx/nginx-install","date":"2022-04-03T02:05:00.000Z","updated":"2023-12-25T05:56:09.188Z","comments":true,"path":"2022/04/03/web/nginx/nginx-install/","link":"","permalink":"https://blog.ifan.host/2022/04/03/web/nginx/nginx-install/","excerpt":"","text":"Nginx安装编译安装 安装编译环境和需要的包 1yum -y install gcc gcc-c++ pcre pcre-devel openssl openssl-devel zlib zlib-devel gd gd-devel 下载包并解压,创建nginx用户 编译安装 12345678910111213141516171819202122232425262728./configure --prefix=/usr/local/nginx \\--user=nginx \\--group=nginx \\--with-pcre \\--with-http_ssl_module \\--with-http_v2_module \\--with-http_realip_module \\--with-http_addition_module \\--with-http_sub_module \\--with-http_dav_module \\--with-http_flv_module \\--with-http_mp4_module \\--with-http_gunzip_module \\--with-http_gzip_static_module \\--with-http_random_index_module \\--with-http_secure_link_module \\--with-http_stub_status_module \\--with-http_auth_request_module \\--with-http_image_filter_module \\--with-http_slice_module \\--with-mail \\--with-threads \\--with-file-aio \\--with-stream \\--with-mail_ssl_module \\--with-stream_ssl_modulemake &amp;&amp; make install 配置nginx命令并添加开机启动 创建配置文件 1vim /usr/lib/systemd/system/nginx.service 12345678910111213141516[Unit]Description=nginx - high performance web serverDocumentation=http://nginx.org/en/docs/After=network.target remote-fs.target nss-lookup.target​[Service]Type=forkingPIDFile=/usr/local/nginx/logs/nginx.pidExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.confExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confExecReload= /usr/local/nginx/sbin/nginx -s reloadExecStop= /usr/local/nginx/sbin/nginx -s stopPrivateTmp=true​[Install]WantedBy=multi-user.target 添加执行权限 1chmod +x /usr/lib/systemd/system/nginx.service 测试命令的有效性 123456systemctl daemon-reloadsystemctl start nginx.service # 启动systemctl stop nginx.service # 停止systemctl reload nginx.service # 修改配置后重新加载生效 systemctl restart nginx.service # 重启systemctl status nginx # 查看服务是否启动 添加开机启动 1systemctl enable nginx.service Nginx 编译参数123456789101112131415161718192021222324252627282930313233343536# 模块参数具体功能 --with-cc-opt=&#x27;-g -O2 -fPIE -fstack-protector&#x27; # 设置额外的参数将被添加到CFLAGS变量。（FreeBSD或者ubuntu使用）--param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2&#x27; --with-ld-opt=&#x27;-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now&#x27; ​--prefix=/usr/share/nginx # 指向安装目录--conf-path=/etc/nginx/nginx.conf # 指定配置文件--http-log-path=/var/log/nginx/access.log # 指定访问日志--error-log-path=/var/log/nginx/error.log # 指定错误日志--lock-path=/var/lock/nginx.lock # 指定lock文件--pid-path=/run/nginx.pid # 指定pid文件​--http-client-body-temp-path=/var/lib/nginx/body # 设定http客户端请求临时文件路径--http-fastcgi-temp-path=/var/lib/nginx/fastcgi # 设定http fastcgi临时文件路径--http-proxy-temp-path=/var/lib/nginx/proxy # 设定http代理临时文件路径--http-scgi-temp-path=/var/lib/nginx/scgi # 设定http scgi临时文件路径--http-uwsgi-temp-path=/var/lib/nginx/uwsgi # 设定http uwsgi临时文件路径​--with-debug # 启用debug日志--with-pcre-jit # 编译PCRE包含“just-in-time compilation”--with-ipv6 # 启用ipv6支持--with-http_ssl_module # 启用ssl支持--with-http_stub_status_module # 获取nginx自上次启动以来的状态--with-http_realip_module # 允许从请求标头更改客户端的IP地址值，默认为关--with-http_auth_request_module # 实现基于一个子请求的结果的客户端授权。如果该子请求返回的2xx响应代码，所述接入是允许的。如果它返回401或403中，访问被拒绝与相应的错误代码。由子请求返回的任何其他响应代码被认为是一个错误。--with-http_addition_module # 作为一个输出过滤器，支持不完全缓冲，分部分响应请求--with-http_dav_module # 增加PUT,DELETE,MKCOL：创建集合,COPY和MOVE方法 默认关闭，需编译开启--with-http_geoip_module # 使用预编译的MaxMind数据库解析客户端IP地址，得到变量值--with-http_gunzip_module # 它为不支持“gzip”编码方法的客户端解压具有“Content-Encoding: gzip”头的响应。--with-http_gzip_static_module # 在线实时压缩输出数据流--with-http_image_filter_module # 传输JPEG/GIF/PNG 图片的一个过滤器）（默认为不启用。gd库要用到）--with-http_spdy_module # SPDY可以缩短网页的加载时间--with-http_sub_module # 允许用一些其他文本替换nginx响应中的一些文本--with-http_xslt_module # 过滤转换XML请求--with-mail # 启用POP3/IMAP4/SMTP代理模块支持--with-mail_ssl_module # 启用ngx_mail_ssl_module支持启用外部模块支持 Nginx 配置文件1234567891011121314151617181920212223242526272829303132333435363738# 全局参数设置 worker_processes 1; # 设置nginx启动进程的数量，一般设置成与逻辑cpu数量相同 error_log logs/error.log; # 指定错误日志 worker_rlimit_nofile 102400; # 设置一个nginx进程能打开的最大文件数 pid /var/run/nginx.pid; events &#123; # 事件配置 worker_connections 10240; # 设置一个进程的最大并发连接数 use epoll; # 事件驱动类型&#125; # http 服务相关设置 http &#123; log_format main &#x27;remote_addr - remote_user [time_local] &quot;request&quot; &#x27; &#x27;status body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;http_user_agent&quot; &quot;http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; #设置访问日志的位置和格式 sendfile on; # 用于开启文件高效传输模式，一般设置为on，若nginx是用来进行磁盘IO负载应用时，可以设置为off，降低系统负载 tcp_nopush on; # 减少网络报文段数量，当有数据时，先别着急发送, 确保数据包已经装满数据, 避免了网络拥塞 tcp_nodelay on; # 提高I/O性能，确保数据尽快发送, 提高可数据传输效率 gzip on; # 是否开启 gzip 压缩 keepalive_timeout 65; # 设置长连接的超时时间，请求完成之后还要保持连接多久，不是请求时间多久，目的是保持长连接，减少创建连接过程给系统带来的性能损 耗，类似于线程池，数据库连接池 types_hash_max_size 2048; # 影响散列表的冲突率。types_hash_max_size 越大，就会消耗更多的内存，但散列key的冲突率会降低，检索速度就更快。 types_hash_max_size越小，消耗的内存就越小，但散列key的冲突率可能上升 include /etc/nginx/mime.types; # 关联mime类型，关联资源的媒体类型(不同的媒体类型的打开方式) default_type application/octet-stream; # 根据文件的后缀来匹配相应的MIME类型，并写入Response header，导致浏览器播放文件而不是下载# 虚拟服务器的相关设置 server &#123; listen 80; # 设置监听的端口 server_name localhost; # 设置绑定的主机名、域名或ip地址 charset koi8-r; # 设置编码字符 location / &#123; root /var/www/nginx; # 设置服务器默认网站的根目录位置 index index.html index.htm; # 设置默认打开的文档 &#125; error_page 500 502 503 504 /50x.html; # 设置错误信息返回页面 location = /50x.html &#123; root html; # 这里的绝对位置是/var/www/nginx/html &#125; &#125; &#125; Nginx命令1234567nginx -c conf/nginx.conf # 以特定目录下的配置文件启动nginx:nginx -s reload # 修改配置后重新加载生效nginx -s reopen # 重新打开日志文件nginx -s stop # 快速停止nginxnginx -s quit # 完整有序的停止nginxnginx -t # 测试当前配置文件是否正确nginx -t -c /path/to/nginx.conf # 测试特定的nginx配置文件是否正确","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"Nginx","slug":"WEB/Nginx","permalink":"https://blog.ifan.host/categories/WEB/Nginx/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.ifan.host/tags/Nginx/"}]},{"title":"Nginx 常用配置文件","slug":"web/nginx/nginx-config","date":"2022-04-03T02:01:00.000Z","updated":"2023-12-25T05:56:09.187Z","comments":true,"path":"2022/04/03/web/nginx/nginx-config/","link":"","permalink":"https://blog.ifan.host/2022/04/03/web/nginx/nginx-config/","excerpt":"","text":"正向代理测试 1curl https://www.baidu.com -v -x 127.0.0.1 12345678910111213141516171819202122server &#123; listen 1080; access_log logs/proxy.log; resolver 114.114.114.114; #指定DNS服务器IP地址 server_name proxy.ifan.tutulis.com; proxy_connect; proxy_connect_allow 80 443; proxy_connect_connect_timeout 10s; proxy_connect_read_timeout 10s; proxy_connect_send_timeout 10s; # forward proxy for non-CONNECT request location / &#123; # proxy_pass $scheme://$http_host$request_uri proxy_pass http://$host; proxy_set_header Host $host; proxy_buffers 256 4k; proxy_max_temp_file_size 0; proxy_connect_timeout 10s; &#125;&#125; 反向代理123location /apis/ &#123; proxy_pass http://127.0.0.1:8000;&#125; 12345678910111213141516171819upstream apis &#123; server 127.0.0.1:8000 weight=10 max_fails=3 fail_timeout=30s; server 127.0.0.1:9000 backup; # 热备 &#125;location / &#123; add_header Cache-Control no-cache; proxy_set_header X-Real-IP $remote_addr; proxy_pass http://apis; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 10m; # 允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; # 缓冲区代理缓冲用户端请求的最大字节数 proxy_connect_timeout 300; # nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 300; # 后端服务器数据回传时间(代理发送超时) proxy_read_timeout 300; # 连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; # 设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; # proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 64k; # 高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; # 设定缓存文件夹大小，大于这个值，将从upstream服务器传 &#125; weight: 加权轮询 max_fails: 允许请求失败的次数默认为1。当超过最大次数时，返回 proxy_next_upstream 模块定义的错误. fail_timeout: max_fails次失败后，暂停的时间 max_conns: 限制最大的接受连接数 backup: 其它所有的非 backup 机器 down 或者忙的时候，请求 backup 机器。所以这台机器压力会最轻。 代理静态文件12345678910location / &#123; add_header Content-Type &quot;text/html;charset=utf-8&quot;; root /opt/web/static; # 站点根目录 index index.html index.htm; # 默认导航页&#125;location /imgs &#123; root /opt/web/imgs/; index index.html; &#125;","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"Nginx","slug":"WEB/Nginx","permalink":"https://blog.ifan.host/categories/WEB/Nginx/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.ifan.host/tags/Nginx/"}]},{"title":"Vim 快捷键","slug":"tools/vim/vim-key","date":"2022-04-03T02:00:00.000Z","updated":"2023-12-25T05:56:09.186Z","comments":true,"path":"2022/04/03/tools/vim/vim-key/","link":"","permalink":"https://blog.ifan.host/2022/04/03/tools/vim/vim-key/","excerpt":"","text":"Vim 快捷键命令行操作移动 键 描述 h 左 j 下 k 上 l 右 10j 向下移动10行 0 到行首 $ 到行尾 gg 到文件首 G 到文件尾 10G 到第10行 复制 键 描述 yy 复制一行 2yy 向下复制2行 yw 复制光标开始的一个单词 y$ 复制光标到行尾 剪切 键 描述 x 剪切光标下的字符 3x 剪切后3个 删除 键 描述 dd 删除一行 5dd 删除5行 dw 删除一个单词 df&quot; 删除到出现第一个双引号 粘贴 键 描述 p 粘贴 3p 粘贴3次 其他| 键 | 描述 || r | 替换字符 || ggVG | 全选 || u | 恢复更改 || J | 合并下一行 || gU | 光标处转大写 || ggguG | 整篇文章大写转化为小写 || % | 跳转到下一个匹配,如在上按%，则跳转到相应的 || :e /tmp/a 在同一个编辑器内打开/tmp/a文件。同一个编辑器的缓冲区是剪贴板是共享的，可以方便在多个文件中复制 | | bp| 跳转到上一个缓冲区 | |bn| 跳转到下一个缓冲区 | |ctrl+r&#96; | 撤销 | 各种模式 模式 进入 行模式 v 块模式 ctrl+v 命令模式 普通模式下 : 1. 如何编辑多列数据 进入块模式，ctrl+v 选择需要编辑的多个列 进入插入模式，I 输入需要的内容 退出即可看到变化，ESC 或者 定位到需要编辑的第一行的位置 进入宏录制，qa，（q：进入宏录制；a：录制的名称为a） 编辑插入等操作 推出宏录制，q 触发宏录制，查看效果，@a 多次出发，完成编辑，10@a 2. 替换字符 命令 描述 %s/$/&quot;/ 在行尾追加&quot; %s/\\^M//g 替换dos的换行符 :g/\\^\\s*$/d 删除空行或者只有空格的行 %s/#.*//g 删除#之后的字符 vimrc12345678syntax on &quot; 支持语法高亮显示filetype plugin indent on &quot; 启用根据文件类型自动缩进set autoindent &quot; 开始新行时处理缩进set expandtab &quot; 将制表符Tab展开为空格，这对于Python尤其有用set tabstop=4 &quot; 要计算的空格数set shiftwidth=4 &quot; 用于自动缩进的空格数set backspace=2 &quot; 在多数终端上修正退格键Backspace的行为colorscheme murphy &quot; 修改配色","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"vim","slug":"工具/vim","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/vim/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"vim","slug":"vim","permalink":"https://blog.ifan.host/tags/vim/"}]},{"title":"ffmpeg 的一些命令","slug":"tools/ffmpeg","date":"2022-04-02T15:38:33.000Z","updated":"2023-12-25T05:56:09.184Z","comments":true,"path":"2022/04/02/tools/ffmpeg/","link":"","permalink":"https://blog.ifan.host/2022/04/02/tools/ffmpeg/","excerpt":"","text":"FFmpeg视频相关视频转换12345678910111213# 检查音视频是否可以播放ffmpeg -v error -i input.mp4 -f null -# 视频格式ffmpeg -i test.mp4 -vcodec h264 output.mp4# 视频分辨率# -1 表示和之前的相同ffmpeg -i test.mp4 -vf scale=480:-1 output.mp4ffmpeg -i test.mp4 -s 800x600 output.mp4# 视频帧率ffmpeg -i test.mp4 -r 10 output.mp4# 改变CRF 18-28是人可以接受的范围，超过之后会明显感觉有瑕疵# 0-50是可选参数，50压缩比是最大的，质量也是最低的ffmpeg -i input.mp4 -c:v libx264 -preset veryslow -crf 24 output.mp4 视频合成123456789// 转换为ts流ffmpeg -i 0.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 0.tsffmpeg -i 1.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 1.tsffmpeg -i 2.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 2.tsffmpeg -i 3.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 3.tsffmpeg -i 4.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 4.tsffmpeg -i 5.mp4 -vcodec copy -acodec copy -vbsf h264_mp4toannexb 5.ts// 合并ts流为mp4ffmpeg -i &quot;concat:0.ts|1.ts|2.ts|3.ts|4.ts|5.ts&quot; -acodec copy -vcodec copy -absf aac_adtstoasc FileName.mp4 视频截取1ffmpeg -ss 00:00:00 -t 00:30:00 -i test-1.mp4 test-2.mp4 视频倒放12345678// 1.视频倒放，无音频ffmpeg.exe -i inputfile.mp4 -filter_complex [0:v]reverse[v] -map [v] -preset superfast reversed.mp4// 2.视频倒放，音频不变ffmpeg.exe -i inputfile.mp4 -vf reverse reversed.mp4// 3.音频倒放，视频不变ffmpeg.exe -i inputfile.mp4 -map 0 -c:v copy -af &quot;areverse&quot; reversed_audio.mp4// 4.音视频同时倒放ffmpeg.exe -i inputfile.mp4 -vf reverse -af areverse -preset superfast reversed.mp4 视频流将流媒体服务器上的流dump到本地1ffmpeg -i rtmp://127.0.0.1/rh/mylive -acodec copy -vcodec copy -f flv test.flv 将视频推送到流媒体服务器上1ffmpeg -re -i pm.mp4 -acodec copy -vcodec copy -f flv rtmp://127.0.0.1/rh/mylive 合并m3u8文件1ffmpeg -f concat -safe 0 -i input.m3u8 -c copy output.mp4 input.m3u8 中的内容为 1file &#x27;文件地址.ts&#x27; 读取视频流到文件1ffmpeg -i https://example.com/live/example.m3u8 -vcodec copy -acodec copy example.mp4 音频相关1234567891011121314151617# 提取音频文件ffmpeg -i test.mp4 -vn -acodec copy output.mp3ffmpeg -i test.mp4 -map 0:a -acodec libmp3lame output.mp3# 音频静音ffmpeg -i test.mp4 -an -vcodec copy output.mp4# 将单独左声道音频提升为立体声道ffmpeg -i left.mp3 -af &quot;channelmap=0-1|0-0&quot; all.mp3# 双声道合并为单声道ffmpeg -i music.mp3 -ac 1 music.aac# 单声道转双声道 ac 指定声道数ffmpeg -i left.aac -ac 2 output.m4a# 将两个音频源合并为双声道ffmpeg -i left.aac -i right.aac -filter_complex &quot;[0:a][1:a]amerge=inputs=2[aout]&quot; -map &quot;[aout]&quot; output.mka# 双声道提取ffmpeg -i music.mp3 -map_channel 0.0.0 letf.aac -map_channel 0.0.1 right.aac# 声道转换ffmpeg -i music.mp3 -filter_complex channelsplit=channel_layout=stereo output.mka 查看音频布局情况1ffmpeg -layouts查看音频的布局情况 绘制音频波形图 （多声道混合）1ffmpeg -i music.mp3 -filter_complex &quot;showwavespic=s=640*120&quot; -frames:v 1 output.png 绘制不同声道的波形图 1ffmpeg -i 1.mp3 -filter_complex &quot;showwavespic=s=640*240:split_channels=1&quot; -frames:v 1 output.png 音量设置声音音量应该仔细调整，以保护我们的耳朵和ffmpeg提供2种方法。第一个使用-vol选项，它接受从0到256的整数值，其中256是最大值 1ffmpeg -i music.mp3 -vol 30 sound_low.mp3 另一种方法是使用表中描述的卷过滤器: 音量降低到三分之二 1ffmpeg -i music.mp3 -af volume=2/3 quiet_music.mp3 增加10分贝的音量 1ffmpeg -i music.mp3 -af volume=10dB louder_sound.mp3 加快&#x2F;减慢节奏atempo 值范围0.5-2.0 1234# 加快ffplay music.mp3 -af atempo=1.5# 减慢ffplay music.mp3 -af atempo=0.5 噪音处理 音频分离 12ffmpeg.exe -i input.mp4 tmpvid.mp4ffmpeg -i input.mp4 tmpaud.wav 提取噪音 1ffmpeg -i noise.mp4 noise.wav 分析噪音，去除噪音 12sox noise.wav -n noiseprof noise.profsox tmpaud.wav denoise.wav noisered noise.prof 0.21 合并音频 1ffmpeg.exe -i denoise.wav -i tmpvid.mp4 fin.mp4","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"ffmpeg","slug":"工具/ffmpeg","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/ffmpeg/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://blog.ifan.host/tags/ffmpeg/"}]},{"title":"Vim 命令图片","slug":"tools/vim/vim-imgs","date":"2022-04-02T15:38:33.000Z","updated":"2023-12-25T05:56:09.185Z","comments":true,"path":"2022/04/02/tools/vim/vim-imgs/","link":"","permalink":"https://blog.ifan.host/2022/04/02/tools/vim/vim-imgs/","excerpt":"","text":"","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"vim","slug":"工具/vim","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/vim/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"vim","slug":"vim","permalink":"https://blog.ifan.host/tags/vim/"}]},{"title":"Centos7 tesseract 安装","slug":"system/centos/centos7-tesseract-install","date":"2022-04-02T15:30:33.000Z","updated":"2023-12-25T05:56:09.180Z","comments":true,"path":"2022/04/02/system/centos/centos7-tesseract-install/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/centos/centos7-tesseract-install/","excerpt":"","text":"Centos7 tesseract 安装正规网址 升级下基础包 1234sudo yum updateyum -y install automakeyum -y install libtoolyum install libpng-devel 这个过程最好用root安装，或者使用sudo命令 123456wget http://www.leptonica.org/source/leptonica-1.79.0.tar.gztar xzvf leptonica-1.79.0.tar.gzcd leptonica-1.73./configuremakemake install 安装tesseract 123456789wget https://github.com/tesseract-ocr/tesseract/archive/3.04.01.tar.gzmv 3.04.01.tar.gz tesseract-3.04.01.tar.gztar xzvf tesseract-3.04.01.tar.gzcd tesseract-3.04.01/./autogen.sh./configuremakemake installldconfig 安装模型文件 12345678wget https://sourceforge.net/projects/tesseract-ocr-alt/files/tesseract-ocr-3.02.eng.tar.gzwget https://sourceforge.net/projects/tesseract-ocr-alt/files/tesseract-ocr-3.02.nld.tar.gzwget https://sourceforge.net/projects/tesseract-ocr-alt/files/tesseract-ocr-3.02.deu.tar.gztar xzvf tesseract-ocr-3.02.eng.tar.gztar xzvf tesseract-ocr-3.02.nld.tar.gztar xzvf tesseract-ocr-3.02.deu.tar.gzecho export TESSDATA_PREFIX=/usr/share/tesseract-ocr/tessdata &gt;&gt; .bashrc 安装ghostpdl 1234567wget https://github.com/ArtifexSoftware/ghostpdl-downloads/releases/download/gs920/ghostscript-9.20.tar.gztar xzvf ghostscript-9.20.tar.gzcd ghostscript-9.20/./autogen.sh./configuremakemake install","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"Centos","slug":"Linux/Centos","permalink":"https://blog.ifan.host/categories/Linux/Centos/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"https://blog.ifan.host/tags/Centos/"},{"name":"tesseract","slug":"tesseract","permalink":"https://blog.ifan.host/tags/tesseract/"}]},{"title":"IDEA 快捷键","slug":"tools/development-tool/idae-shortcut-key","date":"2022-04-02T15:24:33.000Z","updated":"2023-12-25T05:56:09.183Z","comments":true,"path":"2022/04/02/tools/development-tool/idae-shortcut-key/","link":"","permalink":"https://blog.ifan.host/2022/04/02/tools/development-tool/idae-shortcut-key/","excerpt":"","text":"IDEA 快捷键Part 1：Editing（编辑） 快捷键 作用 Control + Space 基本的代码补全（补全任何类、方法、变量） Control + Shift + Space 智能代码补全（过滤器方法列表和变量的预期类型） Command + Shift + Enter 自动结束代码，行末自动添加分号 Command + P 显示方法的参数信息 Control + J 快速查看文档 Shift + F1 查看外部文档（在某些代码上会触发打开浏览器显示相关文档） Command + 鼠标放在代码上 显示代码简要信息 Command + F1 在错误或警告处显示具体描述信息 Command + N, Control + Enter, Control + N 生成代码（getter、setter、hashCode、equals、toString、构造函数等） Control + O 覆盖方法（重写父类方法） Control + I 实现方法（实现接口中的方法） Command + Option + T 包围代码（使用if...else、try...catch、for、synchronized等包围选中的代码） Command + / 注释 &#x2F; 取消注释与行注释 Command + Option + / 注释 &#x2F; 取消注释与块注释 Option + 方向键上 连续选中代码块 Option + 方向键下 减少当前选中的代码块 Control + Shift + Q 显示上下文信息 Option + Enter 显示意向动作和快速修复代码 Command + Option + L 格式化代码 Control + Option + O 优化 import Control + Option + I 自动缩进线 Tab / Shift + Tab 缩进代码 &#x2F; 反缩进代码 Command + X 剪切当前行或选定的块到剪贴板 Command + C 复制当前行或选定的块到剪贴板 Command + V 从剪贴板粘贴 Command + Shift + V 从最近的缓冲区粘贴 Command + D 复制当前行或选定的块 Command + Delete 删除当前行或选定的块的行 Control + Shift + J 智能的将代码拼接成一行 Command + Enter 智能的拆分拼接的行 Shift + Enter 开始新的一行 Command + Shift + U 大小写切换 Command + Shift + ] &#x2F; Command + Shift + [ 选择直到代码块结束 &#x2F; 开始 Option + Fn + Delete 删除到单词的末尾 Option + Delete 删除到单词的开头 Command + 加号 &#x2F; Command + 减号 展开 &#x2F; 折叠代码块 Command + Shift + 加号 展开所以代码块 Command + Shift + 减号 折叠所有代码块 Command + W 关闭活动的编辑器选项卡 Part 2：Search &#x2F; Replace（查询&#x2F;替换） 快捷键 作用 Double Shift 查询任何东西 Command + F 文件内查找 Command + G 查找模式下，向下查找 Command + Shift + G 查找模式下，向上查找 Command + R 文件内替换 Command + Shift + F 全局查找（根据路径） Command + Shift + R 全局替换（根据路径） Command + Shift + S 查询结构（Ultimate Edition 版专用，需要在 Keymap 中设置） Command + Shift + M 替换结构（Ultimate Edition 版专用，需要在 Keymap 中设置） Part 3：Usage Search（使用查询） 快捷键 作用 Option + F7 &#x2F; Command + F7 在文件中查找用法 &#x2F; 在类中查找用法 Command + Shift + F7 在文件中突出显示的用法 Command + Option + F7 显示用法 Part 4：Compile and Run（编译和运行） 快捷键 作用 Command + F9 编译 Project Command + Shift + F9 编译选择的文件、包或模块 Control + Option + R 弹出 Run 的可选择菜单 Control + Option + D 弹出 Debug 的可选择菜单 Control + R 运行 Control + D 调试 Control + Shift + R, Control + Shift + D 从编辑器运行上下文环境配置 Part 5：Debugging（调试） 快捷键 作用 F8 进入下一步，如果当前行断点是一个方法，则不进入当前方法体内 F7 进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中 Shift + F7 智能步入，断点所在行上有多个方法调用，会弹出进入哪个方法 Shift + F8 跳出 Option + F9 运行到光标处，如果光标前有其他断点会进入到该断点 Option + F8 计算表达式（可以更改变量值使其生效） Command + Option + R 恢复程序运行，如果该断点下面代码还有断点则停在下一个断点上 Command + F8 切换断点（若光标当前行有断点则取消断点，没有则加上断点） Command + Shift + F8 查看断点信息 Part 6：Navigation（导航） 快捷键 作用 Command + O 查找类文件 Command + Shift + O 查找所有类型文件、打开文件、打开目录，打开目录需要在输入的内容前面或后面加一个反斜杠/ Command + Option + O 前往指定的变量 &#x2F; 方法 Control + 方向键左 &#x2F; Control + 方向键右 左右切换打开的编辑 tab 页 F12 返回到前一个工具窗口 Esc 从工具窗口进入代码文件窗口 Shift + Esc 隐藏当前或最后一个活动的窗口，且光标进入代码文件窗口 Command + Shift + F4 关闭活动 run/messages/find/... tab Command + L 在当前文件跳转到某一行的指定处 Command + E 显示最近打开的文件记录列表 Option + 方向键左 &#x2F; Option + 方向键右 光标跳转到当前单词 &#x2F; 中文句的左 &#x2F; 右侧开头位置 Command + Option + 方向键左 &#x2F; Command + Option + 方向键右 退回 &#x2F; 前进到上一个操作的地方 Command + Shift + Delete 跳转到最后一个编辑的地方 Option + F1 显示当前文件选择目标弹出层，弹出层中有很多目标可以进行选择(如在代码编辑窗口可以选择显示该文件的 Finder) Command + B &#x2F; Command + 鼠标点击 进入光标所在的方法&#x2F;变量的接口或是定义处 Command + Option + B 跳转到实现处，在某个调用的方法名上使用会跳到具体的实现处，可以跳过接口 Option + Space, Command + Y 快速打开光标所在方法、类的定义 Control + Shift + B 跳转到类型声明处 Command + U 前往当前光标所在方法的父类的方法 &#x2F; 接口定义 Control + 方向键下 &#x2F; Control + 方向键上 当前光标跳转到当前文件的前一个 &#x2F; 后一个方法名位置 Command + ] &#x2F; Command + [ 移动光标到当前所在代码的花括号开始 &#x2F; 结束位置 Command + F12 弹出当前文件结构层，可以在弹出的层上直接输入进行筛选（可用于搜索类中的方法） Control + H 显示当前类的层次结构 Command + Shift + H 显示方法层次结构 Control + Option + H 显示调用层次结构 F2 &#x2F; Shift + F2 跳转到下一个 &#x2F; 上一个突出错误或警告的位置 F4 &#x2F; Command + 方向键下 编辑 &#x2F; 查看代码源 Option + Home 显示到当前文件的导航条 F3 选中文件 &#x2F; 文件夹 &#x2F; 代码行，添加 &#x2F; 取消书签 Option + F3 选中文件 &#x2F; 文件夹&#x2F;代码行，使用助记符添加 &#x2F; 取消书签 Control + 0…Control + 9 定位到对应数值的书签位置 Command + F3 显示所有书签 Part 7：Refactoring（重构） 快捷键 作用 F5 复制文件到指定目录 F6 移动文件到指定目录 Command + Delete 在文件上为安全删除文件，弹出确认框 Shift + F6 重命名文件 Command + F6 更改签名 Command + Option + N 一致性 Command + Option + M 将选中的代码提取为方法 Command + Option + V 提取变量 Command + Option + F 提取字段 Command + Option + C 提取常量 Command + Option + P 提取参数 Part 8：VCS &#x2F; Local History（版本控制 &#x2F; 本地历史记录） 快捷键 作用 Command + K 提交代码到版本控制器 Command + T 从版本控制器更新代码 Option + Shift + C 查看最近的变更记录 Control + C 快速弹出版本控制器操作面板 Part 9：Live Templates（动态代码模板） 快捷键 作用 Command + Option + J 弹出模板选择窗口，将选定的代码使用动态模板包住 Command + J 插入自定义动态代码模板 Part 10：General（通用） 快捷键 作用 Command + 1…Command + 9 打开相应编号的工具窗口 Command + S 保存所有 Command + Option + Y 同步、刷新 Control + Command + F 切换全屏模式 Command + Shift + F12 切换最大化编辑器 Option + Shift + F 添加到收藏夹 Option + Shift + I 检查当前文件与当前的配置文件 Control + &#96; 快速切换当前的 scheme（切换主题、代码样式等） Command + , 打开 IDEA 系统设置 Command + ; 打开项目结构对话框 Shift + Command + A 查找动作（可设置相关选项） Control + Shift + Tab 编辑窗口标签和工具窗口之间切换（如果在切换的过程加按上 delete，则是关闭对应选中的窗口） Ctrl 快捷键 介绍 Ctrl + F 在当前文件进行文本查找 （必备） Ctrl + R 在当前文件进行文本替换 （必备） Ctrl + Z 撤销 （必备） Ctrl + Y 删除光标所在行 或 删除选中的行 （必备） Ctrl + X 剪切光标所在行 或 剪切选择内容 Ctrl + C 复制光标所在行 或 复制选择内容 Ctrl + D 复制光标所在行 或 复制选择内容，并把复制内容插入光标位置下面 （必备） Ctrl + W 递进式选择代码块。可选中光标所在的单词或段落，连续按会在原有选中的基础上再扩展选中范围 （必备） Ctrl + E 显示最近打开的文件记录列表 （必备） Ctrl + N 根据输入的 类名 查找类文件 （必备） Ctrl + G 在当前文件跳转到指定行处 Ctrl + J 插入自定义动态代码模板 （必备） Ctrl + P 方法参数提示显示 （必备） Ctrl + Q 光标所在的变量 / 类名 / 方法名等上面（也可以在提示补充的时候按），显示文档内容 Ctrl + U 前往当前光标所在的方法的父类的方法 / 接口定义 （必备） Ctrl + B 进入光标所在的方法/变量的接口或是定义处，等效于 Ctrl + 左键单击 （必备） Ctrl + K 版本控制提交项目，需要此项目有加入到版本控制才可用 Ctrl + T 版本控制更新项目，需要此项目有加入到版本控制才可用 Ctrl + H 显示当前类的层次结构 Ctrl + O 选择可重写的方法 Ctrl + I 选择可继承的方法 Ctrl + + 展开代码 Ctrl + - 折叠代码 Ctrl + / 注释光标所在行代码，会根据当前不同文件类型使用不同的注释符号 （必备） Ctrl + [ 移动光标到当前所在代码的花括号开始位置 Ctrl + ] 移动光标到当前所在代码的花括号结束位置 Ctrl + F1 在光标所在的错误代码处显示错误信息 （必备） Ctrl + F3 调转到所选中的词的下一个引用位置 （必备） Ctrl + F4 关闭当前编辑文件 Ctrl + F8 在 Debug 模式下，设置光标当前行为断点，如果当前已经是断点则去掉断点 Ctrl + F9 执行 Make Project 操作 Ctrl + F11 选中文件 / 文件夹，使用助记符设定 / 取消书签 （必备） Ctrl + F12 弹出当前文件结构层，可以在弹出的层上直接输入，进行筛选 Ctrl + Tab 编辑窗口切换，如果在切换的过程又加按上delete，则是关闭对应选中的窗口 Ctrl + End 跳到文件尾 Ctrl + Home 跳到文件头 Ctrl + Space 基础代码补全，默认在 Windows 系统上被输入法占用，需要进行修改，建议修改为 Ctrl + 逗号 （必备） Ctrl + Delete 删除光标后面的单词或是中文句 （必备） Ctrl + BackSpace 删除光标前面的单词或是中文句 （必备） Ctrl + 1,2,3...9 定位到对应数值的书签位置 （必备） Ctrl + 左键单击 在打开的文件标题上，弹出该文件路径 （必备） Ctrl + 光标定位 按 Ctrl 不要松开，会显示光标所在的类信息摘要 Ctrl + 左方向键 光标跳转到当前单词 / 中文句的左侧开头位置 （必备） Ctrl + 右方向键 光标跳转到当前单词 / 中文句的右侧开头位置 （必备） Ctrl + 前方向键 等效于鼠标滚轮向前效果 （必备） Ctrl + 后方向键 等效于鼠标滚轮向后效果 （必备） Alt 快捷键 介绍 Alt + ` 显示版本控制常用操作菜单弹出层 （必备） Alt + Q 弹出一个提示，显示当前类的声明 &#x2F; 上下文信息 Alt + F1 显示当前文件选择目标弹出层，弹出层中有很多目标可以进行选择 （必备） Alt + F2 对于前面页面，显示各类浏览器打开目标选择弹出层 Alt + F3 选中文本，逐个往下查找相同文本，并高亮显示 Alt + F7 查找光标所在的方法 &#x2F; 变量 &#x2F; 类被调用的地方 Alt + F8 在 Debug 的状态下，选中对象，弹出可输入计算表达式调试框，查看该输入内容的调试结果 Alt + Home 定位 &#x2F; 显示到当前文件的 Navigation Bar Alt + Enter IntelliJ IDEA 根据光标所在问题，提供快速修复选择，光标放在的位置不同提示的结果也不同 （必备） Alt + Insert 代码自动生成，如生成对象的 set &#x2F; get 方法，构造函数，toString() 等 （必备） Alt + 左方向键 切换当前已打开的窗口中的子视图，比如Debug窗口中有Output、Debugger等子视图，用此快捷键就可以在子视图中切换 （必备） Alt + 右方向键 按切换当前已打开的窗口中的子视图，比如Debug窗口中有Output、Debugger等子视图，用此快捷键就可以在子视图中切换 （必备） Alt + 前方向键 当前光标跳转到当前文件的前一个方法名位置 （必备） Alt + 后方向键 当前光标跳转到当前文件的后一个方法名位置 （必备） Alt + 1,2,3...9 显示对应数值的选项卡，其中 1 是 Project 用得最多 （必备） Shift 快捷键 介绍 Shift + F1 如果有外部文档可以连接外部文档 Shift + F2 跳转到上一个高亮错误 或 警告位置 Shift + F3 在查找模式下，查找匹配上一个 Shift + F4 对当前打开的文件，使用新Windows窗口打开，旧窗口保留 Shift + F6 对文件 &#x2F; 文件夹 重命名 Shift + F7 在 Debug 模式下，智能步入。断点所在行上有多个方法调用，会弹出进入哪个方法 Shift + F8 在 Debug 模式下，跳出，表现出来的效果跟 F9 一样 Shift + F9 等效于点击工具栏的 Debug 按钮 Shift + F10 等效于点击工具栏的 Run 按钮 Shift + F11 弹出书签显示层 （必备） Shift + Tab 取消缩进 （必备） Shift + ESC 隐藏当前 或 最后一个激活的工具窗口 Shift + End 选中光标到当前行尾位置 Shift + Home 选中光标到当前行头位置 Shift + Enter 开始新一行。光标所在行下空出一行，光标定位到新行位置 （必备） Shift + 左键单击 在打开的文件名上按此快捷键，可以关闭当前打开文件 （必备） Shift + 滚轮前后滚动 当前文件的横向滚动轴滚动 （必备） Ctrl + Alt 快捷键 介绍 Ctrl + Alt + L 格式化代码，可以对当前文件和整个包目录使用 （必备） Ctrl + Alt + O 优化导入的类，可以对当前文件和整个包目录使用 （必备） Ctrl + Alt + I 光标所在行 或 选中部分进行自动代码缩进，有点类似格式化 Ctrl + Alt + T 对选中的代码弹出环绕选项弹出层 （必备） Ctrl + Alt + J 弹出模板选择窗口，将选定的代码加入动态模板中 Ctrl + Alt + H 调用层次 Ctrl + Alt + B 在某个调用的方法名上使用会跳到具体的实现处，可以跳过接口 Ctrl + Alt + C 重构-快速提取常量 Ctrl + Alt + F 重构-快速提取成员变量 Ctrl + Alt + V 重构-快速提取变量 Ctrl + Alt + Y 同步、刷新 Ctrl + Alt + S 打开 IntelliJ IDEA 系统设置 （必备） Ctrl + Alt + F7 显示使用的地方。寻找被该类或是变量被调用的地方，用弹出框的方式找出来 Ctrl + Alt + F11 切换全屏模式 Ctrl + Alt + Enter 光标所在行上空出一行，光标定位到新行 （必备） Ctrl + Alt + Home 弹出跟当前文件有关联的文件弹出层 Ctrl + Alt + Space 类名自动完成 Ctrl + Alt + 左方向键 退回到上一个操作的地方 （必备） Ctrl + Alt + 右方向键 前进到上一个操作的地方 （必备） Ctrl + Alt + 前方向键 在查找模式下，跳到上个查找的文件 Ctrl + Alt + 后方向键 在查找模式下，跳到下个查找的文件 Ctrl + Alt + 右括号（]） 在打开多个项目的情况下，切换下一个项目窗口 Ctrl + Alt + 左括号（[） 在打开多个项目的情况下，切换上一个项目窗口 Ctrl + Shift 快捷键 介绍 Ctrl + Shift + F 根据输入内容查找整个项目 或 指定目录内文件 （必备） Ctrl + Shift + R 根据输入内容替换对应内容，范围为整个项目 或 指定目录内文件 （必备） Ctrl + Shift + J 自动将下一行合并到当前行末尾 （必备） Ctrl + Shift + Z 取消撤销 （必备） Ctrl + Shift + W 递进式取消选择代码块。可选中光标所在的单词或段落，连续按会在原有选中的基础上再扩展取消选中范围 （必备） Ctrl + Shift + N 通过文件名定位 &#x2F; 打开文件 &#x2F; 目录，打开目录需要在输入的内容后面多加一个正斜杠 （必备） Ctrl + Shift + U 对选中的代码进行大 &#x2F; 小写轮流转换 （必备） Ctrl + Shift + T 对当前类生成单元测试类，如果已经存在的单元测试类则可以进行选择 （必备） Ctrl + Shift + C 复制当前文件磁盘路径到剪贴板 （必备） Ctrl + Shift + V 弹出缓存的最近拷贝的内容管理器弹出层 Ctrl + Shift + E 显示最近修改的文件列表的弹出层 Ctrl + Shift + H 显示方法层次结构 Ctrl + Shift + B 跳转到类型声明处 （必备） Ctrl + Shift + I 快速查看光标所在的方法 或 类的定义 Ctrl + Shift + A 查找动作 &#x2F; 设置 Ctrl + Shift + / 代码块注释 （必备） Ctrl + Shift + [ 选中从光标所在位置到它的顶部中括号位置 （必备） Ctrl + Shift + ] 选中从光标所在位置到它的底部中括号位置 （必备） Ctrl + Shift + + 展开所有代码 （必备） Ctrl + Shift + - 折叠所有代码 （必备） Ctrl + Shift + F7 高亮显示所有该选中文本，按Esc高亮消失 （必备） Ctrl + Shift + F8 在 Debug 模式下，指定断点进入条件 Ctrl + Shift + F9 编译选中的文件 &#x2F; 包 &#x2F; Module Ctrl + Shift + F12 编辑器最大化 （必备） Ctrl + Shift + Space 智能代码提示 Ctrl + Shift + Enter 自动结束代码，行末自动添加分号 （必备） Ctrl + Shift + Backspace 退回到上次修改的地方 （必备） Ctrl + Shift + 1,2,3...9 快速添加指定数值的书签 （必备） Ctrl + Shift + 左键单击 把光标放在某个类变量上，按此快捷键可以直接定位到该类中 （必备） Ctrl + Shift + 左方向键 在代码文件上，光标跳转到当前单词 &#x2F; 中文句的左侧开头位置，同时选中该单词 &#x2F; 中文句 （必备） Ctrl + Shift + 右方向键 在代码文件上，光标跳转到当前单词 &#x2F; 中文句的右侧开头位置，同时选中该单词 &#x2F; 中文句 （必备） Ctrl + Shift + 前方向键 光标放在方法名上，将方法移动到上一个方法前面，调整方法排序 （必备） Ctrl + Shift + 后方向键 光标放在方法名上，将方法移动到下一个方法前面，调整方法排序 （必备） Alt + Shift 快捷键 介绍 Alt + Shift + N 选择 &#x2F; 添加 task （必备） Alt + Shift + F 显示添加到收藏夹弹出层 &#x2F; 添加到收藏夹 Alt + Shift + C 查看最近操作项目的变化情况列表 Alt + Shift + I 查看项目当前文件 Alt + Shift + F7 在 Debug 模式下，下一步，进入当前方法体内，如果方法体还有方法，则会进入该内嵌的方法中，依此循环进入 Alt + Shift + F9 弹出 Debug 的可选择菜单 Alt + Shift + F10 弹出 Run 的可选择菜单 Alt + Shift + 左键双击 选择被双击的单词 &#x2F; 中文句，按住不放，可以同时选择其他单词 &#x2F; 中文句 （必备） Alt + Shift + 前方向键 移动光标所在行向上移动 （必备） Alt + Shift + 后方向键 移动光标所在行向下移动 （必备） Ctrl + Shift + Alt 快捷键 介绍 Ctrl + Shift + Alt + V 无格式黏贴 （必备） Ctrl + Shift + Alt + N 前往指定的变量 &#x2F; 方法 Ctrl + Shift + Alt + S 打开当前项目设置 （必备） Ctrl + Shift + Alt + C 复制参考信息 其他 快捷键 介绍 F2 跳转到下一个高亮错误 或 警告位置 （必备） F3 在查找模式下，定位到下一个匹配处 F4 编辑源 （必备） F7 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中 F8 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则不进入当前方法体内 F9 在 Debug 模式下，恢复程序运行，但是如果该断点下面代码还有断点则停在下一个断点上 F11 添加书签 （必备） F12 回到前一个工具窗口 （必备） Tab 缩进 （必备） ESC 从工具窗口进入代码文件窗口 （必备） 连按两次Shift 弹出 Search Everywhere 弹出层","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"工具/编程工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"编程工具","permalink":"https://blog.ifan.host/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"VS Code 快捷键","slug":"tools/development-tool/vscode-shortcut-key","date":"2022-04-02T15:24:33.000Z","updated":"2023-12-25T05:56:09.184Z","comments":true,"path":"2022/04/02/tools/development-tool/vscode-shortcut-key/","link":"","permalink":"https://blog.ifan.host/2022/04/02/tools/development-tool/vscode-shortcut-key/","excerpt":"","text":"shortcut-key Vscode快捷键快捷键 WIN MAC Linux","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"工具/编程工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"编程工具","permalink":"https://blog.ifan.host/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"Sublime Text 快捷键","slug":"tools/development-tool/sublime-text","date":"2022-04-02T15:21:33.000Z","updated":"2023-12-25T05:56:09.183Z","comments":true,"path":"2022/04/02/tools/development-tool/sublime-text/","link":"","permalink":"https://blog.ifan.host/2022/04/02/tools/development-tool/sublime-text/","excerpt":"","text":"Sublime Text 快捷键Sublime Text 快捷键通用 Ctrl + Shift + P：调出命令板（Command Palette） &#96;Ctrl + &#96;&#96;：调出控制台 编辑 Ctrl + Enter：在当前行下面新增一行然后跳至该行 Ctrl + Shift + Enter：在当前行上面增加一行并跳至该行 Ctrl + ←/→：进行逐词移动 Ctrl + Shift + ←/→进行逐词选择 Ctrl + ↑/↓移动当前显示区域 Ctrl + Shift + ↑/↓移动当前行 选择（Selecting） Ctrl + D：选择当前光标所在的词并高亮该词所有出现的位置，再次 Ctrl + D 选择该词出现的下一个位置，在多重选词的过程中，使用 Ctrl + K 进行跳过，使用 Ctrl + U 进行回退，使用 Esc 退出多重编辑 Ctrl + Shift + L：将当前选中区域打散 Ctrl + J：把当前选中区域合并为一行 Ctrl + M：在起始括号和结尾括号间切换 Ctrl + Shift + M：快速选择括号间的内容 Ctrl + Shift + J：快速选择同缩进的内容 Ctrl + Shift + Space：快速选择当前作用域（Scope）的内容 查找&amp;替换（Finding&amp;Replacing） F3：跳至当前关键字下一个位置 Shift + F3：跳到当前关键字上一个位置 Alt + F3：选中当前关键字出现的所有位置 Ctrl + F/H：进行标准查找&#x2F;替换，之后： Alt + C：切换大小写敏感（Case-sensitive）模式 Alt + W：切换整字匹配（Whole matching）模式 Alt + R：切换正则匹配（Regex matching）模式 Ctrl + Shift + H：替换当前关键字 Ctrl + Alt + Enter：替换所有关键字匹配 Ctrl + Shift + F：多文件搜索&amp;替换 跳转（Jumping） Ctrl + P：跳转到指定文件，输入文件名后可以： @ 符号跳转：输入 @symbol 跳转到 symbol 符号所在的位置 # 关键字跳转：输入 #keyword 跳转到 keyword 所在的位置 : 行号跳转：输入 :12 跳转到文件的第12行。 Ctrl + R：跳转到指定符号 Ctrl + G：跳转到指定行号 窗口（Window） Ctrl + Shift + N：创建一个新窗口 Ctrl + N：在当前窗口创建一个新标签 Ctrl + W：关闭当前标签，当窗口内没有标签时会关闭该窗口 Ctrl + Shift + T：恢复刚刚关闭的标签 屏幕（Screen） F11：切换普通全屏 Shift + F11：切换无干扰全屏 Alt + Shift + 2：进行左右分屏 Alt + Shift + 8：进行上下分屏 Alt + Shift + 5：进行上下左右分屏 分屏之后，使用 Ctrl + 数字键 跳转到指定屏，使用 Ctrl + Shift + 数字键 将当前屏移动到指定屏 快捷键 快捷键 描述 Ctrl+Shift+P 开命令面板 Ctrl+P 搜索项目中的文件 Ctrl+G 跳转到第几行 Ctrl+W 关闭当前打开文件 Ctrl+Shift+W 关闭所有打开文件 Ctrl+Shift+V 粘贴并格式化 Ctrl+D 选择单词，重复可增加选择下一个相同的单词 Ctrl+L 选择行，重复可依次增加选择下一行 Ctrl+Shift+L 选择多行 Ctrl+Shift+Enter 在当前行前插入新行 Ctrl+X 删除当前行 Ctrl+M 跳转到对应括号 Ctrl+U 软撤销，撤销光标位置 Ctrl+J 选择标签内容 Ctrl+F 查找内容 Ctrl+Shift+F 查找并替换 Ctrl+H 替换 Ctrl+R 前往 method Ctrl+N 新建窗口 Ctrl+K+B 开关侧栏 Ctrl+Shift+M 选中当前括号内容，重复可选着括号本身 Ctrl+F2 设置&#x2F;删除标记 Ctrl+&#x2F; 注释当前行 Ctrl+Shift+&#x2F; 当前位置插入注释 Ctrl+Alt+&#x2F; 块注释，并Focus到首行，写注释说明用的 Ctrl+Shift+A 选择当前标签前后，修改标签用的 F11 全屏 Shift+F11 全屏免打扰模式，只编辑当前文件 Alt+F3 选择所有相同的词 Alt+. 闭合标签 Alt+Shift+数字 分屏显示 Alt+数字 切换打开第N个文件 Shift+右键拖动 光标多不，用来更改或插入列内容 鼠标的前进后退键可切换Tab文件 按Ctrl，依次点击或选取，可需要编辑的多个位置 按Ctrl+Shift+上下键，可替换行 选择类 快捷键 描述 Ctrl+D 选中光标所占的文本，继续操作则会选中下一个相同的文本。 Alt+F3 选中文本按下快捷键，即可一次性选择全部的相同文本进行同时编辑。举个栗子：快速选中并更改所有相同的变量名、函数名等。 Ctrl+L 选中整行，继续操作则继续选择下一行，效果和 Shift+↓ 效果一样。 Ctrl+Shift+L 先选中多行，再按下快捷键，会在每行行尾插入光标，即可同时编辑这些行。 Ctrl+Shift+M 选择括号内的内容（继续选择父括号）。举个栗子：快速选中删除函数中的代码，重写函数体代码或重写括号内里的内容。 Ctrl+M 光标移动至括号内结束或开始的位置。 Ctrl+Enter 在下一行插入新行。举个栗子：即使光标不在行尾，也能快速向下插入一行。 Ctrl+Shift+Enter 在上一行插入新行。举个栗子：即使光标不在行首，也能快速向上插入一行。 Ctrl+Shift+[ 选中代码，按下快捷键，折叠代码。 Ctrl+Shift+] 选中代码，按下快捷键，展开代码。 Ctrl+K+0 展开所有折叠代码。 Ctrl+← 向左单位性地移动光标，快速移动光标。 Ctrl+→ 向右单位性地移动光标，快速移动光标。 shift+↑ 向上选中多行。 shift+↓ 向下选中多行。 Shift+← 向左选中文本。 Shift+→ 向右选中文本。 Ctrl+Shift+← 向左单位性地选中文本。 Ctrl+Shift+→ 向右单位性地选中文本。 Ctrl+Shift+↑ 将光标所在行和上一行代码互换（将光标所在行插入到上一行之前）。 Ctrl+Shift+↓ 将光标所在行和下一行代码互换（将光标所在行插入到下一行之后）。 Ctrl+Alt+↑ 向上添加多行光标，可同时编辑多行。 Ctrl+Alt+↓ 向下添加多行光标，可同时编辑多行。 编辑类 快捷键 描述 Ctrl+J 合并选中的多行代码为一行。举个栗子：将多行格式的CSS属性合并为一行。 Ctrl+Shift+D 复制光标所在整行，插入到下一行。 Tab 向右缩进。 Shift+Tab 向左缩进。 Ctrl+K+K 从光标处开始删除代码至行尾。 Ctrl+Shift+K 删除整行。 Ctrl+&#x2F; 注释单行。 Ctrl+Shift+&#x2F; 注释多行。 Ctrl+K+U 转换大写。 Ctrl+K+L 转换小写。 Ctrl+Z 撤销。 Ctrl+Y 恢复撤销。 Ctrl+U 软撤销，感觉和 Gtrl+Z 一样。 Ctrl+F2 设置书签 Ctrl+T 左右字母互换。 F6 单词检测拼写 搜索类 快捷键 描述 Ctrl+F 打开底部搜索框，查找关键字。 Ctrl+shift+F 在文件夹内查找，与普通编辑器不同的地方是sublime允许添加多个文件夹进行查找，略高端，未研究。 Ctrl+P 打开搜索框。举个栗子：1、输入当前项目中的文件名，快速搜索文件，2、输入@和关键字，查找文件中函数名，3、输入：和数字，跳转到文件中该行代码，4、输入#和关键字，查找变量名。 Ctrl+G 打开搜索框，自动带：，输入数字跳转到该行代码。举个栗子：在页面代码比较长的文件中快速定位。 Ctrl+R 打开搜索框，自动带@，输入关键字，查找文件中的函数名。举个栗子：在函数较多的页面快速查找某个函数。 Ctrl+： 打开搜索框，自动带#，输入关键字，查找文件中的变量名、属性名等。 Ctrl+Shift+P 打开命令框。场景栗子：打开命名框，输入关键字，调用sublime text或插件的功能，例如使用package安装插件。 Esc 退出光标多行选择，退出搜索框，命令框等。 显示类 快捷键 描述 Ctrl+Tab 按文件浏览过的顺序，切换当前窗口的标签页。 Ctrl+PageDown 向左切换当前窗口的标签页。 Ctrl+PageUp 向右切换当前窗口的标签页。 Alt+Shift+1 窗口分屏，恢复默认1屏（非小键盘的数字） Alt+Shift+2 左右分屏-2列 Alt+Shift+3 左右分屏-3列 Alt+Shift+4 左右分屏-4列 Alt+Shift+5 等分4屏 Alt+Shift+8 垂直分屏-2屏 Alt+Shift+9 垂直分屏-3屏 Ctrl+K+B 开启&#x2F;关闭侧边栏。 F11 全屏模式 Shift+F11 免打扰模式","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"工具/编程工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"编程工具","slug":"编程工具","permalink":"https://blog.ifan.host/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"}]},{"title":"Git Cmd 命令图片","slug":"tools/git/git-cmd-img","date":"2022-04-02T15:10:33.000Z","updated":"2023-12-25T05:56:09.184Z","comments":true,"path":"2022/04/02/tools/git/git-cmd-img/","link":"","permalink":"https://blog.ifan.host/2022/04/02/tools/git/git-cmd-img/","excerpt":"","text":"","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"工具/Git","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Git/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"Git","permalink":"https://blog.ifan.host/tags/Git/"}]},{"title":"Git Cmd","slug":"tools/git/git-cmd","date":"2022-04-02T15:09:33.000Z","updated":"2023-12-25T05:56:09.184Z","comments":true,"path":"2022/04/02/tools/git/git-cmd/","link":"","permalink":"https://blog.ifan.host/2022/04/02/tools/git/git-cmd/","excerpt":"","text":"Git 命令1. 初始化仓库12345678# 在当前目录创建 Git 仓库git init # 新建一个目录 将其初始化为Git代码库git init [project-name]# 下载一个仓库和它的历史git clone [url] 2. 配置Git的设置文件为**.gitconfig**,可以在用户主目录下（全局配置），也可以在项目目录下。 123456789# 显示当前的Git配置git config --list# 编辑Git配置文件git config -e [--global]# 设置提交代码时用户的信息git config [--global] user.name &quot;[name]&quot;git config [--global] user.email &quot;[email address]&quot; gitignore在Git提交文件时忽略的信息 1234.idea*.log*.iml!.gitignore // 不忽略的文件 3. 增加&#x2F;删除文件123456789101112131415161718192021222324252627# 添加指定文件到暂存区git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录git add [dir]# 添加当前目录的所有文件到暂存区git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交git add -p# 删除工作区文件，并且将这次删除放入暂存区git rm [file1] [file2] ...# 停止追踪指定文件，但该文件会保留在工作区git rm --cached [file]# 改名文件，并且将这个改名放入暂存区git mv [file-original] [file-renamed]# 删除远程仓库中的文件git rm --cached giant_file# 暂存我们的巨型文件待删除，但将其留在磁盘上git commit --amend -CHEADgit push 4. 代码提交123456789101112131415161718# 提交暂存区到仓库区git commit -m [message]# 提交暂存区的指定文件到仓库区git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区git commit -a# 提交时显示所有diff信息git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化git commit --amend [file1] [file2] ... 5. 分支123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支git branch# 列出所有远程分支git branch -r# 列出所有本地分支和远程分支git branch -a# 新建一个分支，但依然停留在当前分支git branch [branch-name]# 新建一个分支，并切换到该分支git checkout -b [branch]# 新建一个分支，指向指定commitgit branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区git checkout [branch-name]# 切换到上一个分支git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支git merge [branch]# 选择一个commit，合并进当前分支git cherry-pick [commit]# 删除分支git branch -d [branch-name]# 删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 6. 标签1234567891011121314151617181920212223242526# 列出所有taggit tag# 新建一个tag在当前commitgit tag [tag]# 新建一个tag在指定commitgit tag [tag] [commit]# 删除本地taggit tag -d [tag]# 删除远程taggit push origin :refs/tags/[tagName]# 查看tag信息git show [tag]# 提交指定taggit push [remote] [tag]# 提交所有taggit push [remote] --tags# 新建一个分支，指向某个taggit checkout -b [branch] [tag] 7. 查看信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 显示有变更的文件git status# 显示当前分支的版本历史git log# 显示commit历史，以及每次commit发生变更的文件git log --stat# 搜索提交历史，根据关键词git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名git log --follow [file]git whatchanged [file]# 显示指定文件相关的每一次diffgit log -p [file]# 显示过去5次提交git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序git shortlog -sn# 显示指定文件是什么人在什么时间修改过git blame [file]# 显示暂存区和工作区的差异git diff# 显示暂存区和上一个commit的差异git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异git diff HEAD# 显示两次提交之间的差异git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码git diff --shortstat &quot;@&#123;0 day ago&#125;&quot;# 显示某次提交的元数据和内容变化git show [commit]# 显示某次提交发生变化的文件git show --name-only [commit]# 显示某次提交时，某个文件的内容git show [commit]:[filename]# 显示当前分支的最近几次提交git reflog 8. 远程同步12345678910111213141516171819202122232425262728293031323334353637383940# 下载远程仓库的所有变动git fetch [remote]# 显示所有远程仓库git remote -v# 显示某个远程仓库的信息git remote show [remote]# 增加一个新的远程仓库，并命名git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并# pull = fetch + mergegit pull [remote] [branch]# 下载远程分支git fetch [remote] [branch]# 从远程仓库下载改动git fetch [remote-name]/[branch]# 上传本地指定分支到远程仓库git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突git push [remote] --force# 推送所有分支到远程仓库git push [remote] --all# 删除一个远程仓库git remote rm [name]# 重命名远程仓库git remote rename [old name] [new name]# 一个远程仓库关联多个地址git remote set-url --add [name] [url] 9. 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区git checkout [commit] [file]# 恢复暂存区的所有文件到工作区git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset [file]# 重置暂存区与工作区，与上一次commit保持一致git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支git revert [commit]# 暂时将未提交的变化移除，稍后再移入git stashgit stash pop 10. 子模块12345678910# 添加子模块git submodule add https://github.com/chaconinc/DbConnector# 检出子模块# 直接检出子模块git clone --recurse-submodules https://github.com/chaconinc/MainProject# 更新子模块git submodule update --init# 更新子模块git submodule initgit submodule update 创建 .gitignore 文件12345678# 忽略文件git update-index --assume-unchanged &lt;files&gt;# 取消忽略文件git update-index --no-assume-unchanged &lt;files&gt;# 查看所有被忽略的文件git ls-files -v | grep &#x27;^h\\ &#x27;# 批量取消忽略git ls-files -v | grep &#x27;^h&#x27; | awk &#x27;&#123;print $2&#125;&#x27; |xargs git update-index --no-assume-unchanged 删除分支1234# 删除远程分支git push origin --delete branck_name# 删除本地分支git branch -d branck_name 回退1`git rebase` 命令在另一个分支基础之上重新应用，用于把一个分支的修改合并到当前分支。 远程分支列出所有远程分支 1git branch -a 下载远程分支 1git checkout -b 本地分支名 远程分支名 删除远程分支 1git push origin --delete","categories":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"工具/Git","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Git/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"Git","permalink":"https://blog.ifan.host/tags/Git/"}]},{"title":"Centos7 初始化系统脚本","slug":"system/centos/centos7-system-init","date":"2022-04-02T15:03:33.000Z","updated":"2023-12-25T05:56:09.180Z","comments":true,"path":"2022/04/02/system/centos/centos7-system-init/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/centos/centos7-system-init/","excerpt":"","text":"Centos7初始化系统123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324#!/bin/bash# init centos7 ./centos7-init.sh 主机名# 检查是否为root用户，脚本必须在root权限下运行if [[ &quot;$(whoami)&quot; != &quot;root&quot; ]]; then echo &quot;please run this script as root !&quot; &gt;&amp;2 exit 1fiecho -e &quot;\\033[31m the script only Support CentOS_7 x86_64 \\033[0m&quot;echo -e &quot;\\033[31m system initialization script, Please Seriously. press ctrl+C to cancel \\033[0m&quot;# 检查是否为64位系统，这个脚本只支持64位脚本platform=`uname -i`if [ $platform != &quot;x86_64&quot; ];then echo &quot;this script is only for 64bit Operating System !&quot; exit 1fiif [ &quot;$1&quot; == &quot;&quot; ];then echo &quot;The host name is empty.&quot; exit 1else hostnamectl --static set-hostname $1 hostnamectl set-hostname $1ficat &lt;&lt; EOF+---------------------------------------+| your system is CentOS 7 x86_64 || start optimizing |+---------------------------------------+EOFsleep 1# 安装必要支持工具及软件工具yum_update()&#123;yum update -yyum install -y nmap unzip wget vim lsof xz net-tools iptables-services ntpdate ntp-doc psmisc&#125;# 设置时间同步 set timezone_time()&#123;timedatectl set-timezone Asia/Shanghai/usr/sbin/ntpdate 0.cn.pool.ntp.org &gt; /dev/null 2&gt;&amp;1/usr/sbin/hwclock --systohc/usr/sbin/hwclock -wcat &gt; /var/spool/cron/root &lt;&lt; EOF10 0 * * * /usr/sbin/ntpdate 0.cn.pool.ntp.org &gt; /dev/null 2&gt;&amp;1* * * * */1 /usr/sbin/hwclock -w &gt; /dev/null 2&gt;&amp;1EOFchmod 600 /var/spool/cron/root/sbin/service crond restartsleep 1&#125;# 修改文件打开数 set the file limitlimits_config()&#123;cat &gt; /etc/rc.d/rc.local &lt;&lt; EOF#!/bin/bashtouch /var/lock/subsys/localulimit -SHn 1024000EOFsed -i &quot;/^ulimit -SHn.*/d&quot; /etc/rc.d/rc.localecho &quot;ulimit -SHn 1024000&quot; &gt;&gt; /etc/rc.d/rc.localsed -i &quot;/^ulimit -s.*/d&quot; /etc/profilesed -i &quot;/^ulimit -c.*/d&quot; /etc/profilesed -i &quot;/^ulimit -SHn.*/d&quot; /etc/profilecat &gt;&gt; /etc/profile &lt;&lt; EOFulimit -c unlimitedulimit -s unlimitedulimit -SHn 1024000EOFsource /etc/profileulimit -acat /etc/profile | grep ulimitif [ ! -f &quot;/etc/security/limits.conf.bak&quot; ]; then cp /etc/security/limits.conf /etc/security/limits.conf.bakficat &gt; /etc/security/limits.conf &lt;&lt; EOF* soft nofile 1024000* hard nofile 1024000* soft nproc 1024000* hard nproc 1024000hive - nofile 1024000hive - nproc 1024000EOFif [ ! -f &quot;/etc/security/limits.d/20-nproc.conf.bak&quot; ]; then cp /etc/security/limits.d/20-nproc.conf /etc/security/limits.d/20-nproc.conf.bakficat &gt; /etc/security/limits.d/20-nproc.conf &lt;&lt; EOF* soft nproc 409600root soft nproc unlimitedEOFsleep 1&#125;# 优化内核参数 tune kernel parametressysctl_config()&#123;if [ ! -f &quot;/etc/sysctl.conf.bak&quot; ]; then cp /etc/sysctl.conf /etc/sysctl.conf.bakfi#addcat &gt; /etc/sysctl.conf &lt;&lt; EOFnet.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_synack_retries = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_probes = 3net.ipv4.tcp_keepalive_intvl =15net.ipv4.tcp_retries1 = 3net.ipv4.tcp_retries2 = 5net.ipv4.tcp_fin_timeout = 10net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_window_scaling = 1net.ipv4.tcp_max_tw_buckets = 60000net.ipv4.tcp_max_orphans = 32768net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_mem = 94500000 915000000 927000000net.ipv4.tcp_wmem = 4096 16384 13107200net.ipv4.tcp_rmem = 4096 87380 17476000net.ipv4.ip_local_port_range = 1024 65000net.ipv4.ip_forward = 1net.ipv4.route.gc_timeout = 100net.core.somaxconn = 32768net.core.netdev_max_backlog = 32768net.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_max = 6553500net.netfilter.nf_conntrack_tcp_timeout_established = 180vm.overcommit_memory = 1vm.swappiness = 1fs.file-max = 1024000EOF#reload sysctl/sbin/sysctl -psleep 1&#125;# 设置UTF-8 LANG=&quot;zh_CN.UTF-8&quot;LANG_config()&#123;echo &quot;LANG=\\&quot;en_US.UTF-8\\&quot;&quot;&gt;/etc/locale.confsource /etc/locale.conf&#125;#关闭SELINUX disable selinuxselinux_config()&#123;sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/configsetenforce 0sleep 1&#125;#日志处理log_config()&#123;setenforce 0systemctl start systemd-journaldsystemctl status systemd-journald&#125;# 关闭防火墙firewalld_config()&#123;/usr/bin/systemctl stop firewalld.service/usr/bin/systemctl disable firewalld.service&#125;# SSH配置优化 set sshd_configsshd_config()&#123;if [ ! -f &quot;/etc/ssh/sshd_config.bak&quot; ]; then cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bakficat &gt;/etc/ssh/sshd_config&lt;&lt;EOFPort 22AddressFamily inetListenAddress 0.0.0.0Protocol 2HostKey /etc/ssh/ssh_host_rsa_keyHostKey /etc/ssh/ssh_host_ecdsa_keyHostKey /etc/ssh/ssh_host_ed25519_keySyslogFacility AUTHPRIVPermitRootLogin yesMaxAuthTries 6RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keysPasswordAuthentication yesChallengeResponseAuthentication noUsePAM yesUseDNS noX11Forwarding yesUsePrivilegeSeparation sandboxAcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGESAcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENTAcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGEAcceptEnv XMODIFIERSSubsystem sftp /usr/libexec/openssh/sftp-serverEOF/sbin/service sshd restart&#125;# 关闭ipv6 disable the ipv6ipv6_config()&#123;echo &quot;NETWORKING_IPV6=no&quot;&gt;/etc/sysconfig/networkecho 1 &gt; /proc/sys/net/ipv6/conf/all/disable_ipv6echo 1 &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6echo &quot;127.0.0.1 localhost localhost.localdomain&quot;&gt;/etc/hosts#sed -i &#x27;s/IPV6INIT=yes/IPV6INIT=no/g&#x27; /etc/sysconfig/network-scripts/ifcfg-enp0s8for line in $(ls -lh /etc/sysconfig/network-scripts/ifcfg-* | awk -F &#x27;[ ]+&#x27; &#x27;&#123;print $9&#125;&#x27;)doif [ -f $line ] then sed -i &#x27;s/IPV6INIT=yes/IPV6INIT=no/g&#x27; $line echo $ifidone&#125;# 设置历史命令记录格式 historyhistory_config()&#123;export HISTFILESIZE=10000000export HISTSIZE=1000000export PROMPT_COMMAND=&quot;history -a&quot;export HISTTIMEFORMAT=&quot;%Y-%m-%d_%H:%M:%S &quot;##export HISTTIMEFORMAT=&quot;&#123;\\&quot;TIME\\&quot;:\\&quot;%F %T\\&quot;,\\&quot;HOSTNAME\\&quot;:\\&quot;\\$HOSTNAME\\&quot;,\\&quot;LI\\&quot;:\\&quot;\\$(who -u am i 2&gt;/dev/null| awk &#x27;&#123;print \\$NF&#125;&#x27;|sed -e &#x27;s/[()]//g&#x27;)\\&quot;,\\&quot;LU\\&quot;:\\&quot;\\$(who am i|awk &#x27;&#123;print \\$1&#125;&#x27;)\\&quot;,\\&quot;NU\\&quot;:\\&quot;\\$&#123;USER&#125;\\&quot;,\\&quot;CMD\\&quot;:\\&quot;&quot;cat &gt;&gt;/etc/bashrc&lt;&lt;EOFalias vi=&#x27;vim&#x27;HISTDIR=&#x27;/var/log/command.log&#x27;if [ ! -f \\$HISTDIR ];thentouch \\$HISTDIRchmod 666 \\$HISTDIRfiexport HISTTIMEFORMAT=&quot;&#123;\\&quot;TIME\\&quot;:\\&quot;%F %T\\&quot;,\\&quot;IP\\&quot;:\\&quot;\\$(ip a | grep -E &#x27;192.168|172&#x27; | head -1 | awk &#x27;&#123;print \\$2&#125;&#x27; | cut -d/ -f1)\\&quot;,\\&quot;LI\\&quot;:\\&quot;\\$(who -u am i 2&gt;/dev/null| awk &#x27;&#123;print \\$NF&#125;&#x27;|sed -e &#x27;s/[()]//g&#x27;)\\&quot;,\\&quot;LU\\&quot;:\\&quot;\\$(who am i|awk &#x27;&#123;print \\$1&#125;&#x27;)\\&quot;,\\&quot;NU\\&quot;:\\&quot;\\$&#123;USER&#125;\\&quot;,\\&quot;CMD\\&quot;:\\&quot;&quot;export PROMPT_COMMAND=&#x27;history 1|tail -1|sed &quot;s/^[ ]\\+[0-9]\\+ //&quot;|sed &quot;s/$/\\&quot;&#125;/&quot;&gt;&gt; /var/log/command.log&#x27;EOFsource /etc/bashrc&#125;# 服务优化设置service_config()&#123;/usr/bin/systemctl enable NetworkManager-wait-online.service/usr/bin/systemctl start NetworkManager-wait-online.service/usr/bin/systemctl stop postfix.service/usr/bin/systemctl disable postfix.servicechmod +x /etc/rc.localchmod +x /etc/rc.d/rc.local#ls -l /etc/rc.d/rc.local&#125;# VIM设置vim_config()&#123;cat &gt; /root/.vimrc &lt;&lt; EOFset history=1000EOF#set autoindent &quot; 换行自动缩进#set shiftwidth=4 &quot; 自动缩进时的宽度#set ts=4 &quot; tabstop的宽度#set expandtab &quot; tab换成空格 #set softtabstop=4 &quot; 退格键#set paste &quot; 粘贴时不自动换行#autocmd InsertLeave * se cul#autocmd InsertLeave * se nocul#set nu#set bs=2#syntax on#set laststatus=2#set tabstop=4#set go=#set ruler#set showcmd#set cmdheight=1#hi CursorLine cterm=NONE ctermbg=blue ctermfg=white guibg=blue guifg=white#set hls#set cursorline#set ignorecase#set hlsearch#set incsearch#set helplang=cn&#125;# donedone_ok()&#123;touch /var/log/init-okcat &lt;&lt; EOF+-------------------------------------------------+| optimizer is done || it&#x27;s recommond to restart this server ! || Please Reboot system |+-------------------------------------------------+EOF&#125;# mainmain()&#123; yum_update zone_time limits_config sysctl_config LANG_config selinux_config log_config firewalld_config sshd_config ipv6_config history_config service_config vim_config done_ok&#125;main","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"Centos","slug":"Linux/Centos","permalink":"https://blog.ifan.host/categories/Linux/Centos/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"https://blog.ifan.host/tags/Centos/"}]},{"title":"Debian 国内源","slug":"system/debian/yum-source","date":"2022-04-02T14:59:33.000Z","updated":"2023-12-25T05:56:09.181Z","comments":true,"path":"2022/04/02/system/debian/yum-source/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/debian/yum-source/","excerpt":"","text":"Debian 国内镜像源1．使用说明将 /etc/apt/sources.list 文件中Debian默认的软件仓库地址和安全更新仓库地址修改为国内的镜像地址即可，比如将deb.debian.org和security.debian.org改为mirrors.xxx.com，并使用https访问，可使用如下命令： 1sed -i &quot;s@http://\\(deb\\|security\\).debian.org@https://mirrors.xxx.com@g&quot; /etc/apt/sources.list 修改之后再运行apt update更新索引。 2．国内常见镜像站点阿里云镜像站12345678deb https://mirrors.aliyun.com/debian/ bullseye main non-free contribdeb-src https://mirrors.aliyun.com/debian/ bullseye main non-free contribdeb https://mirrors.aliyun.com/debian-security/ bullseye-security maindeb-src https://mirrors.aliyun.com/debian-security/ bullseye-security maindeb https://mirrors.aliyun.com/debian/ bullseye-updates main non-free contribdeb-src https://mirrors.aliyun.com/debian/ bullseye-updates main non-free contribdeb https://mirrors.aliyun.com/debian/ bullseye-backports main non-free contribdeb-src https://mirrors.aliyun.com/debian/ bullseye-backports main non-free contrib 腾讯云镜像站12345678deb https://mirrors.tencent.com/debian/ bullseye main non-free contribdeb-src https://mirrors.tencent.com/debian/ bullseye main non-free contribdeb https://mirrors.tencent.com/debian-security/ bullseye-security maindeb-src https://mirrors.tencent.com/debian-security/ bullseye-security maindeb https://mirrors.tencent.com/debian/ bullseye-updates main non-free contribdeb-src https://mirrors.tencent.com/debian/ bullseye-updates main non-free contribdeb https://mirrors.tencent.com/debian/ bullseye-backports main non-free contribdeb-src https://mirrors.tencent.com/debian/ bullseye-backports main non-free contrib 网易镜像站12345678deb https://mirrors.163.com/debian/ bullseye main non-free contribdeb-src https://mirrors.163.com/debian/ bullseye main non-free contribdeb https://mirrors.163.com/debian-security/ bullseye-security maindeb-src https://mirrors.163.com/debian-security/ bullseye-security maindeb https://mirrors.163.com/debian/ bullseye-updates main non-free contribdeb-src https://mirrors.163.com/debian/ bullseye-updates main non-free contribdeb https://mirrors.163.com/debian/ bullseye-backports main non-free contribdeb-src https://mirrors.163.com/debian/ bullseye-backports main non-free contrib 华为镜像站12345678deb https://mirrors.huaweicloud.com/debian/ bullseye main non-free contribdeb-src https://mirrors.huaweicloud.com/debian/ bullseye main non-free contribdeb https://mirrors.huaweicloud.com/debian-security/ bullseye-security maindeb-src https://mirrors.huaweicloud.com/debian-security/ bullseye-security maindeb https://mirrors.huaweicloud.com/debian/ bullseye-updates main non-free contribdeb-src https://mirrors.huaweicloud.com/debian/ bullseye-updates main non-free contribdeb https://mirrors.huaweicloud.com/debian/ bullseye-backports main non-free contribdeb-src https://mirrors.huaweicloud.com/debian/ bullseye-backports main non-free contrib 清华大学镜像站12345678deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-freedeb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-freedeb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free 中科大镜像站1234567891011deb https://mirrors.ustc.edu.cn/debian/ bullseye main contrib non-freedeb-src https://mirrors.ustc.edu.cn/debian/ bullseye main contrib non-freedeb https://mirrors.ustc.edu.cn/debian/ bullseye-updates main contrib non-freedeb-src https://mirrors.ustc.edu.cn/debian/ bullseye-updates main contrib non-freedeb https://mirrors.ustc.edu.cn/debian/ bullseye-backports main contrib non-freedeb-src https://mirrors.ustc.edu.cn/debian/ bullseye-backports main contrib non-freedeb https://mirrors.ustc.edu.cn/debian-security/ bullseye-security main contrib non-freedeb-src https://mirrors.ustc.edu.cn/debian-security/ bullseye-security main contrib non-free","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"Debian","slug":"Linux/Debian","permalink":"https://blog.ifan.host/categories/Linux/Debian/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"Debian","slug":"Debian","permalink":"https://blog.ifan.host/tags/Debian/"}]},{"title":"阿里云 Centos8 增加虚拟内存","slug":"system/centos/centos8-aliyun-add-memory","date":"2022-04-02T11:17:33.000Z","updated":"2023-12-25T05:56:09.180Z","comments":true,"path":"2022/04/02/system/centos/centos8-aliyun-add-memory/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/centos/centos8-aliyun-add-memory/","excerpt":"","text":"free -m: 查看内存，根据情况设置内存， swapon -s 查看虚拟内存情况 根据需要设置虚拟内存建立一个&#x2F;home&#x2F;swap的分区文件，大小为2G，可以自行倍增。 1dd if=/dev/zero of=/home/swap bs=1024 count=2048000 制作为swap格式文件 1mkswap /home/swap 再用swapon命令把这个文件分区挂载swap分区 1swapon /home/swap 设置文件权限 1chmod 600 /home/swap 为防止重启后swap分区变成0，要修改&#x2F;etc&#x2F;fstab文件，在文件内增加一行 1/home/swap swap swap default 0 0 最后查看内存设置情况 1free -m 删除虚拟内存-swap交换文件先停止swap分区 1swapoff /home/swap 删除swap分区文件 1rm -rf /home/swap 删除自动挂载配置命令，修改&#x2F;etc&#x2F;fstab，删除如下代码 1/home/swap swap swap default 0 0 这样就能把手动增加的交换文件删除了 注意：增加删除swap的操作只能使用root用户来操作。 装系统时分配的swap分区貌似删除不了。 swap分区一般为内存的2倍，但最大不超过2G","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"Centos","slug":"Linux/Centos","permalink":"https://blog.ifan.host/categories/Linux/Centos/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"https://blog.ifan.host/tags/Centos/"}]},{"title":"阿里云 Centos8 yum 报错","slug":"system/centos/centos8-aliyun-error","date":"2022-04-02T11:14:33.000Z","updated":"2023-12-25T05:56:09.181Z","comments":true,"path":"2022/04/02/system/centos/centos8-aliyun-error/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/centos/centos8-aliyun-error/","excerpt":"","text":"12345Repository extras is listed more than once in the configurationCentOS Linux 8 - AppStream 18 kB/s | 2.3 kB 00:00 Errors during downloading metadata for repository &#x27;appstream&#x27;: - Status code: 404 for http://mirrors.cloud.aliyuncs.com/centos/8/AppStream/x86_64/os/repodata/repomd.xml (IP: 100.100.2.148)Error: Failed to download metadata for repo &#x27;appstream&#x27;: Cannot download repomd.xml: Cannot download repodata/repomd.xml: All mirrors were tried 12345678910111213# 1、执行如下命令先将之前的yum文件备份：rename &#x27;.repo&#x27; &#x27;.repo.bak&#x27; /etc/yum.repos.d/*.repo # 2、运行以下命令下载最新的repo文件：wget https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo -O /etc/yum.repos.d/Centos-vault-8.5.2111.repowget https://mirrors.aliyun.com/repo/epel-archive-8.repo -O /etc/yum.repos.d/epel-archive-8.repo# 3、运行以下命令替换repo文件中的链接：sed -i &#x27;s/mirrors.cloud.aliyuncs.com/url_tmp/g&#x27; /etc/yum.repos.d/Centos-vault-8.5.2111.repo &amp;&amp; sed -i &#x27;s/mirrors.aliyun.com/mirrors.cloud.aliyuncs.com/g&#x27; /etc/yum.repos.d/Centos-vault-8.5.2111.repo &amp;&amp; sed -i &#x27;s/url_tmp/mirrors.aliyun.com/g&#x27; /etc/yum.repos.d/Centos-vault-8.5.2111.reposed -i &#x27;s/mirrors.aliyun.com/mirrors.cloud.aliyuncs.com/g&#x27; /etc/yum.repos.d/epel-archive-8.repo# 4、运行以下命令重新创建缓存,若没报错,则正常了。yum clean all &amp;&amp; yum makecache","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"Centos","slug":"Linux/Centos","permalink":"https://blog.ifan.host/categories/Linux/Centos/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"https://blog.ifan.host/tags/Centos/"}]},{"title":"crontab 定时任务","slug":"system/service/crontab","date":"2022-04-02T09:55:33.000Z","updated":"2023-12-25T05:56:09.181Z","comments":true,"path":"2022/04/02/system/service/crontab/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/service/crontab/","excerpt":"","text":"Crontab 定时任务查看日志 1tail -f /var/log/cron 1. 命令格式1crontab [-u user] file crontab [-u user] [ -e | -l | -r ] 2. 命令参数123456-u user：用来设定某个用户的crontab服务；file：file是命令文件的名字,表示将file做为crontab的任务列表文件并载入crontab。如果在命令行中没有指定这个文件，crontab命令将接受标准输入（键盘）上键入的命令，并将它们载入crontab。-e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。-l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。-r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。-i：在删除用户的crontab文件时给确认提示。 3. crontab的文件格式分 时 日 月 星期 要运行的命令 第1列分钟0～59 第2列小时0～23（0表示子夜） 第3列日1～31 第4列月1～12 第5列星期0～7（0和7表示星期天） 第6列要运行的命令 4. 使用实例实例1：每1分钟执行一次myCommand1* * * * * myCommand 实例2：每小时的第3和第15分钟执行13,15 * * * * myCommand 实例3：在上午8点到11点的第3和第15分钟执行13,15 8-11 * * * myCommand 实例4：每隔两天的上午8点到11点的第3和第15分钟执行13,15 8-11 */2 * * myCommand 实例5：每周一上午8点到11点的第3和第15分钟执行13,15 8-11 * * 1 myCommand 实例6：每晚的21:30重启smb130 21 * * * /etc/init.d/smb restart 实例7：每月1、10、22日的4 : 45重启smb145 4 1,10,22 * * /etc/init.d/smb restart 实例8：每周六、周日的1 : 10重启smb110 1 * * 6,0 /etc/init.d/smb restart 实例9：每天18 : 00至23 : 00之间每隔30分钟重启smb10,30 18-23 * * * /etc/init.d/smb restart 实例10：每星期六的晚上11 : 00 pm重启smb10 23 * * 6 /etc/init.d/smb restart 实例11：每一小时重启smb1* */1 * * * /etc/init.d/smb restart 实例12：晚上11点到早上7点之间，每隔一小时重启smb10 23-7 * * * /etc/init.d/smb restart 5. 使用注意事项注意环境变量问题有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。 在crontab文件中定义多个调度任务时，需要特别注环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点： 脚本中涉及文件路径时写全局路径； 脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如: 12345cat start_cbp.sh!/bin/shsource /etc/profileexport RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf/usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 当手动执行脚本OK，但是crontab死活不执行时,很可能是环境变量惹的祸，可尝试在crontab中直接引入环境变量解决问题。如: 10 * * * * . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 注意清理系统用户的邮件日志每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在crontab文件中设置如下形式，忽略日志输出: 10 */3 * * * /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 /dev/null 2&gt;&amp;1表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到&#x2F;dev&#x2F;null，这样日志输出问题就解决了。 系统级任务调度与用户级任务调度系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过crontab –uroot –e来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"service","slug":"Linux/service","permalink":"https://blog.ifan.host/categories/Linux/service/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"service","slug":"service","permalink":"https://blog.ifan.host/tags/service/"}]},{"title":"logroate 切割日志","slug":"system/service/logroate","date":"2022-04-02T09:54:33.000Z","updated":"2023-12-25T05:56:09.182Z","comments":true,"path":"2022/04/02/system/service/logroate/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/service/logroate/","excerpt":"","text":"logroate 切割日志Logrotate是基于CRON来运行的，其脚本是&#x2F;etc&#x2F;cron.daily&#x2F;logrotate，实际运行时，Logrotate会调用配置文件 /etc/logrotate.conf 重要参数说明 123456789101112131415161718192021222324252627282930compress #通过gzip 压缩转储以后的日志nocompress #不做gzip压缩处理copytruncate #用于还在打开中的日志文件，把当前日志备份并截断；是先拷贝再清空的方式，拷贝和清空之间有一个时间差，可能会丢失部分日志数据。nocopytruncate #备份日志文件不过不截断create mode owner group #轮转时指定创建新文件的属性，如create 0777 nobody nobodynocreate #不建立新的日志文件delaycompress #和compress 一起使用时，转储的日志文件到下一次转储时才压缩nodelaycompress #覆盖 delaycompress 选项，转储同时压缩。missingok #如果日志丢失，不报错继续滚动下一个日志errors address #专储时的错误信息发送到指定的Email 地址ifempty #即使日志文件为空文件也做轮转，这个是logrotate的缺省选项。notifempty #当日志文件为空时，不进行轮转mail address #把转储的日志文件发送到指定的E-mail 地址nomail #转储时不发送日志文件olddir directory #转储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统noolddir #转储后的日志文件和当前日志文件放在同一个目录下sharedscripts #运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本prerotate/endscript #在logrotate转储之前需要执行的指令，例如修改文件的属性等动作；必须独立成行postrotate/endscript #在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行daily #指定转储周期为每天weekly #指定转储周期为每周monthly #指定转储周期为每月rotate count #指定日志文件删除之前转储的次数，0 指没有备份，5 指保留5 个备份dateext #使用当期日期作为命名格式dateformat .%s #配合dateext使用，紧跟在下一行出现，定义文件切割后的文件名，必须配合dateext使用，只支持 %Y %m %d %s 这四个参数size(或minsize) log-size #当日志文件到达指定的大小时才转储，log-size能指定bytes(缺省)及KB (sizek)或MB(sizem).当日志文件 &gt;= log-size 的时候就转储。 以下为合法格式：（其他格式的单位大小写没有试过）size = 5 或 size 5 （&gt;= 5 个字节就转储）size = 100k 或 size 100ksize = 100M 或 size 100M 例子nginx 日志分割123456789101112131415vim /etc/logrotate.d/nginx/usr/local/nginx/logs/*.log &#123; daily # 指定转储周期为每天 rotate 7 # 保留7份 missingok # 如果日志丢失，不报错继续滚动下一个日志 notifempty # 当日志文件为空时，不进行轮转 dateext # 使用当期日期作为命名格式 sharedscripts # 运行postrotate脚本，作用是在所有日志都轮转后统一执行一次脚本。如果没有配置这个，那么每个日志轮转后都会执行一次脚本 postrotate # 在logrotate转储之后需要执行的指令，例如重新启动 (kill -HUP) 某个服务！必须独立成行 if [ -f /usr/local/nginx/logs/nginx.pid ]; then kill -USR1 `cat /usr/local/nginx/logs/nginx.pid` # ## 向 Nginx 主进程发送 USR1 信号。USR1 信号是重新打开日志文件 fi endscript&#125; 命令行123456logrotate [OPTION...] &lt;configfile&gt; -d, --debug ：debug模式，测试配置文件是否有错误。 -f, --force ：强制转储文件。 -m, --mail=command ：压缩日志后，发送日志到指定邮箱。 -s, --state=statefile ：使用指定的状态文件。 -v, --verbose ：显示转储过程。 手动触发日志轮转12logrotate -f /etc/logrotate.d/nginxlogrotate -f /etc/logrotate.d/php","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"service","slug":"Linux/service","permalink":"https://blog.ifan.host/categories/Linux/service/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"service","slug":"service","permalink":"https://blog.ifan.host/tags/service/"}]},{"title":"screen 分割控制台","slug":"system/service/screen","date":"2022-04-02T09:53:33.000Z","updated":"2023-12-25T05:56:09.182Z","comments":true,"path":"2022/04/02/system/service/screen/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/service/screen/","excerpt":"","text":"screen 分割控制台安装1sudo yum install screen 语法1screen [-AmRvx -ls -wipe][-d &lt;作业名称&gt;][-h &lt;行数&gt;][-r &lt;作业名称&gt;][-s ][-S &lt;作业名称&gt;] 选项12345678910111213-A # 将所有的视窗都调整为目前终端机的大小。-d &lt;作业名称&gt; # 将指定的screen作业离线。-h &lt;行数&gt; # 指定视窗的缓冲区行数。-m # 即使目前已在作业中的screen作业，仍强制建立新的screen作业。-r &lt;作业名称&gt; # 恢复离线的screen作业。-R # 先试图恢复离线的作业。若找不到离线的作业，即建立新的screen作业。-s # 指定建立新视窗时，所要执行的shell。-S &lt;作业名称&gt; # 指定screen作业的名称。-v # 显示版本信息。-x # 恢复之前离线的screen作业。-ls # 显示目前所有的screen作业。--list # 同 -ls-wipe # 检查目前所有的screen作业，并删除已经无法使用的screen作业。 常用参数12345screen -S yourname -&gt; 新建一个叫yourname的sessionscreen -ls -&gt; 列出当前所有的sessionscreen -r yourname -&gt; 回到yourname这个sessionscreen -d yourname -&gt; 远程detach某个sessionscreen -d -r yourname -&gt; 结束当前session并回到yourname这个session 键命令所有的命令都是以 Ctrl+a(C-a) 开始的 12345678910111213141516171819202122232425C-a ? -&gt; 显示所有键绑定信息C-a c -&gt; 创建一个新的运行shell的窗口并切换到该窗口C-a n -&gt; Next，切换到下一个 window C-a p -&gt; Previous，切换到前一个 window C-a 0..9 -&gt; 切换到第 0..9 个 windowCtrl+a [Space] -&gt; 由视窗0循序切换到视窗9C-a C-a -&gt; 在两个最近使用的 window 间切换 C-a x -&gt; 锁住当前的 window，需用用户密码解锁C-a d -&gt; detach，暂时离开当前session，将目前的 screen session (可能含有多个 windows) 丢到后台执行，并会回到还没进 screen 时的状态，此时在 screen session 里，每个 window 内运行的 process (无论是前台/后台)都在继续执行，即使 logout 也不影响。 C-a z -&gt; 把当前session放到后台执行，用 shell 的 fg 命令则可回去。C-a w -&gt; 显示所有窗口列表C-a t -&gt; time，显示当前时间，和系统的 load C-a k -&gt; kill window，强行关闭当前的 windowC-a [ -&gt; 进入 copy mode，在 copy mode 下可以回滚、搜索、复制就像用使用 vi 一样 C-b Backward，PageUp C-f Forward，PageDown H(大写) High，将光标移至左上角 L Low，将光标移至左下角 0 移到行首 $ 行末 w forward one word，以字为单位往前移 b backward one word，以字为单位往后移 Space 第一次按为标记区起点，第二次按为终点 Esc 结束 copy mode C-a ] -&gt; paste，把刚刚在 copy mode 选定的内容贴上","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"service","slug":"Linux/service","permalink":"https://blog.ifan.host/categories/Linux/service/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"service","slug":"service","permalink":"https://blog.ifan.host/tags/service/"}]},{"title":"supervisor 服务监控","slug":"system/service/supervisor","date":"2022-04-02T09:50:33.000Z","updated":"2023-12-25T05:56:09.182Z","comments":true,"path":"2022/04/02/system/service/supervisor/","link":"","permalink":"https://blog.ifan.host/2022/04/02/system/service/supervisor/","excerpt":"","text":"supervisor 服务进程监控1yum install -y supervisor 配置文件介绍配置文件: /etc/supervisord.conf 子进程配置文件路径: /etc/supervisord.d/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[unix_http_server]file=/var/run/supervisor/supervisor.sock ; UNIX socket 文件，supervisorctl 会使用;chmod=0700 ; socket文件的权限 (default 0700);chown=nobody:nogroup ; socket文件的组 uid:gid;username=user ; (default is no username (open server));password=123 ; (default is no password (open server));[inet_http_server] ; Http服务器，提供WEB管理界面;port=127.0.0.1:9001 ; 管理的IP和端口，注意安全性;username=user ; 管理后台的用户名;password=123 ; 管理后台的密码[supervisord]logfile=/var/log/supervisor/supervisord.log ; 日志文件logfile_maxbytes=50MB ; 日志文件的大小，默认为 50MB，设置为0则不限制大小logfile_backups=10 ; 日志文件保留备份数量，默认为 10，设置为0则不备份loglevel=info ; 日志级别pidfile=/var/run/supervisord.pid ; pid文件nodaemon=false ; 是否在前台启动，false -&gt; 以 daemon 方式启动minfds=1024 ; 可以打开的文件描述符的最小值minprocs=200 ; 可以打开的进程数的最小值;umask=022 ; (process file creation umask;default 022);user=chrism ; (default is current user, required if root);identifier=supervisor ; (supervisord identifier, default is &#x27;supervisor&#x27;);directory=/tmp ; (default is not to cd during start);nocleanup=true ; (don&#x27;t clean up tempfiles at start;default false);childlogdir=/tmp ; (&#x27;AUTO&#x27; child log dir, default $TEMP);environment=KEY=value ; (key value pairs to add to environment);strip_ansi=false ; (strip ansi escape codes in logs; def. false); the below section must remain in the config file for RPC; (supervisorctl/web interface) to work, additional interfaces may be; added by defining them in separate rpcinterface: sections[rpcinterface:supervisor]supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface[supervisorctl]serverurl=unix:///var/run/supervisor/supervisor.sock ; 通过UNIX socket连接supervisord，路径与unix_http_server部分的file一致;serverurl=http://127.0.0.1:9001 ; 通过HTTP链接;username=chris ; should be same as http_username if set;password=123 ; should be same as http_password if set;prompt=mysupervisor ; cmd line prompt (default &quot;supervisor&quot;);history_file=~/.sc_history ; use readline history if available; The below sample program section shows all possible program subsection values,; create one or more &#x27;real&#x27; program: sections to be able to control them under; supervisor.;[program:theprogramname] ; 需要管理的进程的名称;command=/bin/cat ; 程序启动的命令;process_name=%(program_name)s ; 进程名;numprocs=1 ; 需要启动的进程数量;directory=/tmp ; 脚本目录;umask=022 ; umask for process (default None);priority=999 ; 启动优先级 默认为999 值小的优先启动;autostart=true ; 在 supervisord 启动的时候启动 (default: true);autorestart=true ; 程序退出自动重启 (default: true);startsecs=10 ; 启动10秒后没有异常退出，就表示进程正常启动了，默认为1秒 (def. 1);startretries=3 ; 启动失败自动重试次数，默认是3 (default 3);exitcodes=0,2 ; &#x27;expected&#x27; exit codes for process (default 0,2);stopsignal=QUIT ; signal used to kill process (default TERM);stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10);user=chrism ; 用哪个用户启动;redirect_stderr=true ; 将stderr重定向到stdout (default false);stdout_logfile=/a/path ; 日志输出, NONE for none; default AUTO;stdout_logfile_maxbytes=1MB ; 日志文件大小 (default 50MB);stdout_logfile_backups=10 ; 日志的备份数量 (default 10);stdout_capture_maxbytes=1MB ; number of bytes in &#x27;capturemode&#x27; (default 0);stdout_events_enabled=false ; emit events on stdout writes (default false);stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB);stderr_logfile_backups=10 ; # of stderr logfile backups (default 10);stderr_capture_maxbytes=1MB ; number of bytes in &#x27;capturemode&#x27; (default 0);stderr_events_enabled=false ; emit events on stderr writes (default false);environment=A=1,B=2 ; 环境变量 (def no adds);serverurl=AUTO ; override serverurl computation (childutils); 对子进程的监控，当进程异常退出时告警;[eventlistener:theeventlistenername];command=/bin/eventlistener ; the program (relative uses PATH, can take args);process_name=%(program_name)s ; process_name expr (default %(program_name)s);numprocs=1 ; number of processes copies to start (def 1);events=EVENT ; event notif. types to subscribe to (req&#x27;d);buffer_size=10 ; event buffer queue size (default 10);directory=/tmp ; directory to cwd to before exec (def no cwd);umask=022 ; umask for process (default None);priority=-1 ; the relative start priority (default -1);autostart=true ; start at supervisord start (default: true);autorestart=unexpected ; restart at unexpected quit (default: unexpected);startsecs=10 ; number of secs prog must stay running (def. 1);startretries=3 ; max # of serial start failures (default 3);exitcodes=0,2 ; &#x27;expected&#x27; exit codes for process (default 0,2);stopsignal=QUIT ; signal used to kill process (default TERM);stopwaitsecs=10 ; max num secs to wait b4 SIGKILL (default 10);user=chrism ; setuid to this UNIX account to run the program;redirect_stderr=true ; redirect proc stderr to stdout (default false);stdout_logfile=/a/path ; stdout log path, NONE for none; default AUTO;stdout_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB);stdout_logfile_backups=10 ; # of stdout logfile backups (default 10);stdout_events_enabled=false ; emit events on stdout writes (default false);stderr_logfile=/a/path ; stderr log path, NONE for none; default AUTO;stderr_logfile_maxbytes=1MB ; max # logfile bytes b4 rotation (default 50MB);stderr_logfile_backups ; # of stderr logfile backups (default 10);stderr_events_enabled=false ; emit events on stderr writes (default false);environment=A=1,B=2 ; process environment additions;serverurl=AUTO ; override serverurl computation (childutils); 将多个文件组合成一个组;[group:thegroupname];programs=progname1,progname2 ; each refers to &#x27;x&#x27; in [program:x] definitions;priority=999 ; the relative start priority (default 999); 包含其他文件[include]files = supervisord.d/*.ini 实例管理 Gogs123456789101112131415directory=/home/git/go/src/github.com/gogits/gogs/command=/home/git/go/src/github.com/gogits/gogs/gogs webautostart=trueautorestart=truestartsecs=10stdout_logfile=/var/log/gogs/stdout.logstdout_logfile_maxbytes=1MBstdout_logfile_backups=10stdout_capture_maxbytes=1MBstderr_logfile=/var/log/gogs/stderr.logstderr_logfile_maxbytes=1MBstderr_logfile_backups=10stderr_capture_maxbytes=1MBuser = gitenvironment = HOME=&quot;/home/git&quot;, USER=&quot;git&quot; 管理MySQL12345678[program:mysql]command=/opt/mysql/bin/mysqld_safe --defaults-file=/opt/mysql/conf/my.iniuser=ifanautostart=trueautorestart=truedirectory=/home/ifan/software/mysqlstderr_logfile=/var/mysql/logs/err.logstdout_logfile=/var/mysql/logs/out.log 管理redis注意需要关闭daemonize no后台守护进程 12345678[program:redis]command=/opt/redis/bin/redis-server /opt/redis/conf/redis.confuser=ifanautostart=trueautorestart=truedirectory=/opt/redis/stderr_logfile=/var/redis/log/err.logstdout_logfile=/var/redis/log/out.log CMD123456789101112131415161718192021# 设置为开机自启systemctl enable supervisord# 启动systemctl start supervisord.service# 查看状态systemctl status supervisord.service# 停止systemctl stop supervisord.service# 重启systemctl restart supervisord.service# 重新加载配置文件supervisorctl reload# 查看进程supervisorctl status# 启动某个进程supervisorctl start xxxx# 停止某个进程supervisorctl stop xxxx# 重启某个进程supervisorctl restart xxxx 报错无法重启stackoverflow 1234567891011121314● supervisord.service - Process Monitoring and Control DaemonLoaded: loaded (/usr/lib/systemd/system/supervisord.service; enabled; vendor preset: disabled)Active: failed (Result: exit-code) since Thu 2021-04-15 22:49:51 CST; 6s agoProcess: 27052 ExecStart=/usr/bin/supervisord -c /etc/supervisord.conf (code=exited, status=2)Apr 15 22:49:50 iFan systemd[1]: Starting Process Monitoring and Control Daemon...Apr 15 22:49:51 iFan supervisord[27052]: Error: Another program is already listening on a port that one of our HTT...isord.Apr 15 22:49:51 iFan supervisord[27052]: For help, use /usr/bin/supervisord -hApr 15 22:49:51 iFan systemd[1]: supervisord.service: control process exited, code=exited status=2Apr 15 22:49:51 iFan systemd[1]: Failed to start Process Monitoring and Control Daemon.Apr 15 22:49:51 iFan systemd[1]: Unit supervisord.service entered failed state.Apr 15 22:49:51 iFan systemd[1]: supervisord.service failed.Hint: Some lines were ellipsized, use -l to show in full. 1unlink /var/run/supervisor/supervisor.sock","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"service","slug":"Linux/service","permalink":"https://blog.ifan.host/categories/Linux/service/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"service","slug":"service","permalink":"https://blog.ifan.host/tags/service/"}]},{"title":"Wrk压测","slug":"web/nginx/wrk","date":"2022-04-02T08:03:29.000Z","updated":"2023-12-25T05:56:09.188Z","comments":true,"path":"2022/04/02/web/nginx/wrk/","link":"","permalink":"https://blog.ifan.host/2022/04/02/web/nginx/wrk/","excerpt":"","text":"Wrk压测下载安装12git clone https://github.com/wg/wrkmake mac 1brew install wrk 命令查看 1234567891011121314Usage: wrk &lt;options&gt; &lt;url&gt; Options: -c, --connections &lt;N&gt; 连接数 -d, --duration &lt;T&gt; 持续时间 -t, --threads &lt;N&gt; 线程数 -s, --script &lt;S&gt; 制定lua脚本 -H, --header &lt;H&gt; 添加请求头 --latency 打印延迟分布信息 --timeout &lt;T&gt; 设置请求超时 -v, --version 打印版本信息 &lt;N&gt;表示数字参数，支持国际单位 (1k, 1M, 1G) &lt;T&gt;表示时间参数，支持国际单位 (2s, 2m, 2h) 压测结果 12345678910111213141516Running 30s test @ http://www.bing.com （压测时间30s） 8 threads and 200 connections （共8个测试线程，200个连接） Thread Stats Avg Stdev Max +/- Stdev （平均值） （标准差）（最大值）（正负一个标准差所占比例） Latency 46.67ms 215.38ms 1.67s 95.59% （延迟） Req/Sec 7.91k 1.15k 10.26k 70.77% （处理中的请求数） Latency Distribution （延迟分布） 50% 2.93ms 75% 3.78ms 90% 4.73ms 99% 1.35s （99分位的延迟） 1790465 requests in 30.01s, 684.08MB read （30.01秒内共处理完成了1790465个请求，读取了684.08MB数据）Requests/sec: 59658.29 （平均每秒处理完成59658.29个请求）Transfer/sec: 22.79MB （平均每秒读取数据22.79MB） Lua 脚本启动阶段、运行阶段和结束阶段 启动阶段1function setup(thread) 在脚本文件中实现setup方法，wrk就会在测试线程已经初始化但还没有启动的时候调用该方法。wrk会为每一个测试线程调用一次setup方法，并传入代表测试线程的对象thread作为参数。setup方法中可操作该thread对象，获取信息、存储信息、甚至关闭该线程。 1234thread.addr - get or set the thread&#x27;s server addressthread:get(name) - get the value of a global in the thread&#x27;s envthread:set(name, value) - set the value of a global in the thread&#x27;s envthread:stop() - stop the thread 运行阶段1234function init(args)function delay()function request()function response(status, headers, body) init: 由测试线程调用，只会在进入运行阶段时，调用一次。支持从启动wrk的命令中，获取命令行参数； delay: 在每次发送request之前调用，如果需要delay，那么delay相应时间； request: 用来生成请求；每一次请求都会调用该方法，所以注意不要在该方法中做耗时的操作； reponse: 在每次收到一个响应时调用；为提升性能，如果没有定义该方法，那么wrk不会解析headers和body； 结束阶段1function done(summary, latency, requests) 该方法在整个测试过程中只会调用一次，可从参数给定的对象中，获取压测结果，生成定制化的测试报告。 自定义脚本中可访问的变量和方法变量：wrk 12345678910wrk = &#123; scheme = &quot;http&quot;, host = &quot;localhost&quot;, port = nil, method = &quot;GET&quot;, path = &quot;/&quot;, headers = &#123;&#125;, body = nil, thread = &lt;userdata&gt;, &#125; 一个table类型的变量wrk，是全局变量，修改该table，会影响所有请求。 1234567891011121314151617function wrk.format(method, path, headers, body) wrk.format returns a HTTP request string containing the passed parameters merged with values from the wrk table. 根据参数和全局变量wrk，生成一个HTTP rquest string。function wrk.lookup(host, service) wrk.lookup returns a table containing all known addresses for the host and service pair. This corresponds to the POSIX getaddrinfo() function. 给定host和service（port/well known service name），返回所有可用的服务器地址信息。function wrk.connect(addr) wrk.connect returns true if the address can be connected to, otherwise it returns false. The address must be one returned from wrk.lookup(). 测试与给定的服务器地址信息是否可以成功创建连接 使用POST METHOD 123wrk.method = &quot;POST&quot;wrk.body = &quot;foo=bar&amp;baz=quux&quot;wrk.headers[&quot;Content-Type&quot;] = &quot;application/x-www-form-urlencoded&quot; 通过修改全局变量wrk，使得所有请求都使用POST方法，并指定了body和Content-Type头。 为每次request更换一个参数 12345request = function() uid = math.random(1, 10000000) path = &quot;/test?uid=&quot; .. uid return wrk.format(nil, path)end 通过在request方法中随机生成1~10000000之间的uid，使得请求中的uid参数随机。 每次请求之前延迟10ms 123function delay() return 10end 每个线程要先进行认证，认证之后获取token以进行压测 1234567891011121314token = nilpath = &quot;/authenticate&quot;request = function() return wrk.format(&quot;GET&quot;, path)endresponse = function(status, headers, body) if not token and status == 200 then token = headers[&quot;X-Token&quot;] path = &quot;/resource&quot; wrk.headers[&quot;X-Token&quot;] = token endend 在没有token的情况下，先访问authenticate认证。认证成功后，读取token并替换path为resource。 压测支持HTTP pipeline的服务 123456789101112init = function(args) local r = &#123;&#125; r[1] = wrk.format(nil, &quot;/?foo&quot;) r[2] = wrk.format(nil, &quot;/?bar&quot;) r[3] = wrk.format(nil, &quot;/?baz&quot;) req = table.concat(r)endrequest = function() return reqend 通过在init方法中将三个HTTP request请求拼接在一起，实现每次发送三个请求，以使用HTTP pipeline。 上传文件 12345678910111213wrk.method = &quot;POST&quot;wrk.headers[&quot;Content-Type&quot;] = &quot;multipart/form-data;boundary=------WebKitFormBoundaryX3bY6PBMcxB1vCan&quot;file = io.open(&quot;path/to/fake.jpg&quot;, &quot;rb&quot;)-- 拼装form-dataform = &quot;------WebKitFormBoundaryX3bY6PBMcxB1vCan\\r\\n&quot;form = form .. &quot;Content-Disposition: form-data; name=&quot;file&quot;; filename=&quot;fake.jpg&quot;\\r\\n&quot;form = form .. &quot;Content-Type: image/jpeg\\r\\n\\r\\n&quot;form = form .. file:read(&quot;*a&quot;)form = form .. &quot;\\r\\n------WebKitFormBoundaryX3bY6PBMcxB1vCan--&quot;wrk.body = form","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"压测","slug":"WEB/压测","permalink":"https://blog.ifan.host/categories/WEB/%E5%8E%8B%E6%B5%8B/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Wrk","slug":"Wrk","permalink":"https://blog.ifan.host/tags/Wrk/"}]},{"title":"Http Content-Type","slug":"web/http/http-content-type","date":"2022-04-02T07:55:25.000Z","updated":"2023-12-25T05:56:09.186Z","comments":true,"path":"2022/04/02/web/http/http-content-type/","link":"","permalink":"https://blog.ifan.host/2022/04/02/web/http/http-content-type/","excerpt":"","text":"Content-type 文件扩展名 Content-Type(Mime-Type) 文件扩展名 Content-Type(Mime-Type) .*（ 二进制流，不知道下载文件类型） application&#x2F;octet-stream .tif image&#x2F;tiff .001 application&#x2F;x-001 .301 application&#x2F;x-301 .323 text&#x2F;h323 .906 application&#x2F;x-906 .907 drawing&#x2F;907 .a11 application&#x2F;x-a11 .acp audio&#x2F;x-mei-aac .ai application&#x2F;postscript .aif audio&#x2F;aiff .aifc audio&#x2F;aiff .aiff audio&#x2F;aiff .anv application&#x2F;x-anv .asa text&#x2F;asa .asf video&#x2F;x-ms-asf .asp text&#x2F;asp .asx video&#x2F;x-ms-asf .au audio&#x2F;basic .avi video&#x2F;avi .awf application&#x2F;vnd.adobe.workflow .biz text&#x2F;xml .bmp application&#x2F;x-bmp .bot application&#x2F;x-bot .c4t application&#x2F;x-c4t .c90 application&#x2F;x-c90 .cal application&#x2F;x-cals .cat application&#x2F;vnd.ms-pki.seccat .cdf application&#x2F;x-netcdf .cdr application&#x2F;x-cdr .cel application&#x2F;x-cel .cer application&#x2F;x-x509-ca-cert .cg4 application&#x2F;x-g4 .cgm application&#x2F;x-cgm .cit application&#x2F;x-cit .class java&#x2F;* .cml text&#x2F;xml .cmp application&#x2F;x-cmp .cmx application&#x2F;x-cmx .cot application&#x2F;x-cot .crl application&#x2F;pkix-crl .crt application&#x2F;x-x509-ca-cert .csi application&#x2F;x-csi .css text&#x2F;css .cut application&#x2F;x-cut .dbf application&#x2F;x-dbf .dbm application&#x2F;x-dbm .dbx application&#x2F;x-dbx .dcd text&#x2F;xml .dcx application&#x2F;x-dcx .der application&#x2F;x-x509-ca-cert .dgn application&#x2F;x-dgn .dib application&#x2F;x-dib .dll application&#x2F;x-msdownload .doc application&#x2F;msword .dot application&#x2F;msword .drw application&#x2F;x-drw .dtd text&#x2F;xml .dwf Model&#x2F;vnd.dwf .dwf application&#x2F;x-dwf .dwg application&#x2F;x-dwg .dxb application&#x2F;x-dxb .dxf application&#x2F;x-dxf .edn application&#x2F;vnd.adobe.edn .emf application&#x2F;x-emf .eml message&#x2F;rfc822 .ent text&#x2F;xml .epi application&#x2F;x-epi .eps application&#x2F;x-ps .eps application&#x2F;postscript .etd application&#x2F;x-ebx .exe application&#x2F;x-msdownload .fax image&#x2F;fax .fdf application&#x2F;vnd.fdf .fif application&#x2F;fractals .fo text&#x2F;xml .frm application&#x2F;x-frm .g4 application&#x2F;x-g4 .gbr application&#x2F;x-gbr . application&#x2F;x- .gif image&#x2F;gif .gl2 application&#x2F;x-gl2 .gp4 application&#x2F;x-gp4 .hgl application&#x2F;x-hgl .hmr application&#x2F;x-hmr .hpg application&#x2F;x-hpgl .hpl application&#x2F;x-hpl .hqx application&#x2F;mac-binhex40 .hrf application&#x2F;x-hrf .hta application&#x2F;hta .htc text&#x2F;x-component .htm text&#x2F;html .html text&#x2F;html .htt text&#x2F;webviewhtml .htx text&#x2F;html .icb application&#x2F;x-icb .ico image&#x2F;x-icon .ico application&#x2F;x-ico .iff application&#x2F;x-iff .ig4 application&#x2F;x-g4 .igs application&#x2F;x-igs .iii application&#x2F;x-iphone .img application&#x2F;x-img .ins application&#x2F;x-internet-signup .isp application&#x2F;x-internet-signup .IVF video&#x2F;x-ivf .java java&#x2F;* .jfif image&#x2F;jpeg .jpe image&#x2F;jpeg .jpe application&#x2F;x-jpe .jpeg image&#x2F;jpeg .jpg image&#x2F;jpeg .jpg application&#x2F;x-jpg .js application&#x2F;x-javascript .jsp text&#x2F;html .la1 audio&#x2F;x-liquid-file .lar application&#x2F;x-laplayer-reg .latex application&#x2F;x-latex .lavs audio&#x2F;x-liquid-secure .lbm application&#x2F;x-lbm .lmsff audio&#x2F;x-la-lms .ls application&#x2F;x-javascript .ltr application&#x2F;x-ltr .m1v video&#x2F;x-mpeg .m2v video&#x2F;x-mpeg .m3u audio&#x2F;mpegurl .m4e video&#x2F;mpeg4 .mac application&#x2F;x-mac .man application&#x2F;x-troff-man .math text&#x2F;xml .mdb application&#x2F;msaccess .mdb application&#x2F;x-mdb .mfp application&#x2F;x-shockwave-flash .mht message&#x2F;rfc822 .mhtml message&#x2F;rfc822 .mi application&#x2F;x-mi .mid audio&#x2F;mid .midi audio&#x2F;mid .mil application&#x2F;x-mil .mml text&#x2F;xml .mnd audio&#x2F;x-musicnet-download .mns audio&#x2F;x-musicnet-stream .mocha application&#x2F;x-javascript .movie video&#x2F;x-sgi-movie .mp1 audio&#x2F;mp1 .mp2 audio&#x2F;mp2 .mp2v video&#x2F;mpeg .mp3 audio&#x2F;mp3 .mp4 video&#x2F;mpeg4 .mpa video&#x2F;x-mpg .mpd application&#x2F;vnd.ms-project .mpe video&#x2F;x-mpeg .mpeg video&#x2F;mpg .mpg video&#x2F;mpg .mpga audio&#x2F;rn-mpeg .mpp application&#x2F;vnd.ms-project .mps video&#x2F;x-mpeg .mpt application&#x2F;vnd.ms-project .mpv video&#x2F;mpg .mpv2 video&#x2F;mpeg .mpw application&#x2F;vnd.ms-project .mpx application&#x2F;vnd.ms-project .mtx text&#x2F;xml .mxp application&#x2F;x-mmxp .net image&#x2F;pnetvue .nrf application&#x2F;x-nrf .nws message&#x2F;rfc822 .odc text&#x2F;x-ms-odc .out application&#x2F;x-out .p10 application&#x2F;pkcs10 .p12 application&#x2F;x-pkcs12 .p7b application&#x2F;x-pkcs7-certificates .p7c application&#x2F;pkcs7-mime .p7m application&#x2F;pkcs7-mime .p7r application&#x2F;x-pkcs7-certreqresp .p7s application&#x2F;pkcs7-signature .pc5 application&#x2F;x-pc5 .pci application&#x2F;x-pci .pcl application&#x2F;x-pcl .pcx application&#x2F;x-pcx .pdf application&#x2F;pdf .pdf application&#x2F;pdf .pdx application&#x2F;vnd.adobe.pdx .pfx application&#x2F;x-pkcs12 .pgl application&#x2F;x-pgl .pic application&#x2F;x-pic .pko application&#x2F;vnd.ms-pki.pko .pl application&#x2F;x-perl .plg text&#x2F;html .pls audio&#x2F;scpls .plt application&#x2F;x-plt .png image&#x2F;png .png application&#x2F;x-png .pot application&#x2F;vnd.ms-powerpoint .ppa application&#x2F;vnd.ms-powerpoint .ppm application&#x2F;x-ppm .pps application&#x2F;vnd.ms-powerpoint .ppt application&#x2F;vnd.ms-powerpoint .ppt application&#x2F;x-ppt .pr application&#x2F;x-pr .prf application&#x2F;pics-rules .prn application&#x2F;x-prn .prt application&#x2F;x-prt .ps application&#x2F;x-ps .ps application&#x2F;postscript .ptn application&#x2F;x-ptn .pwz application&#x2F;vnd.ms-powerpoint .r3t text&#x2F;vnd.rn-realtext3d .ra audio&#x2F;vnd.rn-realaudio .ram audio&#x2F;x-pn-realaudio .ras application&#x2F;x-ras .rat application&#x2F;rat-file .rdf text&#x2F;xml .rec application&#x2F;vnd.rn-recording .red application&#x2F;x-red .rgb application&#x2F;x-rgb .rjs application&#x2F;vnd.rn-realsystem-rjs .rjt application&#x2F;vnd.rn-realsystem-rjt .rlc application&#x2F;x-rlc .rle application&#x2F;x-rle .rm application&#x2F;vnd.rn-realmedia .rmf application&#x2F;vnd.adobe.rmf .rmi audio&#x2F;mid .rmj application&#x2F;vnd.rn-realsystem-rmj .rmm audio&#x2F;x-pn-realaudio .rmp application&#x2F;vnd.rn-rn_music_package .rms application&#x2F;vnd.rn-realmedia-secure .rmvb application&#x2F;vnd.rn-realmedia-vbr .rmx application&#x2F;vnd.rn-realsystem-rmx .rnx application&#x2F;vnd.rn-realplayer .rp image&#x2F;vnd.rn-realpix .rpm audio&#x2F;x-pn-realaudio-plugin .rsml application&#x2F;vnd.rn-rsml .rt text&#x2F;vnd.rn-realtext .rtf application&#x2F;msword .rtf application&#x2F;x-rtf .rv video&#x2F;vnd.rn-realvideo .sam application&#x2F;x-sam .sat application&#x2F;x-sat .sdp application&#x2F;sdp .sdw application&#x2F;x-sdw .sit application&#x2F;x-stuffit .slb application&#x2F;x-slb .sld application&#x2F;x-sld .slk drawing&#x2F;x-slk .smi application&#x2F;smil .smil application&#x2F;smil .smk application&#x2F;x-smk .snd audio&#x2F;basic .sol text&#x2F;plain .sor text&#x2F;plain .spc application&#x2F;x-pkcs7-certificates .spl application&#x2F;futuresplash .spp text&#x2F;xml .ssm application&#x2F;streamingmedia .sst application&#x2F;vnd.ms-pki.certstore .stl application&#x2F;vnd.ms-pki.stl .stm text&#x2F;html .sty application&#x2F;x-sty .svg text&#x2F;xml .swf application&#x2F;x-shockwave-flash .tdf application&#x2F;x-tdf .tg4 application&#x2F;x-tg4 .tga application&#x2F;x-tga .tif image&#x2F;tiff .tif application&#x2F;x-tif .tiff image&#x2F;tiff .tld text&#x2F;xml .top drawing&#x2F;x-top .torrent application&#x2F;x-bittorrent .tsd text&#x2F;xml .txt text&#x2F;plain .uin application&#x2F;x-icq .uls text&#x2F;iuls .vcf text&#x2F;x-vcard .vda application&#x2F;x-vda .vdx application&#x2F;vnd.visio .vml text&#x2F;xml .vpg application&#x2F;x-vpeg005 .vsd application&#x2F;vnd.visio .vsd application&#x2F;x-vsd .vss application&#x2F;vnd.visio .vst application&#x2F;vnd.visio .vst application&#x2F;x-vst .vsw application&#x2F;vnd.visio .vsx application&#x2F;vnd.visio .vtx application&#x2F;vnd.visio .vxml text&#x2F;xml .wav audio&#x2F;wav .wax audio&#x2F;x-ms-wax .wb1 application&#x2F;x-wb1 .wb2 application&#x2F;x-wb2 .wb3 application&#x2F;x-wb3 .wbmp image&#x2F;vnd.wap.wbmp .wiz application&#x2F;msword .wk3 application&#x2F;x-wk3 .wk4 application&#x2F;x-wk4 .wkq application&#x2F;x-wkq .wks application&#x2F;x-wks .wm video&#x2F;x-ms-wm .wma audio&#x2F;x-ms-wma .wmd application&#x2F;x-ms-wmd .wmf application&#x2F;x-wmf .wml text&#x2F;vnd.wap.wml .wmv video&#x2F;x-ms-wmv .wmx video&#x2F;x-ms-wmx .wmz application&#x2F;x-ms-wmz .wp6 application&#x2F;x-wp6 .wpd application&#x2F;x-wpd .wpg application&#x2F;x-wpg .wpl application&#x2F;vnd.ms-wpl .wq1 application&#x2F;x-wq1 .wr1 application&#x2F;x-wr1 .wri application&#x2F;x-wri .wrk application&#x2F;x-wrk .ws application&#x2F;x-ws .ws2 application&#x2F;x-ws .wsc text&#x2F;scriptlet .wsdl text&#x2F;xml .wvx video&#x2F;x-ms-wvx .xdp application&#x2F;vnd.adobe.xdp .xdr text&#x2F;xml .xfd application&#x2F;vnd.adobe.xfd .xfdf application&#x2F;vnd.adobe.xfdf .xhtml text&#x2F;html .xls application&#x2F;vnd.ms-excel .xls application&#x2F;x-xls .xlw application&#x2F;x-xlw .xml text&#x2F;xml .xpl audio&#x2F;scpls .xq text&#x2F;xml .xql text&#x2F;xml .xquery text&#x2F;xml .xsd text&#x2F;xml .xsl text&#x2F;xml .xslt text&#x2F;xml .xwd application&#x2F;x-xwd .x_b application&#x2F;x-x_b .sis application&#x2F;vnd.symbian.install .sisx application&#x2F;vnd.symbian.install .x_t application&#x2F;x-x_t .ipa application&#x2F;vnd.iphone .apk application&#x2F;vnd.android.package-archive .xap application&#x2F;x-silverlight-app","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"HTTP","slug":"WEB/HTTP","permalink":"https://blog.ifan.host/categories/WEB/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/tags/WEB/"}]},{"title":"Http 状态码","slug":"web/http/http-code","date":"2022-04-02T07:52:33.000Z","updated":"2023-12-25T05:56:09.186Z","comments":true,"path":"2022/04/02/web/http/http-code/","link":"","permalink":"https://blog.ifan.host/2022/04/02/web/http/http-code/","excerpt":"","text":"HTTP 状态码一些常见的状态码为： 200 服务器成功返回网页 404 请求的网页不存在 503 服务不可用 1xx（临时响应）表示临时响应并需要请求者继续执行操作的状态代码。 状态码 类型 说明 100 继续 请求者应当继续提出请求。服务器返回此代码表示已收到请求的第一部分，正在等待其余部分。 101 切换协议 请求者已要求服务器切换协议，服务器已确认并准备切换。 2xx （成功）表示成功处理了请求的状态代码。 状态码 类型 说明 200 成功 服务器已成功处理了请求。通常，这表示服务器提供了请求的网页。 201 已创建 请求成功并且服务器创建了新的资源。 202 已接受 服务器已接受请求，但尚未处理。 203 非授权信息 服务器已成功处理了请求，但返回的信息可能来自另一来源。 204 无内容 服务器成功处理了请求，但没有返回任何内容。 205 重置内容 服务器成功处理了请求，但没有返回任何内容。 206 部分内容 服务器成功处理了部分 GET 请求。 3xx （重定向）表示要完成请求，需要进一步操作。 通常，这些状态代码用来重定向。 状态码 类型 说明 300 多种选择 针对请求，服务器可执行多种操作。服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 301 永久移动 请求的网页已永久移动到新位置。服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。 302 临时移动 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 303 查看其他位置 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。 304 未修改 自从上次请求后，请求的网页未修改过。服务器返回此响应时，不会返回网页内容。 305 使用代理 请求者只能使用代理访问请求的网页。如果服务器返回此响应，还表示请求者应使用代理。 307 临时重定向 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 4xx（请求错误）这些状态代码表示请求可能出错，妨碍了服务器的处理。 状态码 类型 说明 400 错误请求 服务器不理解请求的语法。 401 未授权 请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 403 禁止 服务器拒绝请求。 404 未找到 服务器找不到请求的网页。 405 方法禁用 禁用请求中指定的方法。 406 不接受 无法使用请求的内容特性响应请求的网页。 407 需要代理授权 此状态代码与 401（未授权）类似，但指定请求者应当授权使用代理。 408 请求超时 服务器等候请求时发生超时。 409 冲突 服务器在完成请求时发生冲突。服务器必须在响应中包含有关冲突的信息。 410 已删除 如果请求的资源已永久删除，服务器就会返回此响应。 411 需要有效长度 服务器不接受不含有效内容长度标头字段的请求。 412 未满足前提条件 服务器未满足请求者在请求中设置的其中一个前提条件。 413 请求实体过大 服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 414 请求的 URI 过长 请求的 URI（通常为网址）过长，服务器无法处理。 415 不支持的媒体类型 请求的格式不受请求页面的支持。 416 请求范围不符合要求 如果页面无法提供请求的范围，则服务器会返回此状态代码。 417 未满足期望值 服务器未满足”期望”请求标头字段的要求。 5xx（服务器错误）这些状态代码表示服务器在尝试处理请求时发生内部错误。 这些错误可能是服务器本身的错误，而不是请求出错。 状态码 类型 说明 500 服务器内部错误 服务器遇到错误，无法完成请求。 501 尚未实施 服务器不具备完成请求的功能。例如，服务器无法识别请求方法时可能会返回此代码。 502 错误网关 服务器作为网关或代理，从上游服务器收到无效响应。 503 服务不可用 服务器目前无法使用（由于超载或停机维护）。通常，这只是暂时状态。 504 网关超时 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505 HTTP 版本不受支持 服务器不支持请求中所用的 HTTP 协议版本。","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"HTTP","slug":"WEB/HTTP","permalink":"https://blog.ifan.host/categories/WEB/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/tags/WEB/"}]},{"title":"Http 方法及使用场景","slug":"web/http/http-method-use","date":"2022-04-02T07:52:30.000Z","updated":"2023-12-25T05:56:09.186Z","comments":true,"path":"2022/04/02/web/http/http-method-use/","link":"","permalink":"https://blog.ifan.host/2022/04/02/web/http/http-method-use/","excerpt":"","text":"方法 描述 使用场景 请求是否有BODY 响应是否有BODY GET 获取指定资源 获取网页，查询资源 没有 有 POST 发送数据给服务端 新建资源；用户通过表单登录 有 有 PUT 新建资源或者替换指定资源 更新一条记录 有 有 DELETE 删除指定资源 删除一条记录 有 有 OPTIONS 用于描述特定资源的访问选项 获取服务端支持的请求方法 没有 有 CONNECT 开启双向通信 undefined 没有 有 HEAD 没有 没有 PATCH 对资源部分更改 有 没有 TRACE 有 有","categories":[{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"HTTP","slug":"WEB/HTTP","permalink":"https://blog.ifan.host/categories/WEB/HTTP/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/tags/WEB/"}]}],"categories":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/categories/Sqlalchemy/"},{"name":"ORM","slug":"Sqlalchemy/ORM","permalink":"https://blog.ifan.host/categories/Sqlalchemy/ORM/"},{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Docker","slug":"工具/Docker","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Docker/"},{"name":"Minio","slug":"Minio","permalink":"https://blog.ifan.host/categories/Minio/"},{"name":"Mac","slug":"工具/Mac","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Mac/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"},{"name":"NodeJS","slug":"NodeJS","permalink":"https://blog.ifan.host/categories/NodeJS/"},{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/"},{"name":"frida","slug":"爬虫/frida","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/frida/"},{"name":"编程工具","slug":"工具/编程工具","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"},{"name":"生活","slug":"工具/生活","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/%E7%94%9F%E6%B4%BB/"},{"name":"大数据","slug":"数据库/大数据","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"HBase","slug":"数据库/大数据/HBase","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%A4%A7%E6%95%B0%E6%8D%AE/HBase/"},{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/categories/WEB/"},{"name":"Nginx","slug":"WEB/Nginx","permalink":"https://blog.ifan.host/categories/WEB/Nginx/"},{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/categories/Java/"},{"name":"Spring Boot","slug":"Java/Spring-Boot","permalink":"https://blog.ifan.host/categories/Java/Spring-Boot/"},{"name":"MyBatis","slug":"Java/MyBatis","permalink":"https://blog.ifan.host/categories/Java/MyBatis/"},{"name":"ElasticSearch","slug":"数据库/ElasticSearch","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/ElasticSearch/"},{"name":"MongoDB","slug":"数据库/MongoDB","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MongoDB/"},{"name":"Scala","slug":"Scala","permalink":"https://blog.ifan.host/categories/Scala/"},{"name":"基础知识","slug":"Java/基础知识","permalink":"https://blog.ifan.host/categories/Java/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/categories/golang/"},{"name":"Sqlite","slug":"数据库/Sqlite","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Sqlite/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://blog.ifan.host/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"},{"name":"python","slug":"python","permalink":"https://blog.ifan.host/categories/python/"},{"name":"lxml","slug":"爬虫/lxml","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/lxml/"},{"name":"xpath","slug":"爬虫/xpath","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/xpath/"},{"name":"boto3","slug":"boto3","permalink":"https://blog.ifan.host/categories/boto3/"},{"name":"文件存储","slug":"boto3/文件存储","permalink":"https://blog.ifan.host/categories/boto3/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"name":"mitmproxy","slug":"爬虫/mitmproxy","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/mitmproxy/"},{"name":"puppeteer","slug":"爬虫/puppeteer","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/puppeteer/"},{"name":"pyppeteer","slug":"爬虫/pyppeteer","permalink":"https://blog.ifan.host/categories/%E7%88%AC%E8%99%AB/pyppeteer/"},{"name":"proxy","slug":"工具/proxy","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/proxy/"},{"name":"爬虫","slug":"工具/proxy/爬虫","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/proxy/%E7%88%AC%E8%99%AB/"},{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/categories/Python/"},{"name":"jupyterlab","slug":"Python/jupyterlab","permalink":"https://blog.ifan.host/categories/Python/jupyterlab/"},{"name":"nvm","slug":"NodeJS/nvm","permalink":"https://blog.ifan.host/categories/NodeJS/nvm/"},{"name":"Jupyter","slug":"Python/Jupyter","permalink":"https://blog.ifan.host/categories/Python/Jupyter/"},{"name":"pyenv","slug":"Python/pyenv","permalink":"https://blog.ifan.host/categories/Python/pyenv/"},{"name":"Lua","slug":"Lua","permalink":"https://blog.ifan.host/categories/Lua/"},{"name":"HTTP","slug":"WEB/HTTP","permalink":"https://blog.ifan.host/categories/WEB/HTTP/"},{"name":"vim","slug":"工具/vim","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/vim/"},{"name":"ffmpeg","slug":"工具/ffmpeg","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/ffmpeg/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/categories/Linux/"},{"name":"Centos","slug":"Linux/Centos","permalink":"https://blog.ifan.host/categories/Linux/Centos/"},{"name":"Git","slug":"工具/Git","permalink":"https://blog.ifan.host/categories/%E5%B7%A5%E5%85%B7/Git/"},{"name":"Debian","slug":"Linux/Debian","permalink":"https://blog.ifan.host/categories/Linux/Debian/"},{"name":"service","slug":"Linux/service","permalink":"https://blog.ifan.host/categories/Linux/service/"},{"name":"压测","slug":"WEB/压测","permalink":"https://blog.ifan.host/categories/WEB/%E5%8E%8B%E6%B5%8B/"}],"tags":[{"name":"Sqlalchemy","slug":"Sqlalchemy","permalink":"https://blog.ifan.host/tags/Sqlalchemy/"},{"name":"ORM","slug":"ORM","permalink":"https://blog.ifan.host/tags/ORM/"},{"name":"工具","slug":"工具","permalink":"https://blog.ifan.host/tags/%E5%B7%A5%E5%85%B7/"},{"name":"Minio","slug":"Minio","permalink":"https://blog.ifan.host/tags/Minio/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.ifan.host/tags/Docker/"},{"name":"Mac","slug":"Mac","permalink":"https://blog.ifan.host/tags/Mac/"},{"name":"数据库","slug":"数据库","permalink":"https://blog.ifan.host/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"MySQL","permalink":"https://blog.ifan.host/tags/MySQL/"},{"name":"NodeJS","slug":"NodeJS","permalink":"https://blog.ifan.host/tags/NodeJS/"},{"name":"爬虫","slug":"爬虫","permalink":"https://blog.ifan.host/tags/%E7%88%AC%E8%99%AB/"},{"name":"frida","slug":"frida","permalink":"https://blog.ifan.host/tags/frida/"},{"name":"编程工具","slug":"编程工具","permalink":"https://blog.ifan.host/tags/%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7/"},{"name":"生活","slug":"生活","permalink":"https://blog.ifan.host/tags/%E7%94%9F%E6%B4%BB/"},{"name":"大数据","slug":"大数据","permalink":"https://blog.ifan.host/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"HBase","slug":"HBase","permalink":"https://blog.ifan.host/tags/HBase/"},{"name":"HTTP","slug":"HTTP","permalink":"https://blog.ifan.host/tags/HTTP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.ifan.host/tags/Nginx/"},{"name":"Java","slug":"Java","permalink":"https://blog.ifan.host/tags/Java/"},{"name":"Spring Boot","slug":"Spring-Boot","permalink":"https://blog.ifan.host/tags/Spring-Boot/"},{"name":"MyBatis","slug":"MyBatis","permalink":"https://blog.ifan.host/tags/MyBatis/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://blog.ifan.host/tags/ElasticSearch/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://blog.ifan.host/tags/MongoDB/"},{"name":"Scala","slug":"Scala","permalink":"https://blog.ifan.host/tags/Scala/"},{"name":"基础知识","slug":"基础知识","permalink":"https://blog.ifan.host/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"golang","slug":"golang","permalink":"https://blog.ifan.host/tags/golang/"},{"name":"Sqlite","slug":"Sqlite","permalink":"https://blog.ifan.host/tags/Sqlite/"},{"name":"Redis","slug":"Redis","permalink":"https://blog.ifan.host/tags/Redis/"},{"name":"python","slug":"python","permalink":"https://blog.ifan.host/tags/python/"},{"name":"lxml","slug":"lxml","permalink":"https://blog.ifan.host/tags/lxml/"},{"name":"xpath","slug":"xpath","permalink":"https://blog.ifan.host/tags/xpath/"},{"name":"boto3","slug":"boto3","permalink":"https://blog.ifan.host/tags/boto3/"},{"name":"文件存储","slug":"文件存储","permalink":"https://blog.ifan.host/tags/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8/"},{"name":"mitmproxy","slug":"mitmproxy","permalink":"https://blog.ifan.host/tags/mitmproxy/"},{"name":"puppeteer","slug":"puppeteer","permalink":"https://blog.ifan.host/tags/puppeteer/"},{"name":"pyppeteer","slug":"pyppeteer","permalink":"https://blog.ifan.host/tags/pyppeteer/"},{"name":"proxy","slug":"proxy","permalink":"https://blog.ifan.host/tags/proxy/"},{"name":"Python","slug":"Python","permalink":"https://blog.ifan.host/tags/Python/"},{"name":"jupyterlab","slug":"jupyterlab","permalink":"https://blog.ifan.host/tags/jupyterlab/"},{"name":"nvm","slug":"nvm","permalink":"https://blog.ifan.host/tags/nvm/"},{"name":"Jupyter","slug":"Jupyter","permalink":"https://blog.ifan.host/tags/Jupyter/"},{"name":"pyenv","slug":"pyenv","permalink":"https://blog.ifan.host/tags/pyenv/"},{"name":"Lua","slug":"Lua","permalink":"https://blog.ifan.host/tags/Lua/"},{"name":"WEB","slug":"WEB","permalink":"https://blog.ifan.host/tags/WEB/"},{"name":"vim","slug":"vim","permalink":"https://blog.ifan.host/tags/vim/"},{"name":"ffmpeg","slug":"ffmpeg","permalink":"https://blog.ifan.host/tags/ffmpeg/"},{"name":"Linux","slug":"Linux","permalink":"https://blog.ifan.host/tags/Linux/"},{"name":"Centos","slug":"Centos","permalink":"https://blog.ifan.host/tags/Centos/"},{"name":"tesseract","slug":"tesseract","permalink":"https://blog.ifan.host/tags/tesseract/"},{"name":"Git","slug":"Git","permalink":"https://blog.ifan.host/tags/Git/"},{"name":"Debian","slug":"Debian","permalink":"https://blog.ifan.host/tags/Debian/"},{"name":"service","slug":"service","permalink":"https://blog.ifan.host/tags/service/"},{"name":"Wrk","slug":"Wrk","permalink":"https://blog.ifan.host/tags/Wrk/"}]}